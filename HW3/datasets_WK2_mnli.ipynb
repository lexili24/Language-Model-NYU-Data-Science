{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Wikitext 2 data loading code\n",
    "\n",
    "To help you move faster, here is the code to generate the dataloaders for each task\n",
    "\n",
    "#### Prereqs:\n",
    "1. Make sure you have 'wiki-news-300d-1M.vec' downloaded locally\n",
    "2. Make sure to have 'mnli_val.tsv' and 'mnli_train.tsv' locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import  MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    embedding_size = 300\n",
    "    max_vocab_size = 35000\n",
    "    embedding_dict = np.random.randn(max_vocab_size+2, embedding_size)\n",
    "    all_train_tokens = []\n",
    "    i = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        all_train_tokens.append(tokens[0])\n",
    "        embedding_dict[i+2] = list(map(float, tokens[1:]))\n",
    "        i += 1\n",
    "        if i == max_vocab_size:\n",
    "            break\n",
    "            \n",
    "    return embedding_dict, all_train_tokens\n",
    "  \n",
    "# download the vectors yourself\n",
    "fasttext_embedding_dict, all_fasttext_tokens = load_vectors('wiki-news-300d-1M.vec')\n",
    "  \n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, list_of_token_lists):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "\n",
    "        for sample in list_of_token_lists:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
    "\n",
    "\n",
    "def tokenize_dataset(datasets, dictionary):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>'] + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "    return tokenized_datasets\n",
    "\n",
    "def tokenize_mnli_dataset(datasets, dictionary):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for s1, s2 in tqdm(dataset):\n",
    "            s1 = ['<bos>'] + s1 + ['<eos>']\n",
    "            s2 = ['<bos>'] + s2 + ['<eos>']\n",
    "            encoded_s1 = dictionary.encode_token_seq(s1)            \n",
    "            encoded_s2 = dictionary.encode_token_seq(s2)\n",
    "            _current_dictified.append([encoded_s1, encoded_s2])\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "    return tokenized_datasets\n",
    "\n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat(\n",
    "            [t, torch.tensor([[pad_token] * (max_length - t.size(-1))], dtype=torch.long)], dim=-1)\n",
    "        padded_list.append(padded_tensor)\n",
    "\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "def pad_collate_fn(pad_idx, batch):\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_idx)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_idx)\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "def load_wikitext(data_dir):\n",
    "    import subprocess\n",
    "    filename = os.path.join(data_dir, 'wikitext2-sentencized.json')\n",
    "    if not os.path.exists(filename):\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        url = \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\"\n",
    "        args = ['wget', '-O', filename, url]\n",
    "        subprocess.call(args)\n",
    "    raw_datasets = json.load(open(filename, 'r'))\n",
    "    for name in raw_datasets:\n",
    "        raw_datasets[name] = [x.split() for x in raw_datasets[name]]\n",
    "\n",
    "    if os.path.exists(os.path.join(data_dir, 'vocab.pkl')):\n",
    "        vocab = pickle.load(open(os.path.join(data_dir, 'vocab.pkl'), 'rb'))\n",
    "    else:\n",
    "        vocab = Dictionary(raw_datasets, include_valid=False)\n",
    "        pickle.dump(vocab, open(os.path.join(data_dir, 'vocab.pkl'), 'wb'))\n",
    "\n",
    "    tokenized_datasets = tokenize_dataset(raw_datasets, vocab)\n",
    "    datasets = {name: LMDataset(ds) for name, ds in tokenized_datasets.items()}\n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return raw_datasets, datasets, vocab\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    " \n",
    "\n",
    "raw_datasets, datasets, vocab = load_wikitext(os.getcwd())\n",
    "\n",
    "data_loaders = {name: DataLoader(datasets[name], batch_size=32, shuffle=True,\n",
    "                                     collate_fn=lambda x: pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "                    for name in datasets}\n",
    "wk2_train_dataloader = data_loaders['train']\n",
    "wk2_val_dataloader = data_loaders['valid']\n",
    "wk2_test_dataloader = data_loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WIKITEXT 2 dataset statistics')\n",
    "print(f'training samples:{len(wk2_train_dataloader)}')\n",
    "print(f'val samples:{len(wk2_val_dataloader)}')\n",
    "print(f'test samples:{len(wk2_test_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "## MNLI loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_map = {'contradiction':0,'neutral':1,'entailment':2}\n",
    "import pandas as pd\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "def get_string_tokenized_data(data):\n",
    "    \n",
    "    tokenized_data_x = [];\n",
    "    y_labels = []\n",
    "    all_tokens = [];\n",
    "    \n",
    "    for i,x in enumerate(data):\n",
    "        label = x[2]\n",
    "        if label == 'nan':\n",
    "            continue\n",
    "        \n",
    "        label = y_label_map[label]\n",
    "        y_labels.append(label)\n",
    "        \n",
    "        dp = [x[0].split(), x[1].split()]\n",
    "        tokenized_data_x.append(dp)\n",
    "        all_tokens += (dp[0] + dp[1])\n",
    "        \n",
    "\n",
    "    return all_tokens, tokenized_data_x, y_labels\n",
    "        \n",
    "\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens1, tokens2 in tokens_data:\n",
    "        index_list1 = [token2id[token] if token in token2id else UNK_IDX for token in tokens1]\n",
    "        index_list2 = [token2id[token] if token in token2id else UNK_IDX for token in tokens2]\n",
    "        indices_data.append([index_list1, index_list2])\n",
    "    return indices_data\n",
    "  \n",
    "# convert token to id in the dataset\n",
    "def token2index_using_wikitext2_dict(tokens_data, vocab):\n",
    "    indices_data = []\n",
    "    for tokens1, tokens2 in tokens_data:\n",
    "        index_list1 = vocab.encode_token_seq(tokens1)\n",
    "        index_list2 = vocab.encode_token_seq(tokens2)\n",
    "        indices_data.append([index_list1, index_list2])\n",
    "    return indices_data\n",
    "\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Validation set (used as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD VAL\n",
    "\n",
    "val_df = pd.read_csv('mnli_val.tsv', sep=\"\\t\")\n",
    "print(val_df.head(2))\n",
    "\n",
    "val_df  = np.array(val_df)\n",
    "val_df = val_df.astype(str)\n",
    "val_genre_list = val_df[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, val_data_x, val_data_y = get_string_tokenized_data(val_df)\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "#### Training set (used as training, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('mnli_train.tsv', sep=\"\\t\")\n",
    "\n",
    "train_df  = np.array(train_df)\n",
    "train_df = train_df.astype(str)\n",
    "train_genre_list = train_df[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_data_x, train_data_y = get_string_tokenized_data(train_df)\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token2id, id2token = build_vocab(all_fasttext_tokens)\n",
    "# train_data_indices = token2index_using_wikitext2_dict(train_data_x, vocab)\n",
    "# val_data_indices = token2index_using_wikitext2_dict(val_data_x, vocab)\n",
    "\n",
    "\n",
    "mnli_raw_datasets = {'train': train_data_x, 'val': val_data_x}\n",
    "mnli_tokenized_datasets = tokenize_mnli_dataset(mnli_raw_datasets, vocab)\n",
    "\n",
    "train_data_indices = mnli_tokenized_datasets['train']\n",
    "val_data_indices = mnli_tokenized_datasets['val']\n",
    "\n",
    "\n",
    "# double checking\n",
    "print('\\n')\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "\n",
    "# del train_data_x\n",
    "# del train_data_y\n",
    "# del val_data_x\n",
    "# del val_data_y\n",
    "del mnli_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genre = list(set(val_genre_list));\n",
    "nb_classes = len(y_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "class MNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_x, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_x = data_x;\n",
    "        self.target_list = target_list\n",
    "        \n",
    "        assert(len(data_x) == len(target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        prem_token_idx = self.data_x[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        hyp_token_idx = self.data_x[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, hyp_token_idx, label]\n",
    "\n",
    "\n",
    "def encode_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    prem_data_list = []\n",
    "    hyp_data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    # print(\"collate batch: \", batch[0][0])\n",
    "    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        prem_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        hyp_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_data_list.append(prem_padded_vec)\n",
    "        hyp_data_list.append(hyp_padded_vec)\n",
    "    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "nb_train_samples = int(0.95 * len(train_data_indices))\n",
    "nb_val_samples = len(train_data_indices) - nb_train_samples\n",
    "\n",
    "# train/val split\n",
    "train_val_dataset = MNLIDataset(train_data_indices, train_data_y)\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [nb_train_samples, nb_val_samples])\n",
    "\n",
    "# train loader\n",
    "train_mnli_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# val loader\n",
    "val_mnli_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# test loader\n",
    "test_dataset = MNLIDataset(val_data_indices, val_data_y)\n",
    "test_mnli_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MNLI dataset statistics')\n",
    "print(f'training samples:{len(train_mnli_loader)}')\n",
    "print(f'val samples:{len(val_mnli_loader)}')\n",
    "print(f'test samples:{len(test_mnli_loader)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
