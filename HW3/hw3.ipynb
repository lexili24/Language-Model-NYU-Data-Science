{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Norm \n",
    "(1) reduce the problem the problem of input values changes <br>\n",
    "cause them to be stable so later neural network has ground to stay on <br>\n",
    "even the earlier layer keeps learning, the later layer that is forced to learn is reduced<br>\n",
    "each layer can be learn more independently <br>\n",
    "(2) Regularization effect <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_grad(model_ft):\n",
    "    print(\"Parameters to learn:\")\n",
    "    params_to_update = []\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        total_params+=param.numel()\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            total_trainable_params += param.numel()\n",
    "            print(\"\\t\",name)\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    print(f'{total_trainable_params:,} training parameters.')\n",
    "    return params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import inspect\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "# HW3 - Transfer learning\n",
    "\n",
    "#### Due October 30, 2019\n",
    "\n",
    "In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n",
    "\n",
    "The general steps to transfer learning are as follows:\n",
    "\n",
    "1. Find a huge dataset with similar characteristics to the problem you are interested in.\n",
    "2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n",
    "3. Train this model on the huge dataset.\n",
    "4. Use this model to train on the smaller dataset.\n",
    "\n",
    "\n",
    "### This homework has the following sections:\n",
    "1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n",
    "2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n",
    "3. Question 3: Finetune on MNLI (Part A, B, C, D)\n",
    "4. Question 4: Finetune using pretrained BERT (Part A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "## Question 1 (MNIST transfer learning)\n",
    "To grasp the high-level approach to transfer learning, let's first do a simple example using computer vision. \n",
    "\n",
    "The torchvision library has pretrained models (resnets, vggnets, etc) on the Imagenet dataset. Imagenet is a dataset\n",
    "with 1.3 million images covering over 1000 classes of objects. When you use one of these models, the weights of the model initialize\n",
    "with the weights saved from training on imagenet.\n",
    "\n",
    "In this task we will:\n",
    "1. Choose a pretrained model.\n",
    "2. Freeze the model so that the weights don't change.\n",
    "3. Fine-tune on a few labels of MNIST.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a model\n",
    "Here we pick any of the models from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn as nn\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# init the pretrained feature extractor\n",
    "pretrained_resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# we don't want the built in last layer, we're going to modify it ourselves\n",
    "pretrained_resnet18.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze the model\n",
    "Here we freeze the weights of the model. Freezing means the gradients will not backpropagate\n",
    "into these weights.\n",
    "\n",
    "By doing this you can think about the model as a feature extractor. This feature extractor outputs\n",
    "a **representation** of an input. This representation is a matrix that encodes information about the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "freeze_model(pretrained_resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to learn:\n",
      "11,176,512 total parameters.\n",
      "0 training parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_grad(pretrained_resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init target dataset\n",
    "Here we define the dataset we are actually interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import  MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#  train/val  split\n",
    "mnist_dataset = MNIST(os.getcwd(), train=True, download=True, \n",
    "                      transform = transforms.Compose([\n",
    "                          transforms.Resize((256,256)),\n",
    "                          transforms.Grayscale(3),\n",
    "                          transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                              (0.229, 0.224, 0.225))]))\n",
    "mnist_train, mnist_val = random_split(mnist_dataset, [55000, 5000])\n",
    "\n",
    "mnist_train = DataLoader(mnist_train, batch_size=32,shuffle=True)\n",
    "mnist_val = DataLoader(mnist_val, batch_size=32,shuffle=True)\n",
    "\n",
    "# test split\n",
    "mnist_test = DataLoader(MNIST(os.getcwd(), train=False, download=True, \n",
    "                              transform=transforms.Compose([\n",
    "                                transforms.Resize((256,256)),\n",
    "                                transforms.Grayscale(3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                              (0.229, 0.224, 0.22))])\n",
    "                             ), \n",
    "                        batch_size=32,\n",
    "                        shuffle=False)\n",
    "\n",
    "dataloaders = dict()\n",
    "dataloaders['train'] = mnist_train\n",
    "dataloaders['val'] = mnist_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([32, 3, 256, 256])\n",
      "Image label dimensions: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "for images, targets in mnist_train:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([32, 3, 256, 256])\n",
      "Image label dimensions: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, targets in mnist_test:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A (init fine-tune model)\n",
    "decide what model to use for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,\n",
      "            1.7083e-02, -1.2694e-02],\n",
      "          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,\n",
      "           -1.2907e-01,  3.7424e-03],\n",
      "          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,\n",
      "            2.5632e-01,  6.3573e-02],\n",
      "          ...,\n",
      "          [-2.7535e-02,  1.6045e-02,  7.2595e-02,  ..., -3.3285e-01,\n",
      "           -4.2058e-01, -2.5781e-01],\n",
      "          [ 3.0613e-02,  4.0960e-02,  6.2850e-02,  ...,  4.1384e-01,\n",
      "            3.9359e-01,  1.6606e-01],\n",
      "          [-1.3736e-02, -3.6746e-03, -2.4084e-02,  ..., -1.5070e-01,\n",
      "           -8.2230e-02, -5.7828e-03]],\n",
      "\n",
      "         [[-1.1397e-02, -2.6619e-02, -3.4641e-02,  ...,  3.2521e-02,\n",
      "            6.6221e-04, -2.5743e-02],\n",
      "          [ 4.5687e-02,  3.3603e-02, -1.0453e-01,  ..., -3.1253e-01,\n",
      "           -1.6051e-01, -1.2826e-03],\n",
      "          [-8.3730e-04,  9.8420e-02,  4.0210e-01,  ...,  7.0789e-01,\n",
      "            3.6887e-01,  1.2455e-01],\n",
      "          ...,\n",
      "          [-5.5926e-02, -5.2239e-03,  2.7081e-02,  ..., -4.6178e-01,\n",
      "           -5.7080e-01, -3.6552e-01],\n",
      "          [ 3.2860e-02,  5.5574e-02,  9.9670e-02,  ...,  5.4636e-01,\n",
      "            4.8276e-01,  1.9867e-01],\n",
      "          [ 5.3051e-03,  6.6938e-03, -1.7254e-02,  ..., -1.4822e-01,\n",
      "           -7.7248e-02,  7.2183e-04]],\n",
      "\n",
      "         [[-2.0315e-03, -9.1617e-03,  2.1209e-02,  ...,  8.9177e-02,\n",
      "            3.3655e-02, -2.0102e-02],\n",
      "          [ 1.5398e-02, -1.8648e-02, -1.2591e-01,  ..., -2.5342e-01,\n",
      "           -1.2980e-01, -2.7975e-02],\n",
      "          [ 9.8454e-03,  4.9047e-02,  2.1699e-01,  ...,  3.4872e-01,\n",
      "            1.0433e-01,  1.8413e-02],\n",
      "          ...,\n",
      "          [-2.8356e-02,  1.8404e-02,  9.8647e-02,  ..., -1.1740e-01,\n",
      "           -2.5760e-01, -1.5451e-01],\n",
      "          [ 2.0766e-02, -2.6286e-03, -3.7825e-02,  ...,  2.4141e-01,\n",
      "            2.4345e-01,  1.1796e-01],\n",
      "          [ 7.4684e-04,  7.7677e-04, -1.0050e-02,  ..., -1.4865e-01,\n",
      "           -1.1754e-01, -3.8350e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.4154e-03, -4.0645e-03,  3.1589e-03,  ..., -3.7026e-02,\n",
      "           -2.5158e-02, -4.7945e-02],\n",
      "          [ 5.1310e-02,  5.3402e-02,  8.0436e-02,  ...,  1.4480e-01,\n",
      "            1.4287e-01,  1.2312e-01],\n",
      "          [-7.3337e-03,  2.1755e-03,  3.7580e-02,  ...,  6.1517e-02,\n",
      "            8.0324e-02,  1.1715e-01],\n",
      "          ...,\n",
      "          [-2.6754e-02, -1.2297e-01, -1.3653e-01,  ..., -1.4068e-01,\n",
      "           -1.1155e-01, -4.9556e-02],\n",
      "          [ 2.3524e-02, -1.7288e-02, -1.1122e-02,  ..., -1.8826e-02,\n",
      "           -2.3320e-02, -2.9474e-02],\n",
      "          [ 2.8689e-02,  2.1659e-02,  4.7888e-02,  ...,  2.5498e-02,\n",
      "            3.5346e-02,  1.1280e-02]],\n",
      "\n",
      "         [[ 4.6919e-04,  1.2153e-02,  4.2035e-02,  ...,  4.6403e-02,\n",
      "            4.0423e-02, -1.4439e-02],\n",
      "          [ 4.3463e-02,  6.8779e-02,  1.3268e-01,  ...,  2.8606e-01,\n",
      "            2.6905e-01,  2.0935e-01],\n",
      "          [-5.7621e-02, -2.2642e-02,  3.0547e-02,  ...,  1.3763e-01,\n",
      "            1.6538e-01,  1.7946e-01],\n",
      "          ...,\n",
      "          [-1.0816e-01, -2.5227e-01, -2.9742e-01,  ..., -2.8503e-01,\n",
      "           -2.1493e-01, -1.0320e-01],\n",
      "          [ 4.0709e-02, -3.2771e-02, -6.3450e-02,  ..., -9.2360e-02,\n",
      "           -6.9876e-02, -4.9841e-02],\n",
      "          [ 8.2942e-02,  8.7580e-02,  1.0111e-01,  ...,  5.2714e-02,\n",
      "            6.0968e-02,  4.1198e-02]],\n",
      "\n",
      "         [[-1.6391e-02, -1.3870e-02,  5.2810e-03,  ...,  4.3698e-02,\n",
      "            2.2707e-02, -4.5983e-02],\n",
      "          [ 3.3202e-02,  4.2014e-02,  9.3500e-02,  ...,  2.6162e-01,\n",
      "            2.2970e-01,  1.6694e-01],\n",
      "          [-4.5987e-02, -1.6365e-02,  2.6811e-02,  ...,  1.4951e-01,\n",
      "            1.3216e-01,  1.3579e-01],\n",
      "          ...,\n",
      "          [-7.2129e-02, -1.8902e-01, -2.3389e-01,  ..., -1.9038e-01,\n",
      "           -1.5609e-01, -7.5974e-02],\n",
      "          [ 5.1161e-02, -2.5815e-02, -6.9357e-02,  ..., -5.8999e-02,\n",
      "           -6.1550e-02, -4.4555e-02],\n",
      "          [ 1.1174e-01,  7.8979e-02,  6.5849e-02,  ...,  3.1617e-02,\n",
      "            2.5221e-02,  7.4257e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.0826e-08, -6.4306e-08, -7.3806e-08,  ..., -9.8000e-08,\n",
      "           -1.0905e-07, -8.3421e-08],\n",
      "          [-6.1125e-09,  2.0613e-09, -8.0922e-09,  ..., -4.9840e-08,\n",
      "           -4.3836e-08, -3.0538e-09],\n",
      "          [ 7.1953e-08,  7.5616e-08,  5.9282e-08,  ..., -9.7509e-09,\n",
      "           -1.0951e-09,  4.2442e-08],\n",
      "          ...,\n",
      "          [ 9.5889e-08,  1.0039e-07,  7.9817e-08,  ..., -1.7491e-08,\n",
      "           -4.7666e-08, -1.3265e-08],\n",
      "          [ 1.2904e-07,  1.4762e-07,  1.7477e-07,  ...,  1.3233e-07,\n",
      "            1.0628e-07,  9.3316e-08],\n",
      "          [ 1.2558e-07,  1.3644e-07,  1.8431e-07,  ...,  2.1399e-07,\n",
      "            1.7710e-07,  1.7166e-07]],\n",
      "\n",
      "         [[-1.2690e-07, -9.6139e-08, -1.0372e-07,  ..., -1.1808e-07,\n",
      "           -1.3309e-07, -1.0820e-07],\n",
      "          [-5.7412e-08, -2.5055e-08, -3.0115e-08,  ..., -7.2922e-08,\n",
      "           -6.7022e-08, -2.2574e-08],\n",
      "          [ 2.1813e-08,  4.8608e-08,  3.1222e-08,  ..., -1.8694e-08,\n",
      "           -7.9591e-09,  3.9750e-08],\n",
      "          ...,\n",
      "          [ 5.6013e-08,  7.5526e-08,  4.4496e-08,  ..., -4.4128e-08,\n",
      "           -5.9930e-08, -1.8247e-08],\n",
      "          [ 7.7614e-08,  9.8348e-08,  1.0455e-07,  ...,  6.3272e-08,\n",
      "            4.1781e-08,  4.5901e-08],\n",
      "          [ 5.9834e-08,  7.1006e-08,  9.0437e-08,  ...,  1.1654e-07,\n",
      "            8.7550e-08,  9.8837e-08]],\n",
      "\n",
      "         [[-4.3810e-08,  1.3270e-08,  7.8275e-09,  ..., -5.8804e-09,\n",
      "           -2.6217e-08, -1.5649e-08],\n",
      "          [ 4.1700e-08,  1.0778e-07,  1.0946e-07,  ...,  7.6403e-08,\n",
      "            7.1450e-08,  9.7615e-08],\n",
      "          [ 1.0436e-07,  1.6586e-07,  1.5933e-07,  ...,  1.3517e-07,\n",
      "            1.3487e-07,  1.6449e-07],\n",
      "          ...,\n",
      "          [ 9.8763e-08,  1.5072e-07,  1.2547e-07,  ...,  6.8316e-08,\n",
      "            6.8382e-08,  1.1367e-07],\n",
      "          [ 9.1435e-08,  1.3576e-07,  1.3793e-07,  ...,  1.1678e-07,\n",
      "            1.1723e-07,  1.4394e-07],\n",
      "          [ 6.2183e-08,  8.8184e-08,  1.0456e-07,  ...,  1.3941e-07,\n",
      "            1.3333e-07,  1.5844e-07]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.1896e-02, -3.0206e-02,  1.9225e-02,  ...,  4.3665e-02,\n",
      "           -2.2114e-02, -4.2214e-02],\n",
      "          [-3.8061e-02,  6.0774e-03,  4.5797e-02,  ...,  9.6029e-02,\n",
      "            5.9254e-02,  2.9958e-02],\n",
      "          [-2.9672e-02,  2.7766e-03,  2.0457e-02,  ...,  5.9828e-02,\n",
      "            4.1422e-02,  2.3134e-02],\n",
      "          ...,\n",
      "          [ 1.1916e-02,  4.5701e-02,  4.4892e-02,  ...,  4.7419e-02,\n",
      "            2.2274e-02, -5.4993e-03],\n",
      "          [-3.2468e-02, -1.2210e-02,  2.2023e-02,  ...,  5.8061e-02,\n",
      "           -7.5033e-03, -5.9736e-02],\n",
      "          [-4.3314e-02, -2.8162e-02, -5.9126e-03,  ...,  8.8460e-02,\n",
      "            8.4406e-03, -5.0019e-02]],\n",
      "\n",
      "         [[-6.1292e-02, -1.4004e-02,  1.7229e-02,  ...,  1.8349e-02,\n",
      "           -3.2708e-02, -4.1060e-02],\n",
      "          [-3.1506e-02,  2.4460e-02,  4.5516e-02,  ...,  6.6806e-02,\n",
      "            4.6687e-02,  3.3248e-02],\n",
      "          [-3.2216e-02,  2.0718e-02,  2.3343e-02,  ...,  3.5265e-02,\n",
      "            3.6478e-02,  3.1291e-02],\n",
      "          ...,\n",
      "          [ 1.7739e-02,  6.1040e-02,  4.8247e-02,  ...,  3.7785e-02,\n",
      "            2.8894e-02,  1.3984e-02],\n",
      "          [-1.0890e-02,  2.2079e-02,  4.2737e-02,  ...,  6.0247e-02,\n",
      "            1.6197e-02, -1.2493e-02],\n",
      "          [-2.2284e-02,  1.3220e-02,  3.0897e-02,  ...,  1.0403e-01,\n",
      "            4.0119e-02, -5.3310e-03]],\n",
      "\n",
      "         [[-8.5322e-02, -4.2603e-02,  6.8145e-03,  ...,  3.0751e-02,\n",
      "           -3.4818e-02, -4.9945e-02],\n",
      "          [-2.9215e-02,  1.8165e-02,  5.1092e-02,  ...,  9.0200e-02,\n",
      "            5.3438e-02,  4.0169e-02],\n",
      "          [-3.9932e-02, -1.1100e-03,  9.6176e-03,  ...,  2.4114e-02,\n",
      "            2.6298e-02,  2.5489e-02],\n",
      "          ...,\n",
      "          [-3.1890e-03,  3.0454e-02,  1.6316e-02,  ...,  5.5054e-03,\n",
      "           -6.2689e-03, -8.4638e-03],\n",
      "          [-2.2995e-02, -2.8211e-03,  2.3203e-02,  ...,  3.5888e-02,\n",
      "           -1.4296e-02, -3.2419e-02],\n",
      "          [-9.8894e-03,  7.0542e-03,  1.0659e-02,  ...,  7.0495e-02,\n",
      "            1.2996e-02, -8.3417e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.8699e-03,  1.9911e-02,  3.4208e-02,  ...,  2.8694e-02,\n",
      "            1.2820e-02,  1.8142e-02],\n",
      "          [ 8.7942e-03, -3.2875e-02, -3.5713e-02,  ...,  7.2533e-02,\n",
      "            4.5889e-02,  5.2383e-02],\n",
      "          [-3.6122e-02, -1.1878e-01, -1.3767e-01,  ...,  3.3811e-02,\n",
      "            3.7806e-02,  2.6944e-02],\n",
      "          ...,\n",
      "          [ 1.7322e-02,  3.9589e-03, -8.2269e-03,  ...,  2.7543e-03,\n",
      "            1.8313e-02,  1.6057e-02],\n",
      "          [-9.5007e-04,  1.6428e-02,  1.7156e-02,  ...,  3.3672e-03,\n",
      "            2.2857e-02,  6.5783e-04],\n",
      "          [ 6.1727e-03,  2.7145e-02,  1.4340e-02,  ...,  7.5867e-03,\n",
      "            1.8770e-02,  1.5624e-02]],\n",
      "\n",
      "         [[-1.3423e-02, -5.0696e-04,  8.0959e-03,  ..., -6.0963e-03,\n",
      "            9.2341e-03,  1.5751e-02],\n",
      "          [-1.8343e-02, -6.7982e-02, -7.0685e-02,  ...,  2.9855e-02,\n",
      "            2.6264e-02,  2.3773e-02],\n",
      "          [-5.4359e-02, -1.4663e-01, -1.6211e-01,  ...,  1.1781e-02,\n",
      "            3.2477e-02,  1.1980e-02],\n",
      "          ...,\n",
      "          [ 8.3686e-04, -1.7564e-02, -1.9535e-02,  ..., -4.1382e-03,\n",
      "            2.4658e-02,  1.2893e-02],\n",
      "          [-6.3183e-04,  1.1788e-02,  2.4810e-02,  ...,  6.1105e-03,\n",
      "            3.9210e-02,  9.6696e-03],\n",
      "          [-7.1831e-03,  6.6918e-03,  5.2723e-03,  ..., -7.6077e-03,\n",
      "            2.7253e-02,  1.7735e-02]],\n",
      "\n",
      "         [[-2.3753e-04, -4.9343e-03,  2.2991e-03,  ..., -4.7958e-02,\n",
      "           -2.6154e-02, -2.3525e-02],\n",
      "          [-3.3053e-04, -5.1502e-02, -5.9977e-02,  ..., -1.7369e-02,\n",
      "           -2.3337e-02, -3.7312e-02],\n",
      "          [-2.2674e-02, -9.9412e-02, -1.1176e-01,  ..., -1.1725e-02,\n",
      "           -8.3744e-03, -4.0615e-02],\n",
      "          ...,\n",
      "          [ 1.1437e-02, -8.0313e-03, -1.4955e-03,  ..., -3.4133e-02,\n",
      "           -8.7267e-03, -2.3526e-02],\n",
      "          [ 2.9522e-03,  6.7770e-04,  1.9933e-02,  ..., -2.2002e-02,\n",
      "            1.4814e-02, -1.4487e-02],\n",
      "          [-1.9085e-02, -2.9430e-02, -2.3284e-02,  ..., -4.8587e-02,\n",
      "           -1.3049e-02, -2.4368e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.6296e-02,  7.1996e-03,  1.9100e-02,  ...,  1.9602e-02,\n",
      "            1.4870e-02, -1.7298e-02],\n",
      "          [-1.1061e-02,  8.5665e-02,  1.2667e-01,  ...,  1.3744e-02,\n",
      "           -5.5036e-05, -3.0162e-02],\n",
      "          [ 1.1322e-01,  1.8634e-01,  5.0658e-02,  ..., -1.7333e-01,\n",
      "           -7.2041e-02, -6.2474e-02],\n",
      "          ...,\n",
      "          [-5.3062e-02, -2.5781e-01, -2.6747e-01,  ...,  2.6781e-01,\n",
      "            1.4344e-01,  5.5145e-02],\n",
      "          [-2.1009e-02, -2.9969e-02,  1.0245e-01,  ...,  2.0843e-01,\n",
      "           -4.1518e-03, -3.8118e-02],\n",
      "          [-2.2155e-02,  1.2380e-02,  8.4302e-02,  ..., -4.4992e-02,\n",
      "           -1.4687e-01, -9.0890e-02]],\n",
      "\n",
      "         [[-5.3969e-03,  3.2799e-02,  1.5486e-02,  ..., -7.7451e-03,\n",
      "            3.0229e-03,  1.1216e-03],\n",
      "          [ 6.1723e-02,  1.4899e-01,  1.4645e-01,  ..., -2.8897e-02,\n",
      "           -2.0227e-02, -9.1878e-03],\n",
      "          [ 1.6146e-01,  2.0886e-01, -2.5589e-02,  ..., -2.7278e-01,\n",
      "           -1.0735e-01, -6.2971e-02],\n",
      "          ...,\n",
      "          [-1.3723e-01, -4.0863e-01, -3.8551e-01,  ...,  4.0846e-01,\n",
      "            2.6202e-01,  1.3491e-01],\n",
      "          [-5.9388e-02, -6.1187e-02,  1.4197e-01,  ...,  3.5780e-01,\n",
      "            9.0893e-02, -1.7392e-03],\n",
      "          [ 7.8613e-03,  5.8403e-02,  1.5339e-01,  ...,  4.7045e-02,\n",
      "           -1.0095e-01, -9.7920e-02]],\n",
      "\n",
      "         [[-5.6799e-03,  1.3425e-02, -2.6461e-02,  ...,  4.4881e-03,\n",
      "            2.0666e-03,  1.3902e-02],\n",
      "          [ 6.5943e-03,  4.5181e-02,  6.0260e-02,  ...,  1.4368e-02,\n",
      "           -5.0725e-03,  4.0505e-03],\n",
      "          [ 5.5257e-02,  1.2397e-01,  4.3193e-02,  ..., -1.4486e-01,\n",
      "           -7.4489e-02, -5.7533e-02],\n",
      "          ...,\n",
      "          [-3.1513e-02, -1.6334e-01, -1.5795e-01,  ...,  2.2904e-01,\n",
      "            1.2017e-01,  7.1998e-02],\n",
      "          [-1.0456e-02, -1.1248e-03,  8.4582e-02,  ...,  1.5748e-01,\n",
      "            2.2142e-02, -1.0083e-02],\n",
      "          [-4.8639e-03, -5.0065e-03,  3.6341e-02,  ..., -2.4361e-02,\n",
      "           -7.1195e-02, -6.6788e-02]]]])\n",
      "9408\n",
      "Parameter containing:\n",
      "tensor([ 2.3487e-01,  2.6626e-01, -5.1096e-08,  5.1870e-01,  3.4404e-09,\n",
      "         2.2239e-01,  4.2289e-01,  1.3153e-07,  2.5093e-01,  1.5152e-06,\n",
      "         3.1687e-01,  2.5049e-01,  3.7893e-01,  1.0862e-05,  2.7526e-01,\n",
      "         2.3674e-01,  2.4202e-01,  3.9531e-01,  4.6935e-01,  2.9090e-01,\n",
      "         2.7268e-01,  2.7803e-01,  2.9069e-01,  2.0693e-01,  2.5899e-01,\n",
      "         2.7871e-01,  2.9115e-01,  3.1601e-01,  3.8889e-01,  3.0411e-01,\n",
      "         2.6776e-01,  2.1093e-01,  2.8708e-01,  3.3243e-01,  4.2673e-01,\n",
      "         3.7326e-01,  7.4804e-08,  1.9068e-01,  1.4740e-08,  2.2303e-01,\n",
      "         1.7908e-01,  2.4860e-01,  2.7400e-01,  2.5923e-01,  2.9420e-01,\n",
      "         2.9924e-01,  2.2369e-01,  2.6280e-01,  2.2001e-08,  2.6610e-01,\n",
      "         2.2089e-01,  2.8429e-01,  3.3072e-01,  2.2681e-01,  3.6538e-01,\n",
      "         2.1230e-01,  2.3965e-01,  2.4950e-01,  5.2583e-01,  2.4825e-01,\n",
      "         2.9565e-01,  2.5878e-01,  4.8326e-01,  2.6670e-01])\n",
      "64\n",
      "Parameter containing:\n",
      "tensor([ 2.3072e-01,  2.5382e-01, -1.0543e-06, -6.6439e-01, -1.6571e-08,\n",
      "         1.6152e-01,  4.5450e-01, -4.3020e-07,  3.0051e-01, -8.0052e-06,\n",
      "         3.4942e-01,  3.1148e-01, -2.4953e-01, -3.4749e-05,  1.0773e-01,\n",
      "         2.1897e-01,  3.8141e-01, -5.2988e-01, -6.2864e-01,  5.7140e-01,\n",
      "         2.9985e-01,  5.8430e-01,  4.8202e-01,  3.2853e-01,  1.9672e-01,\n",
      "         1.9496e-01,  1.5215e-01,  8.5522e-02,  5.1314e-01,  1.5237e-02,\n",
      "         1.6644e-01,  3.3239e-01,  2.4921e-01,  4.4337e-01, -2.8017e-01,\n",
      "        -2.0385e-02, -2.4507e-07,  3.2134e-01, -4.9152e-08,  2.3777e-01,\n",
      "         2.3291e-01,  3.1527e-01,  4.2776e-01,  2.9313e-01,  2.6379e-01,\n",
      "         6.7598e-01,  4.2910e-01,  3.4566e-01, -8.6909e-08,  2.4729e-01,\n",
      "         3.0316e-01,  6.1577e-01,  3.9835e-01,  3.3207e-01, -4.1219e-01,\n",
      "         3.7807e-01,  1.7895e-01,  2.5748e-01, -4.4908e-01,  2.1306e-01,\n",
      "         5.6934e-01,  5.7274e-01, -4.0238e-01,  2.3406e-01])\n",
      "64\n",
      "Parameter containing:\n",
      "tensor([[[[ 5.7593e-02, -9.5114e-02, -2.0272e-02],\n",
      "          [-7.4556e-02, -7.9931e-01, -2.1284e-01],\n",
      "          [ 6.5571e-02, -9.6534e-02, -1.2111e-02]],\n",
      "\n",
      "         [[-6.9944e-03,  1.4266e-02,  5.5824e-04],\n",
      "          [ 4.1238e-02, -1.6125e-01, -2.3208e-02],\n",
      "          [ 3.2887e-03,  7.1779e-03,  7.1686e-02]],\n",
      "\n",
      "         [[-2.3627e-09, -3.9270e-08, -3.2971e-08],\n",
      "          [ 2.1737e-08,  8.3299e-09,  1.2543e-08],\n",
      "          [ 1.1382e-08,  8.8096e-09,  1.5506e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6921e-02,  1.8294e-02, -2.9358e-02],\n",
      "          [-9.8615e-02, -4.3645e-02, -5.2717e-02],\n",
      "          [-7.9635e-02,  2.9396e-02,  4.1479e-03]],\n",
      "\n",
      "         [[ 1.6948e-02,  1.3978e-02,  9.6727e-03],\n",
      "          [ 1.4297e-02, -6.6985e-04, -2.2077e-02],\n",
      "          [ 1.2398e-02,  3.5454e-02, -2.2320e-02]],\n",
      "\n",
      "         [[-2.2600e-02, -2.5331e-02, -2.3548e-02],\n",
      "          [ 6.0860e-02, -9.6779e-02,  2.4057e-02],\n",
      "          [-1.2750e-02,  9.2237e-02,  4.0152e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2160e-02,  4.2177e-02, -1.6428e-02],\n",
      "          [-2.9667e-02,  5.6865e-02,  2.5486e-02],\n",
      "          [ 4.3847e-03,  5.1188e-02,  1.0436e-02]],\n",
      "\n",
      "         [[ 2.5342e-02,  5.4374e-02,  5.3888e-02],\n",
      "          [-2.8334e-02, -2.0139e-01, -5.6358e-02],\n",
      "          [ 5.6774e-02,  7.4188e-02,  2.1585e-02]],\n",
      "\n",
      "         [[-3.1458e-08,  3.5335e-08,  5.3791e-08],\n",
      "          [-2.6896e-08,  5.1530e-08,  5.4480e-08],\n",
      "          [-3.8487e-08, -1.1234e-08, -7.5787e-09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2754e-01,  4.3552e-02, -6.5607e-02],\n",
      "          [-6.0462e-02,  1.5989e-01, -7.7070e-03],\n",
      "          [-9.4202e-02,  5.0750e-02, -7.8154e-02]],\n",
      "\n",
      "         [[-3.3309e-02,  1.6631e-03, -8.8497e-03],\n",
      "          [ 1.5553e-02, -5.8277e-02, -2.7437e-02],\n",
      "          [ 1.3126e-02, -3.0268e-02, -2.1661e-03]],\n",
      "\n",
      "         [[-4.2313e-03,  3.4517e-02,  3.8193e-03],\n",
      "          [ 5.4317e-02, -1.2457e-02,  3.2900e-02],\n",
      "          [ 2.2000e-04,  1.6040e-02,  1.2764e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5247e-02,  8.0748e-03,  2.0353e-02],\n",
      "          [ 1.7344e-02, -2.4320e-02, -1.5511e-04],\n",
      "          [-2.7634e-04,  2.8024e-02, -2.3777e-03]],\n",
      "\n",
      "         [[-2.3741e-02, -3.2057e-03, -5.7059e-03],\n",
      "          [-1.1582e-02,  1.7200e-03,  2.1067e-02],\n",
      "          [ 4.3606e-03, -4.6459e-02, -7.2954e-02]],\n",
      "\n",
      "         [[ 3.1002e-08,  5.3568e-08,  3.1873e-08],\n",
      "          [-1.6063e-08, -1.8072e-08, -1.9508e-09],\n",
      "          [-5.8339e-08, -4.5366e-08, -1.2395e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9689e-03, -2.6809e-02, -4.3760e-02],\n",
      "          [ 2.4518e-02, -2.8396e-02, -3.5896e-02],\n",
      "          [-1.7883e-04, -2.4661e-02, -2.0085e-02]],\n",
      "\n",
      "         [[ 2.1551e-02,  2.2789e-03, -2.5823e-02],\n",
      "          [ 2.3272e-02, -7.9333e-03, -2.0814e-03],\n",
      "          [-5.7062e-03, -2.6934e-02, -1.4421e-02]],\n",
      "\n",
      "         [[-1.9674e-02,  2.7914e-02, -2.0025e-02],\n",
      "          [ 6.3222e-02, -3.9077e-02, -3.3220e-03],\n",
      "          [-2.7434e-02,  1.1390e-02, -3.1608e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.3440e-03, -7.6970e-03, -6.4950e-02],\n",
      "          [ 1.3846e-02, -2.2803e-02, -4.6478e-02],\n",
      "          [ 2.7776e-02,  1.6080e-02, -1.3363e-02]],\n",
      "\n",
      "         [[ 4.7379e-02, -2.4982e-02, -2.7605e-02],\n",
      "          [ 7.0091e-02,  4.2084e-03, -1.0805e-01],\n",
      "          [ 1.7526e-02,  4.5647e-02,  7.8810e-03]],\n",
      "\n",
      "         [[ 2.6680e-09,  2.7671e-08,  2.4702e-08],\n",
      "          [ 6.3905e-09,  4.1020e-08,  3.3631e-08],\n",
      "          [ 5.8335e-09,  1.3334e-08,  9.6604e-09]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5900e-03,  4.7084e-02, -8.6949e-03],\n",
      "          [-6.3011e-03,  5.9585e-02,  5.8667e-03],\n",
      "          [-2.0255e-02,  4.3285e-02,  4.5094e-03]],\n",
      "\n",
      "         [[ 1.1253e-03, -5.7461e-03, -6.8411e-03],\n",
      "          [ 6.0616e-03,  7.3295e-03, -1.1784e-02],\n",
      "          [-1.1455e-03,  5.1868e-03, -1.9867e-02]],\n",
      "\n",
      "         [[ 1.7529e-02,  4.4606e-02, -2.6595e-02],\n",
      "          [ 2.2102e-02,  4.5857e-02,  2.3347e-02],\n",
      "          [ 1.8052e-02,  5.9689e-02,  1.7129e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9112e-02,  3.4242e-03, -1.7523e-02],\n",
      "          [-2.3682e-02,  2.2716e-02, -3.8301e-02],\n",
      "          [-1.0308e-02, -4.3802e-03, -2.3582e-02]],\n",
      "\n",
      "         [[-4.9607e-02, -3.2724e-03, -1.5345e-02],\n",
      "          [-1.3524e-02,  5.4842e-02,  1.1187e-02],\n",
      "          [-2.3549e-02, -2.8495e-02, -6.6371e-02]],\n",
      "\n",
      "         [[-4.9804e-08, -2.8211e-08, -2.0583e-08],\n",
      "          [-5.2389e-08, -2.8522e-08, -3.5099e-08],\n",
      "          [-3.2171e-08, -3.4110e-08, -4.3153e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4487e-03,  2.6532e-02, -1.1202e-02],\n",
      "          [ 7.0925e-03,  3.7903e-02, -3.2481e-02],\n",
      "          [ 4.1381e-02,  3.2329e-02,  2.8309e-03]],\n",
      "\n",
      "         [[-6.5955e-03,  1.6476e-02,  2.1810e-02],\n",
      "          [-1.2293e-02,  2.2310e-02,  1.2645e-02],\n",
      "          [-8.9897e-03,  1.1948e-03, -5.2390e-03]],\n",
      "\n",
      "         [[-2.5295e-03,  7.2689e-02, -7.8046e-03],\n",
      "          [-4.2221e-02,  7.9756e-02, -2.7738e-02],\n",
      "          [ 4.6716e-03, -5.6596e-02, -8.2261e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2235e-02,  3.5231e-03, -3.3131e-02],\n",
      "          [ 3.1048e-02,  1.6193e-02,  1.7283e-02],\n",
      "          [ 1.4446e-02,  2.4302e-02, -1.9689e-03]],\n",
      "\n",
      "         [[-2.4717e-02,  8.3009e-03, -6.1336e-02],\n",
      "          [-1.6134e-02,  5.5323e-02, -6.5029e-02],\n",
      "          [-2.4715e-02,  1.0030e-03,  3.2437e-02]],\n",
      "\n",
      "         [[ 1.8496e-08,  5.2798e-09,  4.1820e-08],\n",
      "          [ 3.7489e-08,  2.5450e-08,  3.0419e-08],\n",
      "          [ 1.1246e-08, -5.6956e-09, -2.0008e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.1194e-03, -4.1052e-02, -1.0002e-02],\n",
      "          [ 2.5924e-02, -6.3819e-02,  1.3366e-02],\n",
      "          [ 2.9751e-02, -7.9476e-03,  1.4007e-02]],\n",
      "\n",
      "         [[-2.5166e-03,  2.2051e-02, -1.9967e-02],\n",
      "          [-5.9436e-02,  4.3872e-02,  2.6832e-02],\n",
      "          [-1.7509e-02,  2.4625e-02,  2.4822e-02]],\n",
      "\n",
      "         [[ 3.5832e-02, -7.0357e-02,  3.9452e-03],\n",
      "          [-2.9835e-02,  9.2727e-02,  1.9336e-02],\n",
      "          [-2.9145e-02, -9.7087e-03, -7.3388e-02]]]])\n",
      "36864\n",
      "Parameter containing:\n",
      "tensor([0.3090, 0.2147, 0.2366, 0.4259, 0.5137, 0.2181, 0.2204, 0.2300, 0.2640,\n",
      "        0.2695, 0.2138, 0.4602, 0.2661, 0.2319, 0.3900, 0.2389, 0.2660, 0.3634,\n",
      "        0.3474, 0.2477, 0.3285, 0.5349, 0.6440, 0.2275, 0.4482, 0.3078, 0.2604,\n",
      "        0.4651, 0.2179, 0.2858, 0.3426, 0.4420, 0.4450, 0.4500, 0.5516, 0.5092,\n",
      "        0.2564, 0.2634, 0.5664, 0.6410, 0.2228, 0.1986, 0.2460, 0.2242, 0.2143,\n",
      "        0.1982, 0.6368, 0.3106, 0.5049, 0.2403, 0.3065, 0.3760, 0.3794, 0.4281,\n",
      "        0.2991, 0.3326, 0.2596, 0.3345, 0.2006, 0.4351, 0.1683, 0.5149, 0.2629,\n",
      "        0.3254])\n",
      "64\n",
      "Parameter containing:\n",
      "tensor([ 0.1657,  0.2420,  0.1780, -0.0431, -0.2053,  0.1598,  0.2929,  0.0912,\n",
      "         0.1116,  0.0884,  0.1104, -0.2035,  0.1539,  0.0857, -0.1094,  0.0654,\n",
      "         0.0766, -0.2067, -0.0212,  0.1396,  0.0401, -0.2827, -0.3257, -0.0035,\n",
      "        -0.4373, -0.1248,  0.1282, -0.0874,  0.1199, -0.0829, -0.5315, -0.0780,\n",
      "        -0.3876, -0.0547, -0.1816, -0.1888,  0.1320,  0.0031, -0.2697, -0.2984,\n",
      "         0.1394,  0.2597,  0.1372,  0.0053,  0.0132,  0.3295, -0.2715, -0.0187,\n",
      "        -0.2467,  0.1579,  0.0165, -0.0890, -0.1903, -0.0787,  0.1700, -0.4832,\n",
      "         0.0619, -0.0677,  0.3125, -0.5064,  0.3138, -0.2617, -0.1545,  0.0063])\n",
      "64\n",
      "Parameter containing:\n",
      "tensor([[[[ 2.5947e-02, -1.0458e-01, -4.7712e-03],\n",
      "          [-8.6223e-02, -3.3021e-01, -1.0275e-01],\n",
      "          [-5.7426e-02, -1.9074e-01, -5.4646e-02]],\n",
      "\n",
      "         [[-1.6951e-02,  2.1384e-02, -2.1074e-03],\n",
      "          [-3.2983e-03,  4.5014e-02, -1.1510e-02],\n",
      "          [-5.9602e-02,  6.4942e-03,  2.9080e-03]],\n",
      "\n",
      "         [[-4.4903e-03,  1.9637e-02,  1.3167e-02],\n",
      "          [ 1.3050e-02, -7.7471e-03,  1.1931e-02],\n",
      "          [ 1.3454e-02,  1.1103e-02,  5.5145e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2706e-03, -7.7438e-03,  2.0753e-02],\n",
      "          [-4.0024e-02, -4.0383e-02, -3.4821e-02],\n",
      "          [-2.0251e-02, -9.5164e-03,  1.3954e-02]],\n",
      "\n",
      "         [[-2.3430e-03,  3.2303e-02, -4.3342e-03],\n",
      "          [ 8.6194e-03,  1.0553e-02,  1.8074e-03],\n",
      "          [-1.2760e-02, -1.0232e-02,  4.5711e-03]],\n",
      "\n",
      "         [[ 1.5302e-02,  2.1361e-02, -7.0908e-03],\n",
      "          [-1.4221e-02,  4.5979e-02,  2.1369e-02],\n",
      "          [ 3.1312e-02,  6.6428e-02,  2.1465e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3422e-02,  4.0515e-02,  9.6680e-03],\n",
      "          [ 3.2884e-02, -2.3474e-02,  3.4642e-02],\n",
      "          [-1.2861e-02,  5.0066e-02,  5.4579e-02]],\n",
      "\n",
      "         [[ 2.8764e-02,  4.3431e-02,  2.8258e-02],\n",
      "          [ 2.8734e-02, -3.5459e-02, -5.2788e-02],\n",
      "          [-5.5119e-02, -7.1813e-02, -8.2970e-02]],\n",
      "\n",
      "         [[ 9.5293e-02,  1.2549e-01, -6.4001e-02],\n",
      "          [-4.1166e-02, -9.0480e-04,  5.1387e-02],\n",
      "          [-1.1311e-01, -7.9823e-02,  1.4373e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.6924e-03,  2.0647e-02,  1.9521e-02],\n",
      "          [-6.7352e-03,  1.2601e-04,  4.8309e-03],\n",
      "          [-6.2405e-03, -9.2119e-03, -2.5806e-04]],\n",
      "\n",
      "         [[-2.6153e-02, -2.4641e-02,  4.0970e-02],\n",
      "          [-1.9164e-02, -1.0160e-02,  3.3163e-02],\n",
      "          [ 5.4200e-03,  9.0485e-04,  6.7799e-04]],\n",
      "\n",
      "         [[ 7.7762e-03,  2.6447e-02,  6.3650e-02],\n",
      "          [-3.0608e-02,  2.4959e-02,  1.2951e-02],\n",
      "          [-2.0938e-02, -7.7342e-03, -3.8790e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0893e-02, -1.4409e-02,  1.5730e-02],\n",
      "          [ 1.6655e-02,  4.4535e-02,  6.3212e-02],\n",
      "          [ 3.4121e-02,  7.3135e-02,  5.9203e-02]],\n",
      "\n",
      "         [[ 2.3195e-03,  7.7598e-03,  2.0308e-02],\n",
      "          [ 2.0457e-02,  4.0029e-02,  3.4744e-02],\n",
      "          [-4.7356e-02, -3.7286e-02,  1.4542e-02]],\n",
      "\n",
      "         [[-2.2742e-02, -1.9000e-02, -8.4317e-03],\n",
      "          [-9.8759e-04,  2.1510e-02,  6.3959e-03],\n",
      "          [-9.4558e-03,  2.6833e-03, -3.1136e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5787e-03, -1.6056e-02, -6.4204e-04],\n",
      "          [-5.5104e-03,  1.4252e-02,  4.5000e-02],\n",
      "          [-9.2800e-03,  2.2351e-02,  4.1728e-02]],\n",
      "\n",
      "         [[ 2.5705e-02,  4.8207e-02,  7.9145e-02],\n",
      "          [-4.4350e-03,  3.8872e-03,  4.1694e-02],\n",
      "          [ 8.0536e-04, -1.0601e-02,  9.2706e-03]],\n",
      "\n",
      "         [[-3.3892e-02,  9.3543e-03,  4.1746e-02],\n",
      "          [-1.6470e-02,  3.9542e-03,  6.2438e-02],\n",
      "          [-3.1055e-02, -3.6302e-03,  7.0817e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.1044e-05, -9.0020e-03, -2.6998e-03],\n",
      "          [ 3.0072e-03,  1.1579e-02,  1.5214e-02],\n",
      "          [ 3.4832e-03,  1.1353e-05,  1.6320e-02]],\n",
      "\n",
      "         [[-2.6334e-02,  2.1967e-02, -6.0039e-02],\n",
      "          [ 4.4519e-02,  1.3203e-01, -9.1163e-03],\n",
      "          [ 5.4242e-02,  1.3726e-01,  2.7454e-02]],\n",
      "\n",
      "         [[ 1.7122e-02,  3.7646e-03,  1.4872e-02],\n",
      "          [ 1.2092e-02,  1.1319e-02,  3.4667e-02],\n",
      "          [ 8.1790e-03, -2.0805e-02,  2.7143e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0111e-02, -1.0526e-02,  2.8394e-02],\n",
      "          [-2.5112e-02, -2.2196e-02,  3.7229e-02],\n",
      "          [-3.8220e-02, -4.6644e-02,  1.5660e-02]],\n",
      "\n",
      "         [[-2.5913e-03, -2.4307e-02,  1.0611e-02],\n",
      "          [-2.1730e-02, -4.3938e-02, -7.1536e-03],\n",
      "          [-2.5171e-02, -5.9467e-02, -2.5577e-02]],\n",
      "\n",
      "         [[ 2.8652e-02,  2.5850e-04,  1.1416e-03],\n",
      "          [ 3.7812e-02, -1.1271e-03,  9.6027e-03],\n",
      "          [ 3.9350e-02,  1.0134e-02,  1.0449e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.9305e-03,  7.0872e-03,  2.1412e-02],\n",
      "          [-6.0065e-02,  1.4147e-03,  9.7281e-02],\n",
      "          [-6.0130e-02, -2.1725e-02,  3.6863e-02]],\n",
      "\n",
      "         [[ 2.8024e-02,  2.6183e-02, -2.3027e-02],\n",
      "          [ 5.1900e-02, -2.0588e-03, -1.0940e-01],\n",
      "          [-3.2729e-02, -6.2752e-03,  8.0630e-03]],\n",
      "\n",
      "         [[-1.8062e-02, -1.9510e-02,  4.3163e-02],\n",
      "          [ 4.6080e-02,  2.9494e-02,  4.0844e-02],\n",
      "          [ 5.9607e-03, -6.5891e-03, -6.4623e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2193e-02,  8.4653e-03,  3.6764e-03],\n",
      "          [ 1.7549e-02,  2.1971e-02, -4.5108e-03],\n",
      "          [ 2.1124e-02,  3.4591e-02, -1.6310e-02]],\n",
      "\n",
      "         [[ 3.8144e-02,  4.8395e-02, -9.5556e-02],\n",
      "          [ 1.8923e-02,  1.1341e-02, -7.6311e-02],\n",
      "          [ 4.7358e-03,  3.2138e-02, -7.4777e-02]],\n",
      "\n",
      "         [[-1.9031e-02, -3.2568e-02, -3.8251e-02],\n",
      "          [ 1.0705e-02,  2.3121e-03, -7.5078e-02],\n",
      "          [ 3.3316e-02,  3.5515e-02, -2.1023e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3330e-01,  7.4683e-02, -3.8624e-03],\n",
      "          [ 9.1377e-02,  8.2415e-02,  3.9469e-02],\n",
      "          [-1.8265e-02, -5.9943e-02,  8.9354e-02]],\n",
      "\n",
      "         [[ 1.5566e-02, -4.1716e-02,  1.0633e-02],\n",
      "          [ 7.2644e-03,  3.1934e-02,  1.2732e-03],\n",
      "          [-2.0851e-02, -3.7593e-03, -7.0170e-02]],\n",
      "\n",
      "         [[-6.6139e-02,  1.0627e-01,  1.9590e-02],\n",
      "          [ 5.4987e-02, -1.5552e-01, -1.8819e-02],\n",
      "          [-4.2554e-03,  4.4964e-02, -2.4632e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1691e-02, -4.5531e-02, -9.1721e-03],\n",
      "          [ 4.3995e-02,  4.5703e-02, -7.0108e-02],\n",
      "          [ 1.1388e-02,  4.4678e-02, -4.5953e-02]],\n",
      "\n",
      "         [[ 4.3432e-03,  2.3194e-02, -2.1895e-02],\n",
      "          [-8.0216e-02, -5.7606e-02, -9.8455e-03],\n",
      "          [-3.3285e-02, -1.1468e-01, -2.3779e-02]],\n",
      "\n",
      "         [[-6.3785e-02, -2.4485e-02, -4.9061e-02],\n",
      "          [-6.1594e-02,  1.0328e-01,  5.9685e-03],\n",
      "          [ 8.1863e-02, -3.0314e-02, -4.6373e-03]]]])\n",
      "36864\n"
     ]
    }
   ],
   "source": [
    "for i,p in enumerate(model.parameters()):\n",
    "    print(p)\n",
    "    print(p.numel())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding dropout: decrease accuracy \n",
    "class finetune(nn.Module):\n",
    "    def __init__(self, input_size, n_classes, d =0.35):\n",
    "        super(finetune, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn = nn.BatchNorm1d(256*2,eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256*2, n_classes)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(d)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sm(x) \n",
    "        return x \n",
    "\n",
    "def init_fine_tune_model():\n",
    "    return finetune(input_size=512, n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3055 Acc: 0.1002\n"
     ]
    }
   ],
   "source": [
    "init_fine_tune_model()\n",
    "test_accuracy, test_loss = calculate_mnist_test_accuracy(pretrained_resnet18, init_fine_tune_model(), mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Second: run using 2 linear with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple MLP\n",
    "def init_fine_tune_model(n_inputs=512, n_classes=10 ):\n",
    "    return nn.Sequential(\n",
    "                      nn.Linear(n_inputs, 256), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(256, n_classes),                   \n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3067 Acc: 0.0922\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_loss = calculate_mnist_test_accuracy(pretrained_resnet18, init_fine_tune_model(), mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) First run using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple MLP\n",
    "# Runned This \n",
    "def init_fine_tune_model(n_inputs=512, n_classes=10 ):\n",
    "    return nn.Sequential(\n",
    "                      nn.Linear(n_inputs, n_classes),                   \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "#test_accuracy, test_loss = calculate_mnist_test_accuracy(pretrained_resnet18, init_fine_tune_model(), mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3042 Acc: 0.0934\n"
     ]
    }
   ],
   "source": [
    "def init_fine_tune_model(n_inputs=512, n_classes=10 ):\n",
    "    return nn.Sequential(\n",
    "    nn.BatchNorm1d(n_inputs),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=n_inputs, out_features=2048),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, n_classes))\n",
    "test_accuracy, test_loss = calculate_mnist_test_accuracy(pretrained_resnet18, init_fine_tune_model(), mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class finetune_resnet(nn.Module):\n",
    "    def __init__(self, input_size, n_classes, block, inplanes=512, planes = 1024, blocks = 2, stride=1, norm_layer=None):\n",
    "        super(finetune_resnet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            self._norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        #self.conv1 = nn.Conv1d(512, 512, kernel_size = 1, stride=1, padding =3, bias=False)\n",
    "#         self.bn1 = self._norm_layer(inplanes)\n",
    "#         self.relu = nn.ReLU(inplace = True)\n",
    "            \n",
    "        self.layer5 = self._make_layer(block, inplanes, planes, blocks , stride=2) \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) ## new \n",
    "        self.fc = nn.Linear(planes * block.expansion, n_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "           # print(downsample)\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride,downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes, stride=1,downsample=None))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "#         #x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "def init_fine_tune_model():\n",
    "    return finetune_resnet(input_size= 512, n_classes=10, block=BasicBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet18_s = nn.Sequential(*list(pretrained_resnet18.children())[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3043 Acc: 0.1010\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_loss = calculate_mnist_test_accuracy(pretrained_resnet18_s, init_fine_tune_model(), mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B (Fine-tune (Frozen))\n",
    "\n",
    "The actual problem we care about solving likely has a different number of classes or is a different task altogether. Fine-tuning is the process of using the extracted representations (features) to solve this downstream task  (the task you're interested in).\n",
    "\n",
    "To illustrate this, we'll use our pretrained model (on Imagenet), to solve the MNIST classification task.\n",
    "\n",
    "There are two types of finetuning. \n",
    "\n",
    "#### 1. Frozen feature_extractor\n",
    "In the first type we pretrain with the FROZEN feature_extractor and NEVER unfreeze it during finetuning.\n",
    "\n",
    "\n",
    "#### 2. Unfrozen feature_extractor\n",
    "In the second, we finetune with a FROZEN feature_extractor for a few epochs, then unfreeze the feature extractor and finish training.\n",
    "\n",
    "\n",
    "In this part we will use the first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_multiple(feature_extractor, fine_tune_model, optimizer, criterion, dataloaders, num_epochs, frozen = True): \n",
    "    '''\n",
    "    This model takes input of multiple models and return trianing and validation losses \n",
    "    '''\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    best_acc = 0.0\n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 40)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase, dataloader in dataloaders.items():\n",
    "            if phase == 'train':\n",
    "                fine_tune_model.train()\n",
    "                feature_extractor.train()\n",
    "#                 if frozen == False:\n",
    "#                     feature_extractor.train() \n",
    "#                 else:\n",
    "#                     feature_extractor.eval()  \n",
    "            else:\n",
    "                feature_extractor.eval()   \n",
    "                fine_tune_model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # why is it necessary?? \n",
    "                inputs.requires_grad = True\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                     \n",
    "                    # Get model outputs and calculate loss\n",
    "                    intermediates = feature_extractor(inputs)\n",
    "                    # only if ResNet\n",
    "                    # intermediates= intermediates.view(intermediates.size()[0],intermediates.size()[1],1,1)\n",
    "                    outputs = fine_tune_model(intermediates)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.max(outputs, 1)[1]\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_ft_dict = copy.deepcopy(fine_tune_model.state_dict())\n",
    "                best_model_ft = fine_tune_model\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    print('Saving best model...')\n",
    "    torch.save({\n",
    "    'train_loss': train_acc_history,\n",
    "    'val_loss': val_acc_history,\n",
    "    'model_dict': best_model_ft_dict\n",
    "            }, './unfreeze_model_ft.pt')\n",
    "    \n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## abbrev version\n",
    "def FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, dataloaders, num_epochs, frozen = True):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    \n",
    "    return the fine_tune model\n",
    "    \"\"\"         \n",
    "    n_inputs = models.resnet18(pretrained=True).fc.in_features\n",
    "    n_classes = 10\n",
    "\n",
    "    # cpu/gpu\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # put model on device\n",
    "    fine_tune_model = fine_tune_model()\n",
    "    feature_extractor, fine_tune_model = feature_extractor.to(device), fine_tune_model.to(device)       \n",
    "    print('Training on', device)  \n",
    "    # params_to_update= check_grad(model_ft)  \n",
    "\n",
    "    optimizer = optim.SGD(fine_tune_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = optim.SGD(fine_tune_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print('Frozen feature extractor and train fine tune for 10 epochs on ', device) \n",
    "    \n",
    "    print('\\n Feature extractor model:')\n",
    "    check_grad(feature_extractor)\n",
    "    print('\\n Fine tune model:')\n",
    "    check_grad(fine_tune_model)\n",
    "    \n",
    "    best_model_ft, train_acc_history, val_acc_history = train_val_multiple(feature_extractor, \n",
    "                                                                     fine_tune_model, optimizer, \n",
    "                                                                     criterion, dataloaders,\n",
    "                                                                     num_epochs=10,frozen = True)    \n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, dataloaders, num_epochs, frozen = True):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    \n",
    "    return the fine_tune model\n",
    "    \"\"\"         \n",
    "    n_inputs = models.resnet18(pretrained=True).fc.in_features\n",
    "    n_classes = 10\n",
    "\n",
    "    # cpu/gpu\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # put model on device\n",
    "    fine_tune_model = fine_tune_model()\n",
    "    feature_extractor, fine_tune_model = feature_extractor.to(device), fine_tune_model.to(device)       \n",
    "    print('Training on', device)  \n",
    "    # params_to_update= check_grad(model_ft)\n",
    "    \n",
    "    ## starts to train, source: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "    \n",
    "    # record time \n",
    "    since = time.time()\n",
    "    # record val and train loss accuracy \n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    best_acc = 0.0\n",
    "    #optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "    optimizer = optim.SGD(fine_tune_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 40)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                feature_extractor.train()  \n",
    "                fine_tune_model.train()\n",
    "            else:\n",
    "                feature_extractor.eval()   \n",
    "                fine_tune_model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # why is it necessary?? \n",
    "                inputs.requires_grad = True\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                     \n",
    "                    # Get model outputs and calculate loss\n",
    "                    intermediates = feature_extractor(inputs)\n",
    "                    # only if ResNet\n",
    "                    #intermediates= intermediates.view(intermediates.size()[0],intermediates.size()[1],1,1)\n",
    "                    outputs = fine_tune_model(intermediates)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.max(outputs, 1)[1]\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_ft_dict = copy.deepcopy(fine_tune_model.state_dict())\n",
    "                best_model_ft = fine_tune_model\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    print('Saving best model...')\n",
    "    torch.save({\n",
    "    'train_loss': train_acc_history,\n",
    "    'val_loss': val_acc_history,\n",
    "    'model_dict': best_model_ft_dict\n",
    "            }, './frozen_model_ft.pt')\n",
    "    \n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Epoch 0/9\n",
      "----------------------------------------\n",
      "train Loss: 0.5477 Acc: 0.8709\n",
      "val Loss: 0.2761 Acc: 0.9342\n",
      "Epoch 1/9\n",
      "----------------------------------------\n",
      "train Loss: 0.2741 Acc: 0.9290\n",
      "val Loss: 0.2117 Acc: 0.9456\n",
      "Epoch 2/9\n",
      "----------------------------------------\n",
      "train Loss: 0.2306 Acc: 0.9371\n",
      "val Loss: 0.1849 Acc: 0.9514\n",
      "Epoch 3/9\n",
      "----------------------------------------\n",
      "train Loss: 0.2066 Acc: 0.9420\n",
      "val Loss: 0.1703 Acc: 0.9536\n",
      "Epoch 4/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1937 Acc: 0.9444\n",
      "val Loss: 0.1606 Acc: 0.9550\n",
      "Epoch 5/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1818 Acc: 0.9476\n",
      "val Loss: 0.1512 Acc: 0.9566\n",
      "Epoch 6/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1747 Acc: 0.9496\n",
      "val Loss: 0.1444 Acc: 0.9576\n",
      "Epoch 7/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1699 Acc: 0.9518\n",
      "val Loss: 0.1398 Acc: 0.9582\n",
      "Epoch 8/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1622 Acc: 0.9526\n",
      "val Loss: 0.1368 Acc: 0.9582\n",
      "Epoch 9/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1584 Acc: 0.9543\n",
      "val Loss: 0.1343 Acc: 0.9616\n",
      "Training complete in 75m 11s\n",
      "Best val Acc: 0.961600\n",
      "Saving best model...\n"
     ]
    }
   ],
   "source": [
    "# (1) A simple linear layer with softmax\n",
    "model_ft, train_acc, val_acc = FROZEN_fine_tune_mnist(\n",
    "    pretrained_resnet18, init_fine_tune_model, dataloaders, 10,frozen = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Epoch 0/9\n",
      "----------------------------------------\n",
      "train Loss: 0.6308 Acc: 0.8471\n",
      "val Loss: 0.2591 Acc: 0.9232\n",
      "Epoch 1/9\n",
      "----------------------------------------\n",
      "train Loss: 0.2376 Acc: 0.9290\n",
      "val Loss: 0.1742 Acc: 0.9478\n",
      "Epoch 2/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1925 Acc: 0.9409\n",
      "val Loss: 0.1501 Acc: 0.9546\n",
      "Epoch 3/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1706 Acc: 0.9471\n",
      "val Loss: 0.1368 Acc: 0.9580\n",
      "Epoch 4/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1580 Acc: 0.9507\n",
      "val Loss: 0.1335 Acc: 0.9586\n",
      "Epoch 5/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1485 Acc: 0.9534\n",
      "val Loss: 0.1233 Acc: 0.9610\n",
      "Epoch 6/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1418 Acc: 0.9555\n",
      "val Loss: 0.1146 Acc: 0.9624\n",
      "Epoch 7/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1343 Acc: 0.9580\n",
      "val Loss: 0.1134 Acc: 0.9644\n",
      "Epoch 8/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1304 Acc: 0.9588\n",
      "val Loss: 0.1060 Acc: 0.9640\n",
      "Epoch 9/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1234 Acc: 0.9617\n",
      "val Loss: 0.1067 Acc: 0.9652\n",
      "Training complete in 75m 11s\n",
      "Best val Acc: 0.965200\n",
      "Saving best model...\n"
     ]
    }
   ],
   "source": [
    "# (2) Trying to reproduce \n",
    "model_ft, train_acc, val_acc = FROZEN_fine_tune_mnist(\n",
    "    pretrained_resnet18, init_fine_tune_model, dataloaders, 10,frozen = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try abbrev version\n",
    "model_ft, train_acc, val_acc = FROZEN_fine_tune_mnist(\n",
    "    pretrained_resnet18, init_fine_tune_model, dataloaders, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Epoch 0/9\n",
      "----------------------------------------\n",
      "train Loss: 0.3161 Acc: 0.9074\n",
      "val Loss: 0.1537 Acc: 0.9544\n",
      "Epoch 1/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1949 Acc: 0.9394\n",
      "val Loss: 0.1290 Acc: 0.9616\n",
      "Epoch 2/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1769 Acc: 0.9438\n",
      "val Loss: 0.1201 Acc: 0.9620\n",
      "Epoch 3/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1705 Acc: 0.9455\n",
      "val Loss: 0.1137 Acc: 0.9640\n",
      "Epoch 4/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1637 Acc: 0.9483\n",
      "val Loss: 0.1095 Acc: 0.9668\n",
      "Epoch 5/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1601 Acc: 0.9489\n",
      "val Loss: 0.1056 Acc: 0.9672\n",
      "Epoch 6/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1585 Acc: 0.9484\n",
      "val Loss: 0.1072 Acc: 0.9650\n",
      "Epoch 7/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1561 Acc: 0.9505\n",
      "val Loss: 0.1027 Acc: 0.9670\n",
      "Epoch 8/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1532 Acc: 0.9509\n",
      "val Loss: 0.1000 Acc: 0.9674\n",
      "Epoch 9/9\n",
      "----------------------------------------\n",
      "train Loss: 0.1502 Acc: 0.9502\n",
      "val Loss: 0.1057 Acc: 0.9638\n",
      "Training complete in 82m 54s\n",
      "Best val Acc: 0.967400\n",
      "Saving best model...\n"
     ]
    }
   ],
   "source": [
    "# try some different results \n",
    "# forgot wtf this actually was......\n",
    "model_ft, train_acc, val_acc = FROZEN_fine_tune_mnist(\n",
    "    pretrained_resnet18, init_fine_tune_model, dataloaders, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C (compute test accuracy)\n",
    "Compute the test accuracy of fine-tuned model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mnist_test_accuracy(feature_extractor, fine_tune_model, mnist_test):\n",
    "    \n",
    "    test_correct = 0 # YOUR CODE HERE...\n",
    "    test_loss = 0\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    feature_extractor, fine_tune_model = feature_extractor.to(device), fine_tune_model.to(device)       \n",
    "\n",
    "    feature_extractor.eval()\n",
    "    fine_tune_model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for inputs, labels in mnist_test:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        intermediates = feature_extractor(inputs)\n",
    "#         intermediates= intermediates.view(intermediates.size()[0],intermediates.size()[1],1,1)\n",
    "        logits = fine_tune_model(intermediates)\n",
    "        outputs = F.softmax(logits, dim =1)\n",
    "        #preds = outputs.max(1, keepdim = True)[1]\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "        test_correct += preds.eq(labels.view_as(preds).to(device)).sum().item()\n",
    "\n",
    "    epoch_loss = test_loss / len(mnist_test.dataset)\n",
    "    epoch_acc = test_correct / len(mnist_test.dataset)\n",
    "    print('Test Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8469 Acc: 0.6338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6338, 1.8469313505172729)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single Liear layer + softmax with val > 0.96\n",
    "calculate_mnist_test_accuracy(pretrained_resnet18, model_ft, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8881 Acc: 0.5809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5809, 1.8880692113876343)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduce\n",
    "calculate_mnist_test_accuracy(pretrained_resnet18, model_ft, mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade!\n",
    "Let's see how you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-3f7a963ef48b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfrozen_test_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrade_mnist_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-3f7a963ef48b>\u001b[0m in \u001b[0;36mgrade_mnist_frozen\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# run the transfer learning routine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mFROZEN_fine_tune_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_resnet18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# calculate test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-da61146d312a>\u001b[0m in \u001b[0;36mFROZEN_fine_tune_mnist\u001b[0;34m(feature_extractor, fine_tune_model, dataloaders)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# first get the fine tune model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "def grade_mnist_frozen():\n",
    "    \n",
    "    # init a ft model\n",
    "    fine_tune_model = init_fine_tune_model()\n",
    "    \n",
    "    # run the transfer learning routine\n",
    "    model_ft, train_acc, val_acc = FROZEN_fine_tune_mnist(pretrained_resnet18, init_fine_tune_model, dataloaders, 10)\n",
    "    \n",
    "    # calculate test accuracy\n",
    "    test_accuracy = calculate_mnist_test_accuracy(pretrained_resnet18, model_ft, mnist_test, frozen = True)\n",
    "    \n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
    "    \n",
    "    return test_accuracy\n",
    "    \n",
    "frozen_test_accuracy = grade_mnist_frozen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D (Fine-tune Unfrozen)\n",
    "Now we'll learn how to train using the \"unfrozen\" approach.\n",
    "\n",
    "In this approach we'll:\n",
    "1. keep the feature_extract frozen for a few epochs (10)\n",
    "2. Unfreeze it.\n",
    "3. Finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val(model_ft, optimizer, criterion, dataloaders, num_epochs, save_location): \n",
    "    '''\n",
    "    This model takes input of multiple models and return trianing and validation losses \n",
    "    '''\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    best_acc = 0.0\n",
    "    since = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 40)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase, dataloader in dataloaders.items():\n",
    "            if phase == 'train':\n",
    "                model_ft.train()\n",
    "            else:\n",
    "                model_ft.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # why is it necessary?? \n",
    "                inputs.requires_grad = True\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                \n",
    "                    outputs = model_ft(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.max(outputs, 1)[1]\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_ft_dict = copy.deepcopy(model_ft.state_dict())\n",
    "                best_model_ft = model_ft\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    print('Saving best model...')\n",
    "    torch.save({\n",
    "    'train_loss': train_acc_history,\n",
    "    'val_loss': val_acc_history,\n",
    "    'model_dict': best_model_ft_dict\n",
    "            }, './{}.pt'.format(save_location))\n",
    "    \n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UNFROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, dataloaders, epochs = 10):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    \n",
    "    return the fine_tune model\n",
    "    \"\"\"     \n",
    "    # INSERT YOUR CODE:\n",
    "    n_inputs = models.resnet18(pretrained=True).fc.in_features\n",
    "    n_classes = 10\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    fine_tune_model = init_fine_tune_model()\n",
    "    feature_extractor, fine_tune_model = feature_extractor.to(device), fine_tune_model.to(device)         \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Frozen First \n",
    "    print('Frist freeze feature extractor and only train finetune model for 1 epochs on', device) \n",
    "    print('\\nOn feature extractor model')\n",
    "    check_grad(feature_extractor)\n",
    "    print('\\nOn fine tune model')\n",
    "    check_grad(fine_tune_model)\n",
    "    optimizer = optim.SGD(fine_tune_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    best_model_ft, train_acc_history, val_acc_history = train_val_multiple(feature_extractor, \n",
    "                                                                     fine_tune_model, optimizer, \n",
    "                                                                     criterion, dataloaders,\n",
    "                                                                     num_epochs=epochs,frozen = True)\n",
    "\n",
    "    # Train on the both models\n",
    "    print('Next unfreeze feature extractor and train both models for 1 epochs on', device) \n",
    "    unfreeze_model(feature_extractor)\n",
    "    feature_extractor.ft = best_model_ft # add finetune to last layer\n",
    "    check_grad(feature_extractor) # make sure all grads are now require_grad\n",
    "    optimizer = optim.SGD(feature_extractor.parameters(), lr=0.001, momentum=0.9) # update the trainable parameters\n",
    "    best_model_ft, train_acc_history, val_acc_history = train_val_single(feature_extractor, \n",
    "                                                                     optimizer, \n",
    "                                                                     criterion, \n",
    "                                                                     dataloaders,\n",
    "                                                                     num_epochs= epochs)\n",
    "\n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UNFROZEN_fine_tune_mnist() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-0a210d94e672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mUNFROZEN_fine_tune_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_resnet18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fine_tune_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: UNFROZEN_fine_tune_mnist() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "test_loader = dict()\n",
    "test_loader['test'] = mnist_test\n",
    "\n",
    "UNFROZEN_fine_tune_mnist(pretrained_resnet18, init_fine_tune_model, test_loader, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade UNFROZEN\n",
    "Let's see if there's a difference in accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frist freeze feature extractor and only train finetune model for 1 epochs on cuda:0\n",
      "\n",
      " On feature extractor model\n",
      "Parameters to learn:\n",
      "11,176,512 total parameters.\n",
      "0 training parameters.\n",
      "\n",
      " On fine tune model\n",
      "Parameters to learn:\n",
      "\t bn.weight\n",
      "\t bn.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "12,298 total parameters.\n",
      "12,298 training parameters.\n",
      "Epoch 0/9\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fe3f117e3351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0munfrozen_test_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrade_mnist_unfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-fe3f117e3351>\u001b[0m in \u001b[0;36mgrade_mnist_unfrozen\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# run the transfer learning routine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     best_model_ft, train_acc_history, val_acc_history = UNFROZEN_fine_tune_mnist(pretrained_resnet18, \n\u001b[0;32m----> 8\u001b[0;31m                                                                                fine_tune_model, dataloaders)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# calculate test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-9ef9ad935da9>\u001b[0m in \u001b[0;36mUNFROZEN_fine_tune_mnist\u001b[0;34m(feature_extractor, fine_tune_model, dataloaders)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                                      \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                                      \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                                                      num_epochs=10,frozen = True)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-795215111698>\u001b[0m in \u001b[0;36mtrain_val_fn\u001b[0;34m(feature_extractor, fine_tune_model, optimizer, criterion, dataloaders, num_epochs, frozen)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def grade_mnist_unfrozen():\n",
    "    \n",
    "    # init a ft model\n",
    "    fine_tune_model = init_fine_tune_model()\n",
    "    \n",
    "    # run the transfer learning routine\n",
    "    best_model_ft, train_acc_history, val_acc_history = UNFROZEN_fine_tune_mnist(pretrained_resnet18, \n",
    "                                                                               fine_tune_model, dataloaders)\n",
    "    \n",
    "    fine_tune_model.load_dict(unfreeze_model_ft['model_dict'])\n",
    "    # calculate test accuracy\n",
    "    test_accuracy, test_loss = calculate_mnist_test_accuracy(best_model_ft, _, mnist_test, frozen = False)\n",
    "    \n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
    "    \n",
    "    return test_accuracy\n",
    "    \n",
    "unfrozen_test_accuracy = grade_mnist_unfrozen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert unfrozen_test_accuracy > frozen_test_accuracy, 'the unfrozen model should be better'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_tune_mnist(feature_extractor, fine_tune_model, dataloaders, num_epochs, frozen = True, save_point):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    use frozen to train feature_extractor or not train the feature extractor \n",
    "    return the fine_tune model\n",
    "    \"\"\"         \n",
    "    n_inputs = models.resnet18(pretrained=True).fc.in_features\n",
    "    n_classes = 10\n",
    "\n",
    "    # cpu/gpu\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # put model on device\n",
    "    feature_extractor.train()\n",
    "    fine_tune_model = fine_tune_model()\n",
    "    if frozen: # keep feature extractor frozen\n",
    "        freeze_model(feature_extractor)\n",
    "    else: # train entire model\n",
    "        unfreeze_model(feature_extractor)\n",
    "    \n",
    "    feature_extractor.fc = fine_tune_model\n",
    "    feature_extractor = feature_extractor.to(device)\n",
    "    parameters_to_train = checkgrad(feature_extractor)\n",
    "    print('Training on', device)  \n",
    "  \n",
    "    optimizer = optim.SGD(parameters_to_train, lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_model_ft, train_acc_history, val_acc_history = train_val(feature_extractor, \n",
    "                                                                  optimizer, criterion, \n",
    "                                                                  dataloaders, num_epochs,\n",
    "                                                                  save_point)\n",
    "    \n",
    "    return best_model_ft, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "_, _, _, = fine_tune_mnist(pretrained_resnet18, init_fine_tune_model, dataloaders, 2, frozen = True, 'test_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, = fine_tune_mnist(pretrained_resnet18, init_fine_tune_model, dataloaders, 2, frozen = False, 'test_false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Question 2 (train a model on Wikitext-2)\n",
    "\n",
    "Here we'll apply what we just learned to NLP. In this section we'll make our own feature extractor and pretrain it on Wikitext-2.\n",
    "\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\n",
    "\n",
    "#### Part A\n",
    "In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "\n",
    "def init_wikitext_dataset():\n",
    "    \"\"\"\n",
    "    Fill in the details\n",
    "    \"\"\"\n",
    "    wikitext_val = None # YOUR CODE HERE\n",
    "    wikitext_train = None # YOUR CODE HERE\n",
    "    wikitext_test = None # YOUR CODE HERE\n",
    "    \n",
    "    return wikitext_train, wikitext_val, wikitext_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B   \n",
    "Here we design our own feature extractor. In MNIST that was a resnet because we were dealing with images. Now we need to pick a model that can model sequences better. Design an RNN-based model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_feature_extractor():\n",
    "    \n",
    "    feature_extractor = None #  YOUR CODE\n",
    "    \n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Pretrain the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val):\n",
    "    # FILL IN THE DETAILS\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Calculate the test perplexity on wikitext2. Feel free to recycle code from previous assignments from this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_wiki2_test_perplexity(feature_extractor, wikitext_test):\n",
    "    \n",
    "    # FILL IN DETAILS\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's grade your results!\n",
    "(don't touch this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grade_wikitext2():\n",
    "    # load data\n",
    "    wikitext_train, wikitext_val, wikitext_test = init_wikitext_dataset()\n",
    "\n",
    "    # load feature extractor\n",
    "    feature_extractor = init_feature_extractor()\n",
    "\n",
    "    # pretrain using the feature extractor\n",
    "    fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_ppl = calculate_wiki2_test_perplexity(feature_extractor, wikitext_test)\n",
    "\n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_ppl < 10000, 'ummm... your perplexity is too high...'\n",
    "    \n",
    "grade_wikitext2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "## Question 3 (fine-tune on MNLI)\n",
    "In this question you will use your feature_extractor from question 2\n",
    "to fine-tune on MNLI.\n",
    "\n",
    "(From the website):\n",
    "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n",
    "\n",
    "MNLI has 3 genres (3 classes).\n",
    "The goal of this question is to maximize the test accuracy in MNLI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import MultiNLI\n",
    "\n",
    "def init_mnli_dataset():\n",
    "    \"\"\"\n",
    "    Fill in the details\n",
    "    \"\"\"\n",
    "    mnli_val = None # TODO\n",
    "    mnli_train = None # TODO\n",
    "    mnli_test = None # TODO\n",
    "    \n",
    "    return mnli_train, mnli_val, mnli_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Here we again design a model for finetuning. Use the output of your feature-extractor as the input to this model. This should be a powerful classifier (up to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_finetune_model():\n",
    "    \n",
    "    # TODO FILL IN THE DETAILS\n",
    "    fine_tune_model = ...\n",
    "    \n",
    "    return fine_tune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "Use the feature_extractor and your fine_tune_model to fine_tune MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "Evaluate the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mnli_test_accuracy(feature_extractor, fine_tune_model, mnli_test):\n",
    "    \n",
    "    # YOUR CODE HERE...\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's grade your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grade_mnli():\n",
    "    # load data\n",
    "    mnli_train, mnli_val, mnli_test = init_mnli_dataset()\n",
    "\n",
    "    # no need to load feature extractor because it is fine-tuned\n",
    "    feature_extractor = feature_extractor\n",
    "\n",
    "    # init the fine_tune model\n",
    "    fine_tune_model = init_finetune_model()\n",
    "    \n",
    "    # finetune\n",
    "    fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n",
    "\n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_ppl > 0.00, 'ummm... your accuracy is too low...'\n",
    "    \n",
    "grade_mnli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "### Question 4 (BERT)\n",
    "\n",
    "A major direction in research came from a model called BERT, released last year.  \n",
    "\n",
    "In this question you'll use BERT as your feature_extractor instead of the model you\n",
    "designed yourself.\n",
    "\n",
    "To get BERT, head on over to (https://github.com/huggingface/transformers) and load your BERT model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A (init BERT)\n",
    "In this section you need to create an instance of BERT and return if from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "def init_bert():\n",
    "    \n",
    "    BERT = None # ... YOUR CODE HERE\n",
    "    \n",
    "    return BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B (fine-tune with BERT)\n",
    "\n",
    "Use BERT as your feature extractor to finetune MNLI. Use a new finetune model (reset weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_tune_mnli_BERT(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "Evaluate how well we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mnli_test_accuracy_BERT(feature_extractor, fine_tune_model, mnli_test):\n",
    "    \n",
    "    # YOUR CODE HERE...\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's grade your BERT results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grade_mnli_BERT():\n",
    "    BERT_feature_extractor = init_bert()\n",
    "    \n",
    "    # load data\n",
    "    mnli_train, mnli_val, mnli_test = init_mnli_dataset()\n",
    "\n",
    "    # init the fine_tune model\n",
    "    fine_tune_model = init_finetune_model()\n",
    "    \n",
    "    # finetune\n",
    "    fine_tune_mnli(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n",
    "    \n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_ppl > 0.0, 'ummm... your accuracy is too low...'\n",
    "    \n",
    "grade_mnli_BERT()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
