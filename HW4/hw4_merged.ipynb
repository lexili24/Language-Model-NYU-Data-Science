{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drhZPAoRbIiF"
   },
   "source": [
    "# Homework 4: Conversation Modeling and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T01:09:47.705364Z",
     "start_time": "2019-11-13T01:09:46.892963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "b-sdmiwWWIud"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jy2p9OqRWNOm"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hHSF2IQst2qO"
   },
   "outputs": [],
   "source": [
    "from utils import ChatDictionary, ChatDataset, pad_tensor, argsort, batchify, _HypothesisTail, reorder_encoder_states, reorder_decoder_incremental_state, get_nbest_list_from_beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCcRr49JbIiH"
   },
   "source": [
    "# Part 1 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-J6pfDBbIiJ"
   },
   "source": [
    "## 1.1 Attention visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dTr1RmhhbIiL"
   },
   "outputs": [],
   "source": [
    "### set up the model and complete the corresponding task\n",
    "\n",
    "### the pretrained model was trained in ~2 hours, i.e. you can expect attention maps\n",
    "### to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6zydA1TbIiQ"
   },
   "source": [
    "### You present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I6R5ESlvbIiR"
   },
   "outputs": [],
   "source": [
    "# this is some example attention map here, \n",
    "# *make sure you add text tokens on the axes to make it readable!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vk1LNBZbIiV"
   },
   "source": [
    "![Imgur](https://i.imgur.com/xodciCU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKPdfiasbIiW"
   },
   "source": [
    "## 1.2 Encoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6agDLBMcOMNZ"
   },
   "outputs": [],
   "source": [
    "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfHewoRTnUWm",
    "outputId": "4d21a809-8926-4de6-b0a8-0d9cd60d5baa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131438/131438 [00:16<00:00, 7867.96it/s]\n",
      "100%|██████████| 7801/7801 [00:01<00:00, 6134.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "### persona chat dataset\n",
    "if not os.path.exists('./dict'):\n",
    "    !wget \"https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\" -O './dict'\n",
    "if not os.path.exists('./train.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\" -O './train.jsonl'\n",
    "if not os.path.exists('./valid.jsonl'):\n",
    "    !wget \"https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\" -O './valid.jsonl'\n",
    "\n",
    "if not os.path.exists('./chat_model_best_22.pt'):\n",
    "    !wget \"https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\" -O './chat_model_best_22.pt'\n",
    "\n",
    "chat_dict = ChatDictionary('./dict')\n",
    "train_dataset = ChatDataset('./train.jsonl', chat_dict)\n",
    "valid_dataset = ChatDataset('./valid.jsonl', chat_dict, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NjMy68c7nVHJ"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=64)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Unu5CAhBbIiX"
   },
   "outputs": [],
   "source": [
    "### add transformer encoder as optional encoder in seq2seq model.\n",
    "\n",
    "# code below can help you to start it, but feel free to start from scratch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Encodes the input context.\"\"\"\n",
    "\n",
    "    # def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n",
    "    #     super().__init__()\n",
    "    #     self.vocab_size = vocab_size\n",
    "    #     self.embed_size = embed_size\n",
    "    #     self.hidden_size = hidden_size\n",
    "    #     self.num_layers = num_layers\n",
    "    #     self.dropout = nn.Dropout(p=dropout)\n",
    "    #     self.pad_idx = pad_idx\n",
    "        \n",
    "    #     if shared_lt is None:\n",
    "    #         self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n",
    "    #     else:\n",
    "    #         self.embedding = shared_lt\n",
    "            \n",
    "    #     self.gru = nn.GRU(\n",
    "    #         self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "    #     )\n",
    "        \n",
    "        \n",
    "    # def forward(self, text_vec, text_lens, hidden=None, use_packed=True):\n",
    "    #     embedded = self.embedding(text_vec)\n",
    "    #     attention_mask = text_vec.ne(self.pad_idx)\n",
    "\n",
    "    #     embedded = self.dropout(embedded)\n",
    "    #     if use_packed is True:\n",
    "    #         embedded = pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
    "    #     output, hidden = self.gru(embedded, hidden)\n",
    "    #     if use_packed is True:\n",
    "    #         output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "    #     return output, hidden, attention_mask\n",
    "\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout, num_layers,shared_lt, max_len = 7500, nhead=4, pad_idx= 0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.position_embed = nn.Embedding(num_embeddings = self.max_len, embedding_dim = self.embed_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(self.embed_size, self.nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, self.num_layers)\n",
    "        self.token_embed = shared_lt\n",
    "        self.dropout = nn.Dropout(p = self.dropout)\n",
    "\n",
    "    def _generate_mask(self, sentence_len):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (sentence_len, sentence_len)\n",
    "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "        return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "    def forward(self, text_vec, text_lens, hidden=None, use_packed=True): \n",
    "        # text_vec: batch x length \n",
    "        pos = torch.arange(text_vec.size(1), device = text_vec.device).expand(text_vec.size(0),-1).view(-1, text_vec.size(1))\n",
    "        pos_embedded = self.position_embed(pos)\n",
    "        tok_embedded = self.token_embed(text_vec)\n",
    "        embedded = pos_embedded + tok_embedded  # apply pos embedding\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attention_mask_pos = self._generate_mask(text_vec.size(1))\n",
    "        attention_mask_pad = text_vec.eq(self.pad_idx)\n",
    "        \n",
    "        output = self.transformer(embedded.transpose(0,1), \n",
    "                                  src_key_padding_mask=attention_mask_pad.to(text_vec.device),\n",
    "                                  mask = attention_mask_pos.to(text_vec.device))\n",
    "        output.transpose_(0,1)\n",
    "        #print('output shape', output.size())\n",
    "        hidden_encoder = torch.mean(output, dim = 1, keepdim=True).transpose(0,1).expand(2,-1,-1).contiguous() # 2 because decoder num_layers \n",
    "        #print('hidden layer shape', hidden_encoder.size())\n",
    "        return output, hidden_encoder, attention_mask_pad\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.longest_label = 100\n",
    "\n",
    "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
    "        emb = self.embedding(text_vec)\n",
    "        emb = self.dropout(emb)\n",
    "        seqlen = text_vec.size(1)\n",
    "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "        \n",
    "        decoder_hidden = decoder_hidden\n",
    "        output = []\n",
    "        attn_w_log = []\n",
    "\n",
    "        for i in range(seqlen):\n",
    "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
    "            \n",
    "            # compute attention at each time step\n",
    "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
    "            output.append(decoder_output_attended)\n",
    "            attn_w_log.append(attn_weights)\n",
    "            \n",
    "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
    "        scores = self.out(output)\n",
    "        \n",
    "\n",
    "        return scores, decoder_hidden, attn_w_log\n",
    "    \n",
    "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
    "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "        \n",
    "        batch_size = ys.size(0)\n",
    "        target_length = ys.size(1)\n",
    "        longest_label = max(target_length, self.longest_label)\n",
    "        \n",
    "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "        \n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
    "        decoder_input = torch.cat([starts, y_in], 1)\n",
    "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
    "        _, preds = decoder_output.max(dim=2)\n",
    "        \n",
    "        return decoder_output, preds, attn_w_log\n",
    "    \n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        input_dim = hidden_size\n",
    "\n",
    "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
    "\n",
    "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
    "\n",
    "        encoder_output_t = encoder_output.transpose(1,2)\n",
    "        \n",
    "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
    "\n",
    "        attention_scores.masked_fill_((attention_mask), -10e5)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
    "\n",
    "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
    "\n",
    "        output = self.linear_out(combined).unsqueeze(1)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "class seq2seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic seq2seq model with attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, opts):\n",
    "\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        \n",
    "        self.decoder = DecoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_dec'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                )\n",
    "        \n",
    "        self.encoder = EncoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_enc'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                    shared_lt=self.decoder.embedding,\n",
    "                                    # ### Transformer\n",
    "                                    # max_len = self.opts['max_len'],\n",
    "                                    # nhead = self.opts['nhead'],\n",
    "    \n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5thvHRqpbIih"
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "load_pretrained = False\n",
    "    \n",
    "if load_pretrained is True:\n",
    "    if current_device == 'cuda':\n",
    "        model_pt = torch.load('./transformer_best_6.pt')\n",
    "    else:\n",
    "        model_pt = torch.load('./transformer_best_6.pt', map_location=torch.device('cpu'))\n",
    "    opts = model_pt['opts']\n",
    "    \n",
    "    model = seq2seq(opts)\n",
    "    model.load_state_dict(model_pt['state_dict'])\n",
    "    model.to(current_device)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    opts = {}\n",
    "\n",
    "    opts['vocab_size'] = len(chat_dict)\n",
    "    opts['hidden_size'] = 256\n",
    "    opts['embedding_size'] = 256\n",
    "    opts['num_layers_enc'] = 2\n",
    "    opts['num_layers_dec'] = 2\n",
    "    opts['dropout'] = 0.3\n",
    "    opts['encoder_shared_lt'] = True\n",
    "    \n",
    "    ### Transformer \n",
    "    # opts['max_len'] = 7500\n",
    "    # opts['nhead'] = 4\n",
    "    \n",
    "    \n",
    "    model = seq2seq(opts)\n",
    "    model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EqH2fQYOnpUj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 0.001, amsgrad=True, weight_decay = 0.01) # AdamW is Adam + weight decay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AAVeA9MdbIic"
   },
   "outputs": [],
   "source": [
    "# check pdf to see what you expected to present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-Hl2YnSK8cT7",
    "outputId": "be866175-013c-4a93-e123-10f5ce9934df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 train loss = 9.861941372112984\n",
      "iter 100 train loss = 5.971185261298769\n",
      "iter 200 train loss = 5.518927177815331\n",
      "iter 300 train loss = 5.232503547691666\n",
      "iter 400 train loss = 5.037795903197501\n",
      "iter 500 train loss = 4.89660920616663\n",
      "iter 600 train loss = 4.794661637683474\n",
      "iter 700 train loss = 4.715725839428522\n",
      "iter 800 train loss = 4.647014741135447\n",
      "iter 900 train loss = 4.588638307202965\n",
      "iter 1000 train loss = 4.538578561223573\n",
      "iter 1100 train loss = 4.494452043789565\n",
      "iter 1200 train loss = 4.456141659788502\n",
      "iter 1300 train loss = 4.422082164415175\n",
      "iter 1400 train loss = 4.391696713252875\n",
      "iter 1500 train loss = 4.364642817884551\n",
      "iter 1600 train loss = 4.339133131268906\n",
      "iter 1700 train loss = 4.316718772028358\n",
      "iter 1800 train loss = 4.295627925188392\n",
      "iter 1900 train loss = 4.277680073069002\n",
      "iter 2000 train loss = 4.260150919788498\n",
      "Epoch 0 valid loss = 3.9511326420934987\n",
      "Epoch 0 valid perp = 51.994224343403474\n",
      "iter 0 train loss = 3.661064808461418\n",
      "iter 100 train loss = 3.79219581114524\n",
      "iter 200 train loss = 3.797137572535309\n",
      "iter 300 train loss = 3.8077251381942023\n",
      "iter 400 train loss = 3.8127995275398194\n",
      "iter 500 train loss = 3.810740158944345\n",
      "iter 600 train loss = 3.8078466288456587\n",
      "iter 700 train loss = 3.807201105042925\n",
      "iter 800 train loss = 3.805439294958069\n",
      "iter 900 train loss = 3.8071728782069965\n",
      "iter 1000 train loss = 3.803565284200013\n",
      "iter 1100 train loss = 3.800678395698173\n",
      "iter 1200 train loss = 3.7982684804049\n",
      "iter 1300 train loss = 3.797866093426509\n",
      "iter 1400 train loss = 3.7935756712568973\n",
      "iter 1500 train loss = 3.7919804704823226\n",
      "iter 1600 train loss = 3.7883551952391117\n",
      "iter 1700 train loss = 3.786238747979665\n",
      "iter 1800 train loss = 3.782681503644835\n",
      "iter 1900 train loss = 3.780899341058152\n",
      "iter 2000 train loss = 3.778133183988608\n",
      "Epoch 1 valid loss = 3.8277243195150428\n",
      "Epoch 1 valid perp = 45.95783379697579\n",
      "iter 0 train loss = 3.6891080217465237\n",
      "iter 100 train loss = 3.619393023240908\n",
      "iter 200 train loss = 3.6313283635748625\n",
      "iter 300 train loss = 3.6329529374623553\n",
      "iter 400 train loss = 3.6322597145480042\n",
      "iter 500 train loss = 3.633747956976782\n",
      "iter 600 train loss = 3.635258791165229\n",
      "iter 700 train loss = 3.637493194012284\n",
      "iter 800 train loss = 3.6375490014052847\n",
      "iter 900 train loss = 3.6385982247412145\n",
      "iter 1000 train loss = 3.636911374802672\n",
      "iter 1100 train loss = 3.6366315436520393\n",
      "iter 1200 train loss = 3.6370958708073178\n",
      "iter 1300 train loss = 3.6366834958188403\n",
      "iter 1400 train loss = 3.635880817214233\n",
      "iter 1500 train loss = 3.6373070977421804\n",
      "iter 1600 train loss = 3.6359134305923995\n",
      "iter 1700 train loss = 3.6352539895438807\n",
      "iter 1800 train loss = 3.6340109439141854\n",
      "iter 1900 train loss = 3.634611351981808\n",
      "iter 2000 train loss = 3.6352744113239566\n",
      "Epoch 2 valid loss = 3.780840010966071\n",
      "Epoch 2 valid perp = 43.85286315415422\n",
      "iter 0 train loss = 3.5945316260831195\n",
      "iter 100 train loss = 3.5473900726245526\n",
      "iter 200 train loss = 3.5436034014624265\n",
      "iter 300 train loss = 3.5377027855527223\n",
      "iter 400 train loss = 3.5351164584534223\n",
      "iter 500 train loss = 3.535802937232092\n",
      "iter 600 train loss = 3.534239872228419\n",
      "iter 700 train loss = 3.539206131925015\n",
      "iter 800 train loss = 3.5442322037446448\n",
      "iter 900 train loss = 3.5473828700759533\n",
      "iter 1000 train loss = 3.5488750105663276\n",
      "iter 1100 train loss = 3.5495884707363694\n",
      "iter 1200 train loss = 3.548869391621269\n",
      "iter 1300 train loss = 3.5475846589879994\n",
      "iter 1400 train loss = 3.5495211952006653\n",
      "iter 1500 train loss = 3.54995261773584\n",
      "iter 1600 train loss = 3.550129967752091\n",
      "iter 1700 train loss = 3.550884720065127\n",
      "iter 1800 train loss = 3.5511627220359885\n",
      "iter 1900 train loss = 3.551068143266535\n",
      "iter 2000 train loss = 3.550597156384468\n",
      "Epoch 3 valid loss = 3.755668908669472\n",
      "Epoch 3 valid perp = 42.76281466327305\n",
      "iter 0 train loss = 3.3213329068758073\n",
      "iter 100 train loss = 3.451698076574376\n",
      "iter 200 train loss = 3.451990340300859\n",
      "iter 300 train loss = 3.4646378077412825\n",
      "iter 400 train loss = 3.4686715209405232\n",
      "iter 500 train loss = 3.4705839010657926\n",
      "iter 600 train loss = 3.4712959437499373\n",
      "iter 700 train loss = 3.47388578073015\n",
      "iter 800 train loss = 3.4773450237810204\n",
      "iter 900 train loss = 3.478793279543248\n",
      "iter 1000 train loss = 3.481148896387464\n",
      "iter 1100 train loss = 3.4837420593799213\n",
      "iter 1200 train loss = 3.485817832555723\n",
      "iter 1300 train loss = 3.4875974555536233\n",
      "iter 1400 train loss = 3.4869661890075445\n",
      "iter 1500 train loss = 3.488207369416177\n",
      "iter 1600 train loss = 3.48806438000313\n",
      "iter 1700 train loss = 3.488616283361426\n",
      "iter 1800 train loss = 3.4896946473549884\n",
      "iter 1900 train loss = 3.490059478043827\n",
      "iter 2000 train loss = 3.4903172237238898\n",
      "Epoch 4 valid loss = 3.7488694312168795\n",
      "Epoch 4 valid perp = 42.4730361568119\n",
      "iter 0 train loss = 3.162897368526215\n",
      "iter 100 train loss = 3.4345736426835956\n",
      "iter 200 train loss = 3.4122088075005554\n",
      "iter 300 train loss = 3.4186760443113644\n",
      "iter 400 train loss = 3.424133408791792\n",
      "iter 500 train loss = 3.42171596821025\n",
      "iter 600 train loss = 3.4238627108879247\n",
      "iter 700 train loss = 3.4269826526867955\n",
      "iter 800 train loss = 3.4306272267717404\n",
      "iter 900 train loss = 3.433383997161134\n",
      "iter 1000 train loss = 3.433495862024065\n",
      "iter 1100 train loss = 3.435049029989343\n",
      "iter 1200 train loss = 3.435602806379697\n",
      "iter 1300 train loss = 3.435347424097322\n",
      "iter 1400 train loss = 3.437301831785824\n",
      "iter 1500 train loss = 3.440409224075702\n",
      "iter 1600 train loss = 3.4411915379651403\n",
      "iter 1700 train loss = 3.4419109436619384\n",
      "iter 1800 train loss = 3.4426934408771324\n",
      "iter 1900 train loss = 3.4435428736161784\n",
      "iter 2000 train loss = 3.4430083945302616\n",
      "Epoch 5 valid loss = 3.743335578614149\n",
      "Epoch 5 valid perp = 42.23864577419877\n",
      "iter 0 train loss = 3.3621896224435286\n",
      "iter 100 train loss = 3.375299558492519\n",
      "iter 200 train loss = 3.365478760232737\n",
      "iter 300 train loss = 3.3691486327266222\n",
      "iter 400 train loss = 3.3760970044204215\n",
      "iter 500 train loss = 3.3784333453344932\n",
      "iter 600 train loss = 3.377928637977744\n",
      "iter 700 train loss = 3.3826654266846625\n",
      "iter 800 train loss = 3.3829383596671314\n",
      "iter 900 train loss = 3.387180048652397\n",
      "iter 1000 train loss = 3.3876703096098746\n",
      "iter 1100 train loss = 3.390853001167653\n",
      "iter 1200 train loss = 3.3922945453369526\n",
      "iter 1300 train loss = 3.3936658415464347\n",
      "iter 1400 train loss = 3.393405570192262\n",
      "iter 1500 train loss = 3.3963927140863928\n",
      "iter 1600 train loss = 3.3977412963466755\n",
      "iter 1700 train loss = 3.400305890110266\n",
      "iter 1800 train loss = 3.4015010149253704\n",
      "iter 1900 train loss = 3.4030204959335553\n",
      "iter 2000 train loss = 3.403752988860368\n",
      "Epoch 6 valid loss = 3.7371922365688146\n",
      "Epoch 6 valid perp = 41.97995475298662\n",
      "iter 0 train loss = 3.159488357376675\n",
      "iter 100 train loss = 3.326591837220535\n",
      "iter 200 train loss = 3.333878426625121\n",
      "iter 300 train loss = 3.339892253671904\n",
      "iter 400 train loss = 3.3384286977818705\n",
      "iter 500 train loss = 3.3462937299898563\n",
      "iter 600 train loss = 3.347557428892856\n",
      "iter 700 train loss = 3.3518225966300865\n",
      "iter 800 train loss = 3.3549043498492437\n",
      "iter 900 train loss = 3.357866122848381\n",
      "iter 1000 train loss = 3.358451910567447\n",
      "iter 1100 train loss = 3.3605440618804177\n",
      "iter 1200 train loss = 3.360508370248\n",
      "iter 1300 train loss = 3.3604852883380616\n",
      "iter 1400 train loss = 3.3636294128027613\n",
      "iter 1500 train loss = 3.365315696197165\n",
      "iter 1600 train loss = 3.3674440974785242\n",
      "iter 1700 train loss = 3.3675765049681985\n",
      "iter 1800 train loss = 3.369278907055418\n",
      "iter 1900 train loss = 3.371360374142818\n",
      "iter 2000 train loss = 3.37257626965718\n",
      "Epoch 7 valid loss = 3.73402887117779\n",
      "Epoch 7 valid perp = 41.847366639894446\n",
      "iter 0 train loss = 3.391564927095545\n",
      "iter 100 train loss = 3.3157259437753837\n",
      "iter 200 train loss = 3.3122307282936885\n",
      "iter 300 train loss = 3.312173118931306\n",
      "iter 400 train loss = 3.314913732450217\n",
      "iter 500 train loss = 3.3167904266250967\n",
      "iter 600 train loss = 3.3171562148903875\n",
      "iter 700 train loss = 3.3160422609587945\n",
      "iter 800 train loss = 3.31839283713703\n",
      "iter 900 train loss = 3.318704376523764\n",
      "iter 1000 train loss = 3.3220987651737515\n",
      "iter 1100 train loss = 3.323162178975778\n",
      "iter 1200 train loss = 3.32449912808175\n",
      "iter 1300 train loss = 3.32792708534873\n",
      "iter 1400 train loss = 3.3316905855191776\n",
      "iter 1500 train loss = 3.333377087127709\n",
      "iter 1600 train loss = 3.3362603760697485\n",
      "iter 1700 train loss = 3.337526209080015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1800 train loss = 3.3386566210260376\n",
      "iter 1900 train loss = 3.3400878596757853\n",
      "iter 2000 train loss = 3.341656234411288\n",
      "Epoch 8 valid loss = 3.7373228119580024\n",
      "Epoch 8 valid perp = 41.98543665980985\n",
      "iter 0 train loss = 3.054618942086245\n",
      "iter 100 train loss = 3.2877651360570983\n",
      "iter 200 train loss = 3.2891091297497383\n",
      "iter 300 train loss = 3.2933186650195734\n",
      "iter 400 train loss = 3.293524154547039\n",
      "iter 500 train loss = 3.2955450623035443\n",
      "iter 600 train loss = 3.292615239387801\n",
      "iter 700 train loss = 3.2958825643138145\n",
      "iter 800 train loss = 3.300872972768649\n",
      "iter 900 train loss = 3.30015292955547\n",
      "iter 1000 train loss = 3.301979589005867\n",
      "iter 1100 train loss = 3.3041936825103364\n",
      "iter 1200 train loss = 3.3062844332276926\n",
      "iter 1300 train loss = 3.307355339592819\n",
      "iter 1400 train loss = 3.3095116131856703\n",
      "iter 1500 train loss = 3.3123905163767455\n",
      "iter 1600 train loss = 3.313059813067537\n",
      "iter 1700 train loss = 3.3143826449986196\n",
      "iter 1800 train loss = 3.3153266774506447\n",
      "iter 1900 train loss = 3.315940180946495\n",
      "iter 2000 train loss = 3.3163565308605314\n",
      "Epoch 9 valid loss = 3.7382085199234476\n",
      "Epoch 9 valid perp = 42.02263996869355\n",
      "iter 0 train loss = 3.2108067304238506\n",
      "iter 100 train loss = 3.2297238844620804\n",
      "iter 200 train loss = 3.2464023388429175\n",
      "iter 300 train loss = 3.2469543931525835\n",
      "iter 400 train loss = 3.2478271217386987\n",
      "iter 500 train loss = 3.2538899942804393\n",
      "iter 600 train loss = 3.2621058641331886\n",
      "iter 700 train loss = 3.2632635273309765\n",
      "iter 800 train loss = 3.2676905720804594\n",
      "iter 900 train loss = 3.272517335276055\n",
      "iter 1000 train loss = 3.2744047299456733\n",
      "iter 1100 train loss = 3.277587981903362\n",
      "iter 1200 train loss = 3.2786463053311907\n",
      "iter 1300 train loss = 3.2787535644836234\n",
      "iter 1400 train loss = 3.280710345282782\n",
      "iter 1500 train loss = 3.2851662347625408\n",
      "iter 1600 train loss = 3.2866084088884215\n",
      "iter 1700 train loss = 3.28799860435907\n",
      "iter 1800 train loss = 3.2902977431460987\n",
      "iter 1900 train loss = 3.2915964598712297\n",
      "iter 2000 train loss = 3.2932740709676462\n",
      "Epoch 10 valid loss = 3.735577864867344\n",
      "Epoch 10 valid perp = 41.91223817657362\n",
      "iter 0 train loss = 3.208958249250461\n",
      "iter 100 train loss = 3.2348440370634775\n",
      "iter 200 train loss = 3.229698395910015\n",
      "iter 300 train loss = 3.2297034110693428\n",
      "iter 400 train loss = 3.2351759062371195\n",
      "iter 500 train loss = 3.23885936424501\n",
      "iter 600 train loss = 3.2423260500378195\n",
      "iter 700 train loss = 3.244642265168448\n",
      "iter 800 train loss = 3.249444607975666\n",
      "iter 900 train loss = 3.2509991724218112\n",
      "iter 1000 train loss = 3.2534349974310977\n",
      "iter 1100 train loss = 3.25696541546738\n",
      "iter 1200 train loss = 3.2578286097968534\n",
      "iter 1300 train loss = 3.259182566219486\n",
      "iter 1400 train loss = 3.2616267848038705\n",
      "iter 1500 train loss = 3.2632629243920523\n",
      "iter 1600 train loss = 3.2641595963634735\n",
      "iter 1700 train loss = 3.2667173933238427\n",
      "iter 1800 train loss = 3.2690788114155622\n",
      "iter 1900 train loss = 3.2712639422889906\n",
      "iter 2000 train loss = 3.2717728550138414\n",
      "Epoch 11 valid loss = 3.7426143983134113\n",
      "Epoch 11 valid perp = 42.20819507647803\n",
      "iter 0 train loss = 3.1511339948850896\n",
      "iter 100 train loss = 3.193354297719747\n",
      "iter 200 train loss = 3.2004920408587783\n",
      "iter 300 train loss = 3.2092081483740547\n",
      "iter 400 train loss = 3.2184906786467278\n",
      "iter 500 train loss = 3.222518709600771\n",
      "iter 600 train loss = 3.22595825004691\n",
      "iter 700 train loss = 3.2275548885530174\n",
      "iter 800 train loss = 3.2318750256001745\n",
      "iter 900 train loss = 3.2339822385579144\n",
      "iter 1000 train loss = 3.2357996358145256\n",
      "iter 1100 train loss = 3.23731301902816\n",
      "iter 1200 train loss = 3.2394030090182544\n",
      "iter 1300 train loss = 3.2424390978827375\n",
      "iter 1400 train loss = 3.2434993322229118\n",
      "iter 1500 train loss = 3.2440990590352894\n",
      "iter 1600 train loss = 3.246736308431778\n",
      "iter 1700 train loss = 3.248121468759097\n",
      "iter 1800 train loss = 3.2503240126626585\n",
      "iter 1900 train loss = 3.2516195922737094\n",
      "iter 2000 train loss = 3.252969869888284\n",
      "Epoch 12 valid loss = 3.7362147310152145\n",
      "Epoch 12 valid perp = 41.93893916382471\n",
      "iter 0 train loss = 3.0630949365401023\n",
      "iter 100 train loss = 3.192478041962949\n",
      "iter 200 train loss = 3.204079885043092\n",
      "iter 300 train loss = 3.205168240729506\n",
      "iter 400 train loss = 3.205272037026709\n",
      "iter 500 train loss = 3.2077896569818676\n",
      "iter 600 train loss = 3.206015476925627\n",
      "iter 700 train loss = 3.209888630029118\n",
      "iter 800 train loss = 3.2119525109141316\n",
      "iter 900 train loss = 3.2175645591164628\n",
      "iter 1000 train loss = 3.220426856605465\n",
      "iter 1100 train loss = 3.2222685309639854\n",
      "iter 1200 train loss = 3.224520139648947\n",
      "iter 1300 train loss = 3.226358652599239\n",
      "iter 1400 train loss = 3.2268523262312936\n",
      "iter 1500 train loss = 3.228320542387668\n",
      "iter 1600 train loss = 3.229897207841008\n",
      "iter 1700 train loss = 3.231443339057461\n",
      "iter 1800 train loss = 3.2336774201648457\n",
      "iter 1900 train loss = 3.2351198900760783\n",
      "iter 2000 train loss = 3.2365500924640744\n",
      "Epoch 13 valid loss = 3.743350329661511\n",
      "Epoch 13 valid perp = 42.23926884305853\n",
      "iter 0 train loss = 3.1065517253377375\n",
      "iter 100 train loss = 3.1763725014269104\n",
      "iter 200 train loss = 3.1751202696867624\n",
      "iter 300 train loss = 3.180963822035123\n",
      "iter 400 train loss = 3.1800623115756865\n",
      "iter 500 train loss = 3.187577306375065\n",
      "iter 600 train loss = 3.1909316736151663\n",
      "iter 700 train loss = 3.1955787718123685\n",
      "iter 800 train loss = 3.195796953358921\n",
      "iter 900 train loss = 3.1990157032455193\n",
      "iter 1000 train loss = 3.201609288071407\n",
      "iter 1100 train loss = 3.2038726960007433\n",
      "iter 1200 train loss = 3.207067575177352\n",
      "iter 1300 train loss = 3.2097498421721764\n",
      "iter 1400 train loss = 3.2102347825280284\n",
      "iter 1500 train loss = 3.2116475062926417\n",
      "iter 1600 train loss = 3.2131169360253695\n",
      "iter 1700 train loss = 3.214007132376584\n",
      "iter 1800 train loss = 3.2157675006980524\n",
      "iter 1900 train loss = 3.2174261575221905\n",
      "iter 2000 train loss = 3.218277305555837\n",
      "Epoch 14 valid loss = 3.7407411232100563\n",
      "Epoch 14 valid perp = 42.12920152690866\n",
      "iter 0 train loss = 3.05162483252189\n",
      "iter 100 train loss = 3.1532647820068305\n",
      "iter 200 train loss = 3.153079961751258\n",
      "iter 300 train loss = 3.1570439153921166\n",
      "iter 400 train loss = 3.1666814114665423\n",
      "iter 500 train loss = 3.1680882075634593\n",
      "iter 600 train loss = 3.173026760702963\n",
      "iter 700 train loss = 3.176758876727158\n",
      "iter 800 train loss = 3.179237595932881\n",
      "iter 900 train loss = 3.184121647935852\n",
      "iter 1000 train loss = 3.185342860181246\n",
      "iter 1100 train loss = 3.188292621878316\n",
      "iter 1200 train loss = 3.1893508843150196\n",
      "iter 1300 train loss = 3.1929602590598214\n",
      "iter 1400 train loss = 3.1946260266049347\n",
      "iter 1500 train loss = 3.1962327417505048\n",
      "iter 1600 train loss = 3.1971269236416746\n",
      "iter 1700 train loss = 3.1991611748112905\n",
      "iter 1800 train loss = 3.1996808212573966\n",
      "iter 1900 train loss = 3.2014862443684318\n",
      "iter 2000 train loss = 3.2027427911639617\n",
      "Epoch 15 valid loss = 3.7502719560144038\n",
      "Epoch 15 valid perp = 42.53264743662539\n",
      "iter 0 train loss = 3.099030184243267\n",
      "iter 100 train loss = 3.1256944555920163\n",
      "iter 200 train loss = 3.1394765221783323\n",
      "iter 300 train loss = 3.1385445172043207\n",
      "iter 400 train loss = 3.1408872485950554\n",
      "iter 500 train loss = 3.1505493207570145\n",
      "iter 600 train loss = 3.149642547697964\n",
      "iter 700 train loss = 3.154048916143852\n",
      "iter 800 train loss = 3.157117116210356\n",
      "iter 900 train loss = 3.160596706560133\n",
      "iter 1000 train loss = 3.1644883024039174\n",
      "iter 1100 train loss = 3.1669996434741963\n",
      "iter 1200 train loss = 3.170203874596447\n",
      "iter 1300 train loss = 3.174205648917984\n",
      "iter 1400 train loss = 3.177328740922367\n",
      "iter 1500 train loss = 3.1801395413703077\n",
      "iter 1600 train loss = 3.181885017214981\n",
      "iter 1700 train loss = 3.182947134779416\n",
      "iter 1800 train loss = 3.185130157269404\n",
      "iter 1900 train loss = 3.187184207762152\n",
      "iter 2000 train loss = 3.1889202406042876\n",
      "Epoch 16 valid loss = 3.750467447272325\n",
      "Epoch 16 valid perp = 42.54096301015998\n",
      "iter 0 train loss = 3.116722901852545\n",
      "iter 100 train loss = 3.1148003190057465\n",
      "iter 200 train loss = 3.123937114888662\n",
      "iter 300 train loss = 3.1321096005766864\n",
      "iter 400 train loss = 3.136283859019302\n",
      "iter 500 train loss = 3.1397835345837617\n",
      "iter 600 train loss = 3.1463046742434955\n",
      "iter 700 train loss = 3.1483358552041865\n",
      "iter 800 train loss = 3.15026622725413\n",
      "iter 900 train loss = 3.152596265150614\n",
      "iter 1000 train loss = 3.155584430809779\n",
      "iter 1100 train loss = 3.159266082582343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1200 train loss = 3.162484079450204\n",
      "iter 1300 train loss = 3.164635838121023\n",
      "iter 1400 train loss = 3.165581576564407\n",
      "iter 1500 train loss = 3.166534216717141\n",
      "iter 1600 train loss = 3.1681046945788287\n",
      "iter 1700 train loss = 3.169740079471297\n",
      "iter 1800 train loss = 3.1714903678094197\n",
      "iter 1900 train loss = 3.174128256964319\n",
      "iter 2000 train loss = 3.1754388729310916\n",
      "Epoch 17 valid loss = 3.750923181443588\n",
      "Epoch 17 valid perp = 42.56035479909694\n",
      "iter 0 train loss = 3.0940872012867646\n",
      "iter 100 train loss = 3.1032023231888193\n",
      "iter 200 train loss = 3.115444575753218\n",
      "iter 300 train loss = 3.1177709981621486\n",
      "iter 400 train loss = 3.121725518583685\n",
      "iter 500 train loss = 3.126718572066792\n",
      "iter 600 train loss = 3.130486608852637\n",
      "iter 700 train loss = 3.133430468192216\n",
      "iter 800 train loss = 3.135011747401587\n",
      "iter 900 train loss = 3.1398853118472765\n",
      "iter 1000 train loss = 3.1432955740242545\n",
      "iter 1100 train loss = 3.144834224560041\n",
      "iter 1200 train loss = 3.1475188081625802\n",
      "iter 1300 train loss = 3.1486888484924664\n",
      "iter 1400 train loss = 3.152163273925223\n",
      "iter 1500 train loss = 3.155434730425717\n",
      "iter 1600 train loss = 3.157169459864243\n",
      "iter 1700 train loss = 3.158733562262389\n",
      "iter 1800 train loss = 3.159971566146914\n",
      "iter 1900 train loss = 3.161649031744297\n",
      "iter 2000 train loss = 3.1630332164507484\n",
      "Epoch 18 valid loss = 3.753375606406924\n",
      "Epoch 18 valid perp = 42.6648589675931\n",
      "iter 0 train loss = 3.0715889741768647\n",
      "iter 100 train loss = 3.0908834427242824\n",
      "iter 200 train loss = 3.0791196465016535\n",
      "iter 300 train loss = 3.070193542015975\n",
      "iter 400 train loss = 3.06631011048633\n",
      "iter 500 train loss = 3.0662167085613556\n",
      "iter 600 train loss = 3.062503467507954\n",
      "iter 700 train loss = 3.060426435972992\n",
      "iter 800 train loss = 3.060221043719123\n",
      "iter 900 train loss = 3.0606015818314094\n",
      "iter 1000 train loss = 3.059542665221024\n",
      "iter 1100 train loss = 3.060185139962771\n",
      "iter 1200 train loss = 3.06040832405803\n",
      "iter 1300 train loss = 3.059329027293666\n",
      "iter 1400 train loss = 3.058131447786744\n",
      "iter 1500 train loss = 3.057894233346831\n",
      "iter 1600 train loss = 3.0571802004846327\n",
      "iter 1700 train loss = 3.057476776964677\n",
      "iter 1800 train loss = 3.058034059273197\n",
      "iter 1900 train loss = 3.058236649622036\n",
      "iter 2000 train loss = 3.058315706058833\n",
      "Epoch 19 valid loss = 3.714664595620868\n",
      "Epoch 19 valid perp = 41.0448181636909\n",
      "iter 0 train loss = 3.106245282602729\n",
      "iter 100 train loss = 3.0282908796958776\n",
      "iter 200 train loss = 3.0205861902260773\n",
      "iter 300 train loss = 3.0295627962647367\n",
      "iter 400 train loss = 3.028550085040017\n",
      "iter 500 train loss = 3.030360291180644\n",
      "iter 600 train loss = 3.0299409455078385\n",
      "iter 700 train loss = 3.03226261131844\n",
      "iter 800 train loss = 3.03271406556186\n",
      "iter 900 train loss = 3.03358121960978\n",
      "iter 1000 train loss = 3.033884756304359\n",
      "iter 1100 train loss = 3.035104661540376\n",
      "iter 1200 train loss = 3.0350239241247463\n",
      "iter 1300 train loss = 3.035016599264599\n",
      "iter 1400 train loss = 3.035270512256144\n",
      "iter 1500 train loss = 3.0347007288836143\n",
      "iter 1600 train loss = 3.035992947840485\n",
      "iter 1700 train loss = 3.0358739512059696\n",
      "iter 1800 train loss = 3.0349961211064986\n",
      "iter 1900 train loss = 3.0355586511478045\n",
      "iter 2000 train loss = 3.036257695931231\n",
      "Epoch 20 valid loss = 3.709958274898417\n",
      "Epoch 20 valid perp = 40.85210193332529\n",
      "iter 0 train loss = 2.909627079672219\n",
      "iter 100 train loss = 3.0067150842146466\n",
      "iter 200 train loss = 3.017328960668767\n",
      "iter 300 train loss = 3.020953477731172\n",
      "iter 400 train loss = 3.021150136988312\n",
      "iter 500 train loss = 3.0195165907693426\n",
      "iter 600 train loss = 3.022276642032071\n",
      "iter 700 train loss = 3.023824812427601\n",
      "iter 800 train loss = 3.0234808691867587\n",
      "iter 900 train loss = 3.0237017726240047\n",
      "iter 1000 train loss = 3.0246330730428146\n",
      "iter 1100 train loss = 3.025199820395251\n",
      "iter 1200 train loss = 3.025974152880606\n",
      "iter 1300 train loss = 3.0270215188614764\n",
      "iter 1400 train loss = 3.0265845036279604\n",
      "iter 1500 train loss = 3.026134867495834\n",
      "iter 1600 train loss = 3.026787389709482\n",
      "iter 1700 train loss = 3.0273582261143903\n",
      "iter 1800 train loss = 3.0276176055792705\n",
      "iter 1900 train loss = 3.0277477468705913\n",
      "iter 2000 train loss = 3.0275075731039296\n",
      "Epoch 21 valid loss = 3.7080354308334273\n",
      "Epoch 21 valid perp = 40.773625185034824\n",
      "iter 0 train loss = 2.890624388884543\n",
      "iter 100 train loss = 3.025186125222781\n",
      "iter 200 train loss = 3.028719368708591\n",
      "iter 300 train loss = 3.02523895929325\n",
      "iter 400 train loss = 3.023123418676854\n",
      "iter 500 train loss = 3.0209418288834664\n",
      "iter 600 train loss = 3.0153001464611275\n",
      "iter 700 train loss = 3.016510581106505\n",
      "iter 800 train loss = 3.016758078849998\n",
      "iter 900 train loss = 3.0166756931589838\n",
      "iter 1000 train loss = 3.0151651947910296\n",
      "iter 1100 train loss = 3.01583771440469\n",
      "iter 1200 train loss = 3.016612452814586\n",
      "iter 1300 train loss = 3.0184280243541988\n",
      "iter 1400 train loss = 3.0192313294859963\n",
      "iter 1500 train loss = 3.018850303535425\n",
      "iter 1600 train loss = 3.0196325064978713\n",
      "iter 1700 train loss = 3.0200649189066193\n",
      "iter 1800 train loss = 3.0205852333354355\n",
      "iter 1900 train loss = 3.01995643673461\n",
      "iter 2000 train loss = 3.0209243304036124\n",
      "Epoch 22 valid loss = 3.705473815746131\n",
      "Epoch 22 valid perp = 40.669312513094724\n",
      "iter 0 train loss = 3.004850756521102\n",
      "iter 100 train loss = 2.996300005277897\n",
      "iter 200 train loss = 3.001365199791337\n",
      "iter 300 train loss = 3.007958868794201\n",
      "iter 400 train loss = 3.0035232723186254\n",
      "iter 500 train loss = 3.007015473884738\n",
      "iter 600 train loss = 3.006909399142846\n",
      "iter 700 train loss = 3.008872703656451\n",
      "iter 800 train loss = 3.0098047546148816\n",
      "iter 900 train loss = 3.0102085256538453\n",
      "iter 1000 train loss = 3.0099559338386737\n",
      "iter 1100 train loss = 3.009913388855142\n",
      "iter 1200 train loss = 3.0108262340796794\n",
      "iter 1300 train loss = 3.012705506905864\n",
      "iter 1400 train loss = 3.0135224892430474\n",
      "iter 1500 train loss = 3.013044254885531\n",
      "iter 1600 train loss = 3.0156811414239697\n",
      "iter 1700 train loss = 3.0155483733643904\n",
      "iter 1800 train loss = 3.015421472858881\n",
      "iter 1900 train loss = 3.0164549812907135\n",
      "iter 2000 train loss = 3.015999693159517\n",
      "Epoch 23 valid loss = 3.7066269685516677\n",
      "Epoch 23 valid perp = 40.71623749555611\n",
      "iter 0 train loss = 2.8518487231661913\n",
      "iter 100 train loss = 2.9934053349838825\n",
      "iter 200 train loss = 2.9954448036803174\n",
      "iter 300 train loss = 2.9989735799393613\n",
      "iter 400 train loss = 3.001163895013583\n",
      "iter 500 train loss = 3.003025488664327\n",
      "iter 600 train loss = 3.0016753778158294\n",
      "iter 700 train loss = 3.0022980986350256\n",
      "iter 800 train loss = 3.0034521717569778\n",
      "iter 900 train loss = 3.004253423392873\n",
      "iter 1000 train loss = 3.0047980937633008\n",
      "iter 1100 train loss = 3.00558719344441\n",
      "iter 1200 train loss = 3.0063116192439727\n",
      "iter 1300 train loss = 3.0080782938290707\n",
      "iter 1400 train loss = 3.008717389790055\n",
      "iter 1500 train loss = 3.0100693257951736\n",
      "iter 1600 train loss = 3.0110455812305337\n",
      "iter 1700 train loss = 3.010450302068823\n",
      "iter 1800 train loss = 3.0112404152223395\n",
      "iter 1900 train loss = 3.01183459110821\n",
      "iter 2000 train loss = 3.012065948196377\n",
      "Epoch 24 valid loss = 3.705176427781274\n",
      "Epoch 24 valid perp = 40.657219747224964\n",
      "iter 0 train loss = 3.1550319157851607\n",
      "iter 100 train loss = 3.0077015989378357\n",
      "iter 200 train loss = 2.9956433018966973\n",
      "iter 300 train loss = 2.998186391363855\n",
      "iter 400 train loss = 3.001931169669944\n",
      "iter 500 train loss = 3.004085434602202\n",
      "iter 600 train loss = 3.00150220637748\n",
      "iter 700 train loss = 3.003117271646564\n",
      "iter 800 train loss = 3.0026262332484213\n",
      "iter 900 train loss = 3.0011484506357147\n",
      "iter 1000 train loss = 3.0038720621631674\n",
      "iter 1100 train loss = 3.0054024863507256\n",
      "iter 1200 train loss = 3.0057119584810326\n",
      "iter 1300 train loss = 3.004902517398589\n",
      "iter 1400 train loss = 3.0060579157955045\n",
      "iter 1500 train loss = 3.0064391306797833\n",
      "iter 1600 train loss = 3.0074926548025886\n",
      "iter 1700 train loss = 3.0080105888351536\n",
      "iter 1800 train loss = 3.009205947767383\n",
      "iter 1900 train loss = 3.0087260505960396\n",
      "iter 2000 train loss = 3.0086713835530245\n",
      "Epoch 25 valid loss = 3.7052195853859686\n",
      "Epoch 25 valid perp = 40.65897445330697\n",
      "iter 0 train loss = 3.0919274685650886\n",
      "iter 100 train loss = 3.0052708694780326\n",
      "iter 200 train loss = 3.001847840042492\n",
      "iter 300 train loss = 2.99565230786491\n",
      "iter 400 train loss = 2.99277720914477\n",
      "iter 500 train loss = 2.992861540741214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 600 train loss = 2.9946506740561714\n",
      "iter 700 train loss = 2.9963002558101333\n",
      "iter 800 train loss = 2.997952004542492\n",
      "iter 900 train loss = 2.9994965012531654\n",
      "iter 1000 train loss = 3.0002327128563735\n",
      "iter 1100 train loss = 3.0006245937991536\n",
      "iter 1200 train loss = 3.001284492210663\n",
      "iter 1300 train loss = 3.001194202513145\n",
      "iter 1400 train loss = 3.0021843568649724\n",
      "iter 1500 train loss = 3.0029604136089767\n",
      "iter 1600 train loss = 3.003655637574926\n",
      "iter 1700 train loss = 3.0044330287767984\n",
      "iter 1800 train loss = 3.005351474014864\n",
      "iter 1900 train loss = 3.0055567779203236\n",
      "iter 2000 train loss = 3.0051415993425588\n",
      "Epoch 26 valid loss = 3.7040063703880644\n",
      "Epoch 26 valid perp = 40.609676286385344\n",
      "iter 0 train loss = 3.1951957239389714\n",
      "iter 100 train loss = 3.001914056519682\n",
      "iter 200 train loss = 2.9943953453417502\n",
      "iter 300 train loss = 2.9884454162180685\n",
      "iter 400 train loss = 2.9903382731632773\n",
      "iter 500 train loss = 2.9899682000887235\n",
      "iter 600 train loss = 2.9926557243044245\n",
      "iter 700 train loss = 2.992927455889411\n",
      "iter 800 train loss = 2.995285713128146\n",
      "iter 900 train loss = 2.995774496354341\n",
      "iter 1000 train loss = 2.9941522882525318\n",
      "iter 1100 train loss = 2.993348360857515\n",
      "iter 1200 train loss = 2.993881121447626\n",
      "iter 1300 train loss = 2.995155698552209\n",
      "iter 1400 train loss = 2.9959857821824496\n",
      "iter 1500 train loss = 2.996615488993925\n",
      "iter 1600 train loss = 2.998166757530803\n",
      "iter 1700 train loss = 2.9976835409198217\n",
      "iter 1800 train loss = 2.9990747385956733\n",
      "iter 1900 train loss = 2.9993861412867844\n",
      "iter 2000 train loss = 3.0013877774734143\n",
      "Epoch 27 valid loss = 3.70440919737562\n",
      "Epoch 27 valid perp = 40.626038255249455\n",
      "iter 0 train loss = 3.0651542832326832\n",
      "iter 100 train loss = 3.005398681068867\n",
      "iter 200 train loss = 2.992663120798972\n",
      "iter 300 train loss = 2.99085079993482\n",
      "iter 400 train loss = 2.994240045623904\n",
      "iter 500 train loss = 2.996910930734187\n",
      "iter 600 train loss = 2.994276931725997\n",
      "iter 700 train loss = 2.994617867085679\n",
      "iter 800 train loss = 2.9958768562421567\n",
      "iter 900 train loss = 2.995958460826355\n",
      "iter 1000 train loss = 2.9965488443699386\n",
      "iter 1100 train loss = 2.99560025264262\n",
      "iter 1200 train loss = 2.9960257665522625\n",
      "iter 1300 train loss = 2.996785904581304\n",
      "iter 1400 train loss = 2.9972835393113897\n",
      "iter 1500 train loss = 2.9974137903378737\n",
      "iter 1600 train loss = 2.9981589527115995\n",
      "iter 1700 train loss = 2.997876085886986\n",
      "iter 1800 train loss = 2.9980498069421957\n",
      "iter 1900 train loss = 2.9991680368269127\n",
      "iter 2000 train loss = 2.9986988778622563\n",
      "Epoch 28 valid loss = 3.704243191791091\n",
      "Epoch 28 valid perp = 40.61929466577404\n",
      "iter 0 train loss = 2.7895360635527484\n",
      "iter 100 train loss = 2.989959472726402\n",
      "iter 200 train loss = 2.982212255648527\n",
      "iter 300 train loss = 2.9832332494131872\n",
      "iter 400 train loss = 2.983977597745743\n",
      "iter 500 train loss = 2.9876099799342053\n",
      "iter 600 train loss = 2.9902256274240226\n",
      "iter 700 train loss = 2.9880927533401853\n",
      "iter 800 train loss = 2.9864869491560113\n",
      "iter 900 train loss = 2.9853581406480427\n",
      "iter 1000 train loss = 2.9864290614038174\n",
      "iter 1100 train loss = 2.9867101254880835\n",
      "iter 1200 train loss = 2.9883300986234413\n",
      "iter 1300 train loss = 2.9896095605236472\n",
      "iter 1400 train loss = 2.989559660749564\n",
      "iter 1500 train loss = 2.990949371462686\n",
      "iter 1600 train loss = 2.9925867417018432\n",
      "iter 1700 train loss = 2.9941210230331077\n",
      "iter 1800 train loss = 2.994086226237467\n",
      "iter 1900 train loss = 2.995261359962793\n",
      "iter 2000 train loss = 2.9954470639983084\n",
      "Epoch 29 valid loss = 3.7043635601498917\n",
      "Epoch 29 valid perp = 40.62418423787758\n"
     ]
    }
   ],
   "source": [
    "plot_cache = []\n",
    "\n",
    "best_val_loss = 100\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    sum_tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text_vecs = batch['text_vecs'].to('cuda')\n",
    "        target_vecs = batch['target_vecs'].to('cuda')\n",
    " \n",
    "        encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
    "        \n",
    "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
    "        \n",
    "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
    "        \n",
    "        loss = criterion(scores, target_vecs.view(-1))\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
    "        loss /= num_tokens\n",
    "        \n",
    "        sum_tokens += num_tokens\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            avg_train_loss = sum_loss/sum_tokens\n",
    "            print(\"iter {} train loss = {}\".format(i, sum_loss/sum_tokens))\n",
    "\n",
    "    val_loss = 0\n",
    "    val_tokens = 0\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "        model.eval()\n",
    "        \n",
    "        text_vecs = batch['text_vecs'].to('cuda')\n",
    "        target_vecs = batch['target_vecs'].to('cuda')\n",
    "        \n",
    "        encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
    "        #encoded = model.encoder(text_vecs)\n",
    "        \n",
    "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
    "        \n",
    "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
    "        \n",
    "        loss = criterion(scores, target_vecs.view(-1))\n",
    "        \n",
    "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
    "        \n",
    "        val_tokens += num_tokens\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    avg_val_loss = val_loss/val_tokens\n",
    "    val_perp = np.exp(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "        \n",
    "    print(\"Epoch {} valid loss = {}\".format(epoch, avg_val_loss))\n",
    "    print(\"Epoch {} valid perp = {}\".format(epoch, val_perp))\n",
    "    \n",
    "    plot_cache.append( (avg_train_loss, avg_val_loss) )\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'opts': opts,\n",
    "        'plot_cache': plot_cache,\n",
    "            }, f'./transformer_best_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VNtN_PT4nu_7"
   },
   "outputs": [],
   "source": [
    "# model_name = './transformer_best_0.pt'\n",
    "\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "# if num_gpus > 0:\n",
    "#     current_device = 'cuda'\n",
    "# else:\n",
    "#     current_device = 'cpu'\n",
    "\n",
    "# if current_device == 'cuda':\n",
    "#     model_pt = torch.load(model_name)\n",
    "# else:\n",
    "#     model_pt = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "# opts = model_pt['opts']\n",
    "\n",
    "# model = seq2seq(opts)\n",
    "# model.load_state_dict(model_pt['state_dict'])\n",
    "# model.to(current_device)\n",
    "\n",
    "# plot_cache = model_pt['plot_cache']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAxmvNFgbIia"
   },
   "source": [
    "## You present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "rpOU_fRsn3oB",
    "outputId": "795484c6-7eee-4706-8522-6d10d6dc319f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEICAYAAACgQWTXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# plot_cache = model_pt['plot_cache']\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
    "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "m8gEwljBn533",
    "outputId": "fb4d2257-98cb-4937-b386-d626ec0b73ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxV5b3v8c9v7ww7c0hIQiBAmFSQ\nSYgUlVop2FOtY2s5UG2RDrbWng6e3lNvb8+t7em51962p7Xn9mptnY9KrXWqHo9V64DiQFAQCyKD\nQQMhCWHIQOY894+1MhASyJys7O/79dqvNe299rOy4buf/axnrcecc4iISHCFhrsAIiLSPwpyEZGA\nU5CLiAScglxEJOAU5CIiAacgFxEJOAW5iEjAKchlxDCzIjOrNbNqMys1s7vMLNnf9oKZ1fnbDpjZ\nw2aW62+7y8x+MrylFxk+CnIZaS52ziUDC4AC4Acdtn3D33YKkA78chjKh5nFDMf7inRHQS4jknNu\nL/AUMLuLbQeBP3W17WTMbImZrTezw2b2oZld7a9/wcy+3OF5V5vZyx2WnZldZ2Y7gB1mdouZ/bzT\nvh8zs+v9+fFm9iczKzez983smx2et8jMCs2s0v/l8W+9PQ6RjhTkMiKZ2UTgQuCtLraNBT7T1baT\n7HMy3pfDvwNZwHxgUy92cRnwEWAW8ADw92Zm/r7HAJ8A1ppZCPgzsBmYACwDvm1mf+fv52bgZudc\nKjANeLA3xyHSmYJcRppHzeww8DLwIvC/Omz7tb9tM1ACXN/LfX8OeNY594BzrtE5V+Gc602Q/2/n\n3EHnXC2wDnDAR/1tVwCvOuf2AWcCWc65HzvnGpxzu4HfASv95zYC081srHOu2jn3Wi+PQ+QYauuT\nkeYy59yz3Wz7pnPu9/3Y90RgVz9e/2HrjHPOmdlaYBXwEt6XxH/4mycD4/0vnVZhvPAH+BLwY+Bd\nM3sf+JFz7ol+lEuinIJcosmHwKJuttUAiR2Wx3XxnM63Cn0A+IuZ3YTX5HJ5h/d53zk3o6s3cs7t\nAFb5TTCfBh4ys0znXE3PDkPkWGpakdEibGaRDo+4Lp5zH7DczFaYWYyZZZrZfH/bJuDTZpZoZtPx\nas0n5Jx7CzgA/B542jnXWgN/A6gys++ZWYKZhc1stpmdCWBmV5lZlnOuBWh9TUvfD12inYJcRosb\ngNoOj792foJz7gO8E6j/CBzEC+95/uZfAg1AKXA3Xuj3xP3Acn/a+j7NwEV4J1Pfpz3s0/ynfBL4\nm5lV4534XOm3u4v0iWlgCRGRYFONXEQk4BTkIiIBpyAXEQk4BbmISMANaT/ysWPHuvz8/KF8SxGR\nwNu4ceMB51xWd9uHNMjz8/MpLCwcyrcUEQk8M9tzou1qWhERCTgFuYhIwCnIRUQCTjfNEpEB1djY\nSHFxMXV1dcNdlMCJRCLk5eURGxvbq9cpyEVkQBUXF5OSkkJ+fj7+uBvSA845KioqKC4uZsqUKb16\nrZpWRGRA1dXVkZmZqRDvJTMjMzOzT79kFOQiMuAU4n3T179bIIL8kbeK+Y/XTtiNUkQkap00yM3s\nVDPb1OFRaWbfNrMMM3vGzHb40zGDVcj/3LJfQS4iPVJRUcH8+fOZP38+48aNY8KECW3LDQ0NPdrH\nmjVr2L59+6CWMy8vj8OHD5/8iT1w0pOdzrnteDfIx8zCwF7gEbwb+T/nnLvJzG7wl783IKXqJDsl\nnsKig4OxaxEZZTIzM9m0yRtT+8YbbyQ5OZnvfve7xzzHOYdzjlCo67rsnXfeOejlHEi9bVpZBuxy\nzu0BLsUbSQV/etlAFqyjnNQIh442Ut/UPFhvISKj3M6dO5k1axZXXnklp59+OiUlJVxzzTUUFBRw\n+umn8+Mf/7jtuUuWLGHTpk00NTWRnp7ODTfcwLx58zjrrLMoKys7bt8/+MEPWL16NYsXL2bGjBnc\ncccdADz77LMsXbqUCy64gFNPPZXrrruOwRjMp7fdD1fiDTgLkOOcK/Hn9wM5Xb3AzK4BrgGYNGlS\nX8pIdko8AOVV9eSNSTzJs0VkpPjRn//G1n2VA7rPWeNT+eHFp/fpte+++y733HMPBQUFANx0001k\nZGTQ1NTE0qVLueKKK5g1a9Yxrzly5Agf+9jHuOmmm7j++uu54447uOGGG47b95YtW1i/fj2VlZUs\nWLCAT33qUwC8/vrrbN26lYkTJ3L++efz2GOPcdllA1vv7XGN3B/M9hLgj523Oe8rpsuvGefcbc65\nAudcQVZWtzfvOqGc1AgAZVX1fXq9iAjAtGnT2kIc4IEHHmDBggUsWLCAbdu2sXXr1uNek5CQwAUX\nXADAwoULKSoq6nLfl112GZFIhOzsbM4991w2bNgAwOLFi8nPzyccDrNy5UpefvnlAT+u3tTILwDe\ndM6V+sulZpbrnCsxs1zg+N8bAyQ71auRl1XqSjGRIOlrzXmwJCUltc3v2LGDm2++mTfeeIP09HSu\nuuqqLvtwx8XFtc2Hw2Gampq63HfnroOty92tH0i9aSNfRXuzCsDjwGp/fjXw2EAVqrPsFNXIRWRg\nVVZWkpKSQmpqKiUlJTz99NP92t+jjz5KfX095eXlrFu3rq3m/9prr/HBBx/Q3NzMgw8+yJIlSwai\n+MfoUY3czJKA84Gvdlh9E/CgmX0J2AOsGPDS+TKT4giHjFLVyEVkgCxYsIBZs2Zx2mmnMXnyZM45\n55x+7W/27Nl87GMfo6Kigh/96Efk5OSwZcsWFi1axNe+9jV27drF8uXLueSSSwboCNr1KMidczVA\nZqd1FXi9WAZdKGRkJcdTVqkauYj03I033tg2P3369LZuieA1cdx7771dvq5jO3bHvt4rV65k5cqV\nXb7mjDPO4O677z5ufVpaGo8++uhx64uLi09a/p4KxJWdADmp8ZSqaUVE5DiBufthVkqE4kNHh7sY\nIiLH+clPftLl+uXLl7N8+fJBf/9A1ch1slNE5HiBCfLslAgHaxpoaGoZ7qKIiIwogQnyHL8veXm1\nauUiIh0FJsh1UZCISNeCE+T+RUGl6oIoIiewdOnS4y7u+dWvfsW11157wtclJycDsG/fPq644oou\nn3PeeedRWFjY7zK+8MILXHTRRf3eT6vgBHlr00qVauQi0r1Vq1axdu3aY9atXbuWVatW9ej148eP\n56GHHhqMog2awAR5ZlK8f3WnauQi0r0rrriCJ598sm0QiaKiIvbt28dHP/pRqqurWbZsGQsWLGDO\nnDk89tjxdxYpKipi9uzZANTW1rJy5UpmzpzJ5ZdfTm1tbZfvmZ+fzz/90z8xZ84cFi1axM6dOwG4\n+uqr+drXvkZBQQGnnHIKTzzxxKAcc2D6kYdDxtjkOMpUIxcJjqdugP1bBnaf4+bABTd1uzkjI4NF\nixbx1FNPcemll7J27VpWrFiBmRGJRHjkkUdITU3lwIEDLF68mEsuuaTbG1ndcsstJCYmsm3bNt5+\n+20WLFjQ7fumpaWxZcsW7rnnHr797W+3hXZRURFvvPEGu3btYunSpW0hP5ACUyMH73a2qpGLyMl0\nbF7p2KzinOP73/8+c+fOZfny5ezdu5fS0tJu9/PSSy9x1VVXATB37lzmzp17wvdsnb766qtt61es\nWEEoFGLGjBlMnTqVd999t9/H11lgauTgDTCx97Bq5CKBcYKa82C69NJL+c53vsObb77J0aNHWbhw\nIQD33Xcf5eXlbNy4kdjYWPLz87u8dW1fdKzVdzff1fJACFSNPDs1ou6HInJSycnJLF26lC9+8YvH\nnOQ8cuQI2dnZxMbG8vzzz7Nnz4kHdT/33HO5//77AXjnnXd4++23u33uH/7wh7bpWWed1bb+j3/8\nIy0tLezatYvdu3dz6qmn9ufQuhS4GnlFTQONzS3EhgP1HSQiQ2zVqlVcfvnlx/RgufLKK7n44ouZ\nM2cOBQUFnHbaaSfcx7XXXsuaNWuYOXMmM2fObKvZd+XQoUPMnTuX+Ph4HnigfeiGSZMmsWjRIior\nK7n11luJRCL9P7hObDAGAu1OQUGB608fzAfe+ID//vAW1t/wccanJwxgyURkoGzbto2ZM2cOdzGG\nVH5+PoWFhYwdO/aY9VdffTUXXXRRt/3Su9LV38/MNjrnCrp5ScCaVvxBmHXzLBGRdoFqWmkdhFkj\nBYnISNLdgMx33XXXkLy/auQiMuCGssl2NOnr3y1QQZ6ZHE/IdOMskZEsEolQUVGhMO8l5xwVFRV9\nOhkaqKYV7+pOjd0pMpLl5eVRXFxMeXn5cBclcCKRCHl5eb1+XaCCHPyrO3WZvsiIFRsby5QpU4a7\nGFElUE0r4LWT6zJ9EZF2wQvy1IhuZSsi0kHwgjwlngPV3tWdIiISwCBv7Ut+QGN3iogAAQzy1r7k\naicXEfEELshba+TqSy4i4glckLeO3VmqqztFRIAABnlmUhwhg3LVyEVEgAAGeUw4RGay+pKLiLTq\nUZCbWbqZPWRm75rZNjM7y8wyzOwZM9vhT8cMdmFb5aTGaxBmERFfT2vkNwP/5Zw7DZgHbANuAJ5z\nzs0AnvOXh0R2igZhFhFpddIgN7M04FzgdgDnXINz7jBwKXC3/7S7gcsGq5CdeTVyBbmICPSsRj4F\nKAfuNLO3zOz3ZpYE5DjnSvzn7AdyunqxmV1jZoVmVjhQd0PLSolQUVNPk67uFBHpUZDHAAuAW5xz\nZwA1dGpGcd6Nh7u8+bBz7jbnXIFzriArK6u/5QW8GrlzcKC6YUD2JyISZD0J8mKg2Dn3ur/8EF6w\nl5pZLoA/LRucIh4vO0VDvomItDppkDvn9gMfmtmp/qplwFbgcWC1v2418NiglLALOaka8k1EpFVP\nB5b4B+A+M4sDdgNr8L4EHjSzLwF7gBWDU8TjqUYuItKuR0HunNsEFHSxadnAFqdnxibHYaYauYgI\nBPDKTvCv7kyK142zREQIaJCD+pKLiLQKbJB7Y3eqRi4iEtggz0mNqEYuIkKAg9wbu1NXd4qIBDfI\nUyM4BxU1urpTRKJbcIO8bexOtZOLSHQLbJC3j92pdnIRiW6BDfL2sTtVIxeR6BbYIB+bHO9d3aka\nuYhEucAGeWw4RGZSnIZ8E5GoF9ggB+/mWaqRi0i0C3aQp8arjVxEol6ggzxHNXIRkWAHeXaqd3Vn\nc0uXo8yJiESFgAd5hBYHFdWqlYtI9Ap2kLdd3akgF5HoFeggb7u6Uyc8RSSKBTrIVSMXEQl4kGf5\nQa4auYhEs0AHeevVnaqRi0g0C3SQg9dzpVw1chGJYsEP8pR41chFJKoFPshzUuPVRi4iUS0YQX6o\nCIpe7nJTdkqE8ipd3Ski0SsYQf7ItfDotdDSfNymnNR47+rOGjWviEh0CkaQL74WDn8A2/583Kas\nFA35JiLRLRhBftqnYEw+vPqb4zblpKovuYhEt2AEeSgMi78OxW/Ah28csynbv0xfPVdEJFoFI8gB\n5l8JkTR49f8eszor2a+RK8hFJEr1KMjNrMjMtpjZJjMr9NdlmNkzZrbDn44Z1JLGJ8PCNV47+aGi\nttVxMSEykuI0UpCIRK3e1MiXOufmO+cK/OUbgOecczOA5/zlwfWRr4KF4LVbj1mdnRKvGrmIRK3+\nNK1cCtztz98NXNb/4pxE6niY/Rl4616oPdy2Oic1opOdIhK1ehrkDviLmW00s2v8dTnOuRJ/fj+Q\n09ULzewaMys0s8Ly8vJ+Fhc46zpoqIY3725bpRq5iESzngb5EufcAuAC4DozO7fjRuecwwv74zjn\nbnPOFTjnCrKysvpXWoDceZD/UXj9t9DcCHg18nKN3SkiUapHQe6c2+tPy4BHgEVAqZnlAvjTssEq\n5HHO/geo3At/exTwBmFubnEcrGkYsiKIiIwUJw1yM0sys5TWeeATwDvA48Bq/2mrgccGq5DHmX4+\nZM6AV/8dnCM7pbUvudrJRST69KRGngO8bGabgTeAJ51z/wXcBJxvZjuA5f7y0AiF4KyvQ8lm2PMK\n2f7VneVVaicXkegTc7InOOd2A/O6WF8BLBuMQvXIvFXw3L/Aq78h54I7ANXIRSQ6BefKzs5iE+DM\nL8P2p8iq/xCAMtXIRSQKBTfIARZ9BcKxxG241bu6UzVyEYlCwQ7y5GyYuwI23c+0pHrdOEtEolKw\ngxzgrG9AUy0r7BkNwiwiUSn4QZ49E6Yt4+9qHufgkerhLo2IyJALfpADnP0NUpsOclbt87To6k4R\niTKjI8inLuVg8gzWhJ6kolrt5CISXUZHkJux97Q1zAx9SM27zwx3aUREhtToCHKgcdZnKHPppLx1\n23AXRURkSI2aIM8ek8rdTZ8gs+QleO8vw10cEZEhM2qCPCslnnubl3MgcTrcvwJe/Bm0tAx3sURE\nBt2oCfL4mDDhxDH8ZtotMOez8PxPYO2qY0YSEhEZjUZNkANkp0QorgnBp2+DC38OO5+F286D/VuG\nu2giIoNmdAV5arx34ywz7z4sV/8nNNXB78+HTQ8Md/FERAbF6ArylAhlHW+cNekj8NWXYMJCePRr\n8MT10KR+5iIyuoyqIM9Jjae8qv7YqzuTs+ELj3nDwxXeDndeCEf2Dl8hRUQG2KgK8uyUeJpaHAeP\ndhq7MxwDn/gJfPZuKH8Xfnsu7H5xeAopIjLARlWQ56R6Y3eWdXc729Mvg688D4mZcO9l8MwPVTsX\nkcAbVUE+Ls0L8vdKq7p/UtYp8JW/el0UX7kZfjUbHlgFO56BluYhKqmIyMAZVUE+Ny+d/MxE7nzl\nfZw7wV0Q45O9LorffAvO+RYUb4D7roCb58NLP4eq0qErtIhIP42qIA+HjK+cO5XNxUd4dXfFyV+Q\nMQWW3wjf2QqfvQsy8uGv/wK/nAUPfgF2Pa+rQ0VkxLMT1lwHWEFBgSssLBzU96hrbGbJT//K6ePT\nuPuLi3q/gwM7YeOdsOl+qD0IGVPhjM/DuDmQkgup4yFhjNdXXWSwOKd/Y9LGzDY65wq62x4zlIUZ\nCpHYMGvOmcLPnt7O1n2VzBqf2rsdjJ0Of/ev8PF/hm2PQ+Gd8NyPjn1OTMQL9JTx3jQ1t30+ZRwk\nZXmPuCT9ZwyauiNQsQsqdsLhPd55E+cAv8LTOt/ttMWbdy1dzLeAa4bGOmg8Co21/qPDfFOHdePP\ngAt+BnkLh+3PIcEw6mrkAEeONnL2Tc9x/qwcfrXyjP7vsLIEjnwIlfu8R5U/rSyByr1QVQLNDce/\nLibBD/WxXn/2pLHtIZ+UBck53iMlByLpoz/0mxq8v9XRA5CQ4X3pxSb0Y3/13t+/ch9Ul0FMvLe/\n2CR/mghxie3rwrHe37ipAQ6974V16+OAP60p68Ebm/9ZdZpa6NhHd+tjEzo8EruYT4RQDGxeC9Wl\nsOALsOyHkJTZ97+VBFrU1cgB0hJjWbVoEneuL+IfP3EqEzMS+7fD1Fzv0R3n4GiFFyrVZVBT7j2q\ny6DmgDdfuRdKNnvzLU3H7yMc7wd7thdwydmQ7E8TM73afXwKxCV3mE/ywqu7MjXVQ0M11Fe1T+ur\noaHK29bS7JXFNbfPd17nWrxfIG2hmNgeOHEdAjM20atNHvGDtTVgO853FZKRNK/JKjnHm6bkHLvs\nWrzXHynusK+93vscPdC7z9HCXpkbqr39tkrKgszpcMonIHOGN585HcZM9j4Xs+H5kj33u/DCTfD6\nrd6vw2X/ExashlB46MsiI9qorJEDlByp5aM/fZ6rFk/mxktOH5L37BHnoO6wF/LVZV6Nq2q/N219\nVPnT2oMn318o1uuFE5fshWpjbXtwd/WFMZQiaZA6wX+Mb58mZkLtIa92Xl3qTav2+8e9v+tfN237\ny/P2kTbh2H0n50BLIzQc7dBU4c+3rfPn45M7BPY0SEgf2r9Lb5Vtg//8b1C0zmtuufAXam6JMier\nkY/aIAf4xwc38+SWfay/YRkZSXFD9r4DpqneC/vaQ9BQ44VzQ7Vfq67xatYNNf5ytRdUsYntNff4\nZIhLaQ/6jssx8d7P99aHhb2aXijcYTnGb4qo6xSQNd6087pwfHvApuR679NbzrWHfFWJ1xTRGt59\n2d9o4Ry88yd4+n+ouSUKRXWQ7yit4vxfvsS3l8/g28tPGbL3FRk0dZXw4k/htVsgkqrmligR1UEO\n8OW7N7BxzyFeueHjJMaNylMCEo1Kt3rNLXtehuzTve6xCWMgMcObJozxmowSOixH0kb/CfVRKipP\ndnb01Y9N47O3vsofC4tZfXb+cBdHZGDkzIKrn4AtD8Hrt8Ce9X4T3AluT9F6sjcc553AjvGn3S0f\n16umm142FoLmRv9EeaN3bqarZee8Xw7hOP8R401DsV6PonCcP431e/100l2lMxTjvaa1mbBtPtZ7\nj1BsezNha3fQzl1FOy4f93fzeyZ1Od+5bK7rZYD41EH75dTjIDezMFAI7HXOXWRmU4C1QCawEfi8\nc66bs1TD58z8DBZOHsPv1u3myo9MIiY8qi5mlWhmBnM/6z1aNTd6gd7xcfSgP3/QO6/RVOedUG6q\n7zBf53XLrKtsX+7Y371Z9/Hvt+s2ePd6GgS9qZF/C9gGtF5h81Pgl865tWZ2K/Al4JYBLt+A+Oq5\nU7nm3o08uaWES+dPGO7iiAyecKzfdTV7YPfb0nzsBUxNde29gHDtNeBQ+NjacccasoXaa+fNDX6t\nvXW+yZ/66+muybdz05A7/hdAs/8roG19k7fe0d6VtK2ff6jr5Y416hPNH9NU1bmm3mk5aWzv/+49\n1KMgN7M84FPAvwLXm5kBHwc+5z/lbuBGRmiQL5+Zw/TsZG59cTeXzBuPqZ1QpHdCYa/XUDT3HBrB\netrO8Cvgn4DWqygygcPOudaOysVAl1VdM7vGzArNrLC8vLxfhe2rUMi45typbCup5KUdvbyIRERk\nhDtpkJvZRUCZc25jX97AOXebc67AOVeQlZXVl10MiEvnjycnNZ7fvrhr2MogIjIYelIjPwe4xMyK\n8E5ufhy4GUg3s9ammTxgRA+1Ex8T5ktLprB+VwVvFx8e7uKIiAyYkwa5c+6/O+fynHP5wErgr865\nK4HngSv8p60GHhu0Ug6QVYsmkRKJ4VbVykVkFOlPX7zv4Z343InXZn77wBRp8KREYrlq8WSeemc/\nRQdqhrs4IiIDoldB7px7wTl3kT+/2zm3yDk33Tn3WedcIDqarjknn9hQiNvW7R7uooiIDIiouzom\nOyXCZxZO4KGNxZRV1Q13cURE+i3qghzgKx+dSmNzC3e8XDTcRRER6beoDPKpWclcMm88v1u3m+e3\n92REGBGRkSsqgxzgXy+fw6k5KVx335u8s/fIcBdHRKTPojbIk+NjuHPNmYxJjGPNXRsoPnR0uIsk\nItInURvkADmpEe5ccyZ1jc1cfecGjhxtHO4iiYj0WlQHOcApOSn89vML2VNRwzX3FlLf1DzcRRIR\n6ZWoD3KAs6eN5eefncfr7x/kv/3xbVpahm7UJBGR/hr1IwT11KXzJ1B8qJafPb2dCWMS+N4nTxvu\nIomI9IiCvIOvnzeNvYdrueWFXUxIT+CqxZOHu0giIielIO/AzPjxJaez/0gd//Oxd8hNi7BsZs5w\nF0tE5ITURt5JTDjEv686g9PHp/GN+9/SLW9FZMRTkHchKT6G268uIDM5ji/etYEPD6qPuYiMXAry\nbmSnRLhrzZk0NjtW3/kGpZW6wZaIjEwK8hOYnp3C775QQMnhOj7163W8slPjfYrIyKMgP4lFUzJ4\n/BvnkJ4Yx1W3v84vn3mPZvUzF5ERREHeAzNyUnj8G+dw+fwJ3PzcDr5wx+uUVwViHA0RiQIK8h5K\njIvhFyvm8dPPzKGw6BAX/nodr+2uGO5iiYgoyHvDzPj7Myfx6HXnkBIfw+d+9xq/eX6nLukXkWGl\nIO+DmbmpPP4PS/jU3PH87OntrLlrAwdrGoa7WCISpRTkfZQcH8OvV87nJ5fN5tVdFVx48zoKiw4O\nd7FEJAopyPvBzLhq8WQe/vrZxMWE+PvbXuPXz+2grlG3whWRoaMgHwCzJ6TxxDeXcMHscfzbM++x\n7Bcv8vjmfTintnMRGXwK8gGSGonl/35uAfd/+SOkJcTyzQfe4tO3rGfjnkPDXTQRGeUU5APs7Olj\n+fM/LOH/XDGXvYdq+cwt67nu/jd1vxYRGTQK8kEQDhkrCiby/HfP41vLZvDctlKW/eJF/vdT26is\n07igIjKwFOSDKCk+hu+cfwovfHcpF88bz20v7ea8n73Ava/toam5ZbiLJyKjhIJ8CIxLi/CLFfP4\n8zeWMCM7mX9+9B0+efM6Hn6zmIYmBbqI9I8NZc+KgoICV1hYOGTvNxI553hmayk/e3o7O8qqyU6J\nZ/XZ+Vz5kUmkJ8YNd/FEZAQys43OuYJutyvIh4dzjhffK+f2l99n3Y4DJMSG+WxBHmvOmcKUsUnD\nXTwRGUH6HeRmFgFeAuLxxvh8yDn3QzObAqwFMoGNwOedcye8Tl1B3rV391dy+7r3eWzTPhpbWlg+\nM4cvL5nCoikZmNlwF09EhtlABLkBSc65ajOLBV4GvgVcDzzsnFtrZrcCm51zt5xoXwryEyurquPe\nV/fwH6/t4dDRRubmpfGlJVO4cE4usWGdzhCJVgPatGJmiXhBfi3wJDDOOddkZmcBNzrn/u5Er1eQ\n90xtQzMPv1XM7eveZ/eBGjKT4rhgzjgunjueM/MzCIVUSxeJJgMS5GYWxms+mQ78BvgZ8Jpzbrq/\nfSLwlHNudhevvQa4BmDSpEkL9+zZ05fjiEotLY4X3ivjT2/u5bltpdQ1tjAuNcKFc3K5eF4u8yem\nq+lFJAoMdI08HXgE+Gfgrp4EeUeqkfddTX0Tz24r5Ym3S3hxezkNzS3kjUngornjuWhuLqePT1Wo\ni4xSJwvymN7szDl32MyeB84C0s0sxjnXBOQBe/tXVDmRpPgYLp0/gUvnT+BIbSPPbC3lz5v38ft1\nu7n1xV1MHZvERXNz+dTc8ZySk6xQF4kiPTnZmQU0+iGeAPwF+CmwGvhTh5Odbzvn/t+J9qUa+cA7\nWNPAf72znyfe3sdruytocTA1K4kLZ+dy4ZxcZuamKNRFAm4geq3MBe4GwnhXgj7onPuxmU3F636Y\nAbwFXOWcO+GIxArywVVeVbkgpoIAAAymSURBVM/Tf9vPU++U8OouL9TzMxO5YE4uF87OZfYENb+I\nBJEuCIpSFdX1/GVrKf+5pYT1uypobnFMzEjgwtm5XDAnl3l5aQp1kYBQkAuHahp4ZpsX6q/sPEBj\nsyM7JZ6zp2Vy9vSxnDN9LBPSE4a7mCLSDQW5HOPI0Uae3VbKi++Vs37XAQ5Uexfj5mcmeqE+bSxn\nTcskI0n3fREZKRTk0i3nHNtLq3hlZwXrdx7g9fcPUl3fBMCs3FTOme7V2BdOHkNqJHaYSysSvRTk\n0mNNzS1sLj7C+p0HeGXXAd7cc5iG5hbM4LRxqZyZP4aFk8dwZn4G49UUIzJkFOTSZ7UNzWzcc4jC\nPQcpLDrEWx8coqahGYAJ6Ql+qI9h4eQMTh2XQli3DhAZFAN6QZBEl4S4MEtmjGXJjLGAV2N/d38V\nG4oOUrjnEK/truDxzfsASImPYf6kdOZP9B7zJqYzNjl+OIsvEjVUI5c+c85RfKiWwj0H2VB0iLc+\nOMz2/ZW0+P+kJqQneOGe5wX77AmpJMap7iDSW6qRy6AxMyZmJDIxI5HLz8gD4GhDE+/srWTzh4fZ\nVHyYTR8c5sm3SwBvUOpTclKYl5fGrPGpzMxN5bRxKaToRKpIvyjIZUAlxsWwaEoGi6ZktK0rr6rn\n7eLDbP7wMG99eJin3tnP2g0ftm3PG5PAzNxUZo5L8aa5qUzKSNTtekV6SEEugy4rJZ5lM3NYNjMH\n8JpkSo7U8e7+SraVVLGtpJJtJZU8t620rVkmMS7MqeNSODUnhWlZyUzLTmJaVjJ5YxJ1UlWkEwW5\nDDkzY3x6AuPTE/j4aTlt62sbmtlR1hrs3vSZraWsrWmvvceFQ0wZm9QW7NOykpmalcTUrGSS4/XP\nWaKT/uXLiJEQF2ZuXjpz89KPWX+opoHdB6rZVVbDrvJqdpVXs62kiqf/VkpzS/vJ+gnpCZw6LoUZ\nOcmcmpPCKTkpTM9OJhIbHupDERlSCnIZ8cYkxbEwKYOFkzOOWd/Q1MIHB2vYWVbDzrIq3iut5r3S\nKtbtKKex2Qv4kMHkzCRO8cN9hh/u+ZlJJMQp4GV0UJBLYMXFhJiencL07BRgXNv6xuYW9lTU8F5p\nNdv3V/FeaRXbS6t4Zmt7GzxATmo8+ZlJ3mNsElPGJjLZX1bIS5AoyGXUiQ23B/yFc3Lb1tc1NrOr\nvJr3D9RQdKCGooqjFB2o4bl3yzhQfeyt9HNS45mcmcTEMYnkjUnwH958blqEmHBoqA9LpFsKcoka\nkdgwp49P4/Txacdtq6prZE/FUYoqathTcZT3D9Swp6KG9bsOsL+yjo7XzYVDRm5a5Jhwn5DuP8Yk\nkJuWQFyMgl6GjoJcBEiJxDJ7QhqzJxwf8g1NLZQcqaX4UC3Fh476U2/+lZ3HB70ZZKfEM8HvmTNh\nTAJ5/nRCeiLj0iKkRmI0sIcMGAW5yEnExYSYnJnE5MykLrfXNzWz/0gdew/VUny4lr2HatnrT7fs\nPcLTf9vfdvK1VWJcmHFpEXLTIoxL9Zpr2pbTIuSmJTAmMVZhLz2iIBfpp/iY8AmDvqXFUV5dT7Ef\n8KVH6ig5Usf+ylpKjtSxftcBSivrjjkRC/CRKRncetVCxmiQDzkJBbnIIAuFjJzUCDmpERZOHtPl\nc5qaWzhQ3UDJkVr2H6ljV3k1v/7rTj5zy3ruXHNmt18SIqAgFxkRYsIhxvnNKq0WT83kK/cU8un/\nt57frS5gwaSuvwREdGpdZIQqyM/g4a+fQ3IkhlW3vcZ/vbN/uIskI5SCXGQEmzI2iYevPZtZ41O5\n9r6N3P7y+8NdJBmBFOQiI1xmcjwPfGUxnzx9HP/yxFZufPxvx9xjRkRBLhIAkdgwv/ncAr68ZAp3\nrS/ia/+xkVp//FQRBblIQIRCxg8umsWNF8/i2W2lrLztVcqr6k/+Qhn1FOQiAXP1OVP47VUL2V5a\nxadveYWdZdXDXSQZZhp8WSSgNn14mC/fvYG6xhYK8se03YP91HG6D/too8GXRUap+RPTeeTr5/Cr\nZ3ewraSS9bsqaGhqAbz7veR3uA/7Kf6weRPGJJAQG9al/6OMglwkwCZmJPKLFfMA7+rQooqj3v3X\nT3Af9tiwkZYQR1pCDGkJsW2P9MQ4UjssJ8eHSYqPISk+huTWaVwMifFhYnUb3xHlpEFuZhOBe4Ac\nwAG3OeduNrMM4A9APlAErHDOHRq8oorIicSEQ0zPTmZ6dnKX92Hfvr+K0sp6jtQ2cqS2wZ82Ul5d\nz46yao7UNlJV19Sj94qLCfnhHiYxNoZIXJhITIiEuDCRmLA3jQ0TiQ2RENs+Hx8TJj4mRFxM5/kQ\n8bFh4sIh4mNDxIW99a3T2HCI2LDpl0Q3elIjbwL+0Tn3ppmlABvN7BngauA559xNZnYDcAPwvcEr\nqoj0xYnuw95Zc4ujqs4L+Or6Jmrqm6mpb6K6vomjDU1U+8ut62rqm6htbKa2sYW6xmYO1jRQ19js\nrWtood6fbxqgfu/twW5e0PshHxcOtX0pxHb6EmibdpiPjwm3r4sJEd/pObExIWJDRkw4REzYiA35\n07ARGw4RE/a2x7Y+139OKDQ8XzQnDXLnXAlQ4s9Xmdk2YAJwKXCe/7S7gRdQkIsEWjhkpCfGkZ44\nsHdcbGz2gr6hqYX6ppZO0+bj5uubWmhs9ta1ThuaWmhodseua/YfTcc+quubOrym07SpZcC+WDqL\nCVmHXxAh4sLmB32I21cXDNrNz3rVRm5m+cAZwOtAjh/yAPvxml66es01wDUAkyZN6ms5RSTAWoNt\npGhucW2hXt/c3D7fFvQtNDY7mpodjS0tNDU7mppbaGxxNB6z3Zs2dPjSaeywrtH/AmlsbhnUXkQ9\nDnIzSwb+BHzbOVfZsa3KOefMrMuvOOfcbcBt4HU/7F9xRUT6LxwyEuLC/iDbscNdnH7r0VekmcXi\nhfh9zrmH/dWlZpbrb88FyganiCIiciInDXLzqt63A9ucc//WYdPjwGp/fjXw2MAXT0RETqYnTSvn\nAJ8HtpjZJn/d94GbgAfN7EvAHmDF4BRRREROpCe9Vl4GuutTs2xgiyMiIr01ck4ji4hInyjIRUQC\nTkEuIhJwCnIRkYAb0vuRm1k5Xg+XvhgLHBjA4owEo+2YdDwj32g7ptF2PND1MU12zmV194IhDfL+\nMLPCE91YPYhG2zHpeEa+0XZMo+14oG/HpKYVEZGAU5CLiARckIL8tuEuwCAYbcek4xn5Rtsxjbbj\ngT4cU2DayEVEpGtBqpGLiEgXFOQiIgEXiCA3s0+a2XYz2+mPDxpoZlZkZlvMbJOZFQ53efrCzO4w\nszIze6fDugwze8bMdvjTMcNZxt7o5nhuNLO9/ue0ycwuHM4y9oaZTTSz581sq5n9zcy+5a8P8mfU\n3TEF8nMys4iZvWFmm/3j+ZG/foqZve7n3R/M7KTj7o34NnIzCwPvAecDxcAGYJVzbuuwFqwfzKwI\nKHDOBfZCBjM7F6gG7nHOzfbX/R/gYIcBucc45wIxjms3x3MjUO2c+/lwlq0v/MFecjsOmg5chjdo\nelA/o+6OaQUB/Jz8sR6SnHPV/uA9LwPfAq4HHnbOrTWzW4HNzrlbTrSvINTIFwE7nXO7nXMNwFq8\ngZ9lGDnnXgIOdlp9Kd5A3PjTy4a0UP3QzfEElnOuxDn3pj9fBXQcND2on1F3xxRIzlPtL8b6Dwd8\nHHjIX9+jzygIQT4B+LDDcjEB/vB8DviLmW30B6ceLXo0IHfAfMPM3vabXgLTDNFRXwZNH+k6HRME\n9HMys7A/YE8Z8AywCzjsnGvyn9KjvAtCkI9GS5xzC4ALgOv8n/WjivPa7EZ2u93J3QJMA+YDJcAv\nhrc4vdd50PSO24L6GXVxTIH9nJxzzc65+UAeXuvDaX3ZTxCCfC8wscNynr8usJxze/1pGfAI3gc4\nGoyqAbmdc6X+f7QW4HcE7HMajYOmd3VMQf+cAJxzh4HngbOAdDNrHb2tR3kXhCDfAMzwz+TGASvx\nBn4OJDNL8k/UYGZJwCeAd078qsAYVQNytwae73IC9DmNxkHTuzumoH5OZpZlZun+fAJeh45teIF+\nhf+0Hn1GI77XCoDfnehXQBi4wzn3r8NcpD4zs6l4tXDwxky9P4jHY2YPAOfh3XKzFPgh8CjwIDAJ\nf0Bu51wgTiB2czzn4f1cd0AR8NUO7csjmpktAdYBW4AWf/X38dqUg/oZdXdMqwjg52Rmc/FOZobx\nKtUPOud+7GfEWiADeAu4yjlXf8J9BSHIRUSke0FoWhERkRNQkIuIBJyCXEQk4BTkIiIBpyAXEQk4\nBbmISMApyEVEAu7/A+S2Gy1K1Z7sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = numpy.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_vWrwPgbIik"
   },
   "source": [
    "# Part 2 Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T01:08:33.607809Z",
     "start_time": "2019-11-13T01:08:33.588873Z"
    }
   },
   "source": [
    "# 2.1 Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T01:09:27.773017Z",
     "start_time": "2019-11-13T01:09:27.730952Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChatDictionary(object):\n",
    "    \"\"\"\n",
    "    Simple dict loader\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_file_path):\n",
    "        self.word2ind = {}  # word:index\n",
    "        self.ind2word = {}  # index:word\n",
    "        self.counts = {}  # word:count\n",
    "\n",
    "        dict_raw = open(dict_file_path, 'r').readlines()\n",
    "        \n",
    "        for i, w in enumerate(dict_raw):\n",
    "            _word, _count = w.strip().split('\\t')\n",
    "            if _word == '\\\\n':\n",
    "                _word = '\\n'\n",
    "            self.word2ind[_word] = i\n",
    "            self.ind2word[i] = _word\n",
    "            self.counts[_word] = _count\n",
    "            \n",
    "    def t2v(self, tokenized_text):\n",
    "        return [self.word2ind[w] if w in self.counts else self.word2ind['__unk__'] for w in tokenized_text]\n",
    "\n",
    "    def v2t(self, list_ids):\n",
    "        return ' '.join([self.ind2word[i] for i in list_ids])\n",
    "    \n",
    "    def pred2text(self, tensor):\n",
    "        result = []\n",
    "        for i in range(tensor.size(0)):\n",
    "            if tensor[i].item() == '__end__'  or tensor[i].item() == '__null__':  # null is pad\n",
    "                break\n",
    "            else:\n",
    "                result.append(self.ind2word[tensor[i].item()])\n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T01:09:52.995105Z",
     "start_time": "2019-11-13T01:09:52.987078Z"
    }
   },
   "outputs": [],
   "source": [
    "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)\n",
    "class ChatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Json dataset wrapper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_file_path, dictionary, dt='train'):\n",
    "        super().__init__()\n",
    "        \n",
    "        json_text = open(dataset_file_path, 'r').readlines()\n",
    "        self.samples = []\n",
    "        \n",
    "        for sample in tqdm(json_text):\n",
    "            sample = sample.rstrip()\n",
    "            sample = json.loads(sample)\n",
    "            _inp_toked = RETOK.findall(sample['text'])\n",
    "            _inp_toked_id = dictionary.t2v(_inp_toked)\n",
    "\n",
    "            sample['text_vec'] = torch.tensor(_inp_toked_id, dtype=torch.long)\n",
    "            \n",
    "            # train and valid have different key names for target\n",
    "            if dt == 'train':\n",
    "                _tar_toked = RETOK.findall(sample['labels'][0]) + ['__end__']\n",
    "            elif dt == 'valid':\n",
    "                _tar_toked = RETOK.findall(sample['eval_labels'][0]) + ['__end__']\n",
    "                \n",
    "            _tar_toked_id = dictionary.t2v(_tar_toked)\n",
    "            \n",
    "            sample['target_vec'] = torch.tensor(_tar_toked_id, dtype=torch.long)\n",
    "            \n",
    "            self.samples.append(sample)\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]['text_vec'], self.samples[i]['target_vec']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T01:10:06.344157Z",
     "start_time": "2019-11-13T01:10:06.334755Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_tensor(tensors, sort=True, pad_token=0):\n",
    "    rows = len(tensors)\n",
    "    lengths = [len(i) for i in tensors]\n",
    "    max_t = max(lengths)\n",
    "        \n",
    "    output = tensors[0].new(rows, max_t)\n",
    "    output.fill_(pad_token)  # 0 is a pad token here\n",
    "    \n",
    "    for i, (tensor, length) in enumerate(zip(tensors, lengths)):\n",
    "        output[i,:length] = tensor\n",
    "\n",
    "    return output, lengths\n",
    "\n",
    "def argsort(keys, *lists, descending=False):\n",
    "    \"\"\"Reorder each list in lists by the (descending) sorted order of keys.\n",
    "    :param iter keys: Keys to order by.\n",
    "    :param list[list] lists: Lists to reordered by keys's order.\n",
    "                             Correctly handles lists and 1-D tensors.\n",
    "    :param bool descending: Use descending order if true.\n",
    "    :returns: The reordered items.\n",
    "    \"\"\"\n",
    "    ind_sorted = sorted(range(len(keys)), key=lambda k: keys[k])\n",
    "    if descending:\n",
    "        ind_sorted = list(reversed(ind_sorted))\n",
    "    output = []\n",
    "    for lst in lists:\n",
    "        if isinstance(lst, torch.Tensor):\n",
    "            output.append(lst[ind_sorted])\n",
    "        else:\n",
    "            output.append([lst[i] for i in ind_sorted])\n",
    "    return output\n",
    "\n",
    "def batchify(batch):\n",
    "    inputs = [i[0] for i in batch]\n",
    "    labels = [i[1] for i in batch]\n",
    "    \n",
    "    input_vecs, input_lens = pad_tensor(inputs)\n",
    "    label_vecs, label_lens = pad_tensor(labels)\n",
    "    \n",
    "    # sort only wrt inputs here for encoder packinng\n",
    "    input_vecs, input_lens, label_vecs, label_lens = argsort(input_lens, input_vecs, input_lens, label_vecs, label_lens, descending=True)\n",
    "\n",
    "    return {\n",
    "        \"text_vecs\": input_vecs,\n",
    "        \"text_lens\": input_lens,\n",
    "        \"target_vecs\": label_vecs,\n",
    "        \"target_lens\": label_lens,\n",
    "        'use_packed': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=256)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab code from chat.ipynb\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Encodes the input context.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        if shared_lt is None:\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n",
    "        else:\n",
    "            self.embedding = shared_lt\n",
    "            \n",
    "        self.gru = nn.GRU(\n",
    "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, text_vec, text_lens, hidden=None, use_packed=True):\n",
    "        embedded = self.embedding(text_vec)\n",
    "        attention_mask = text_vec.ne(self.pad_idx) # ne function calculates not equal to element-wise\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        if use_packed is True:\n",
    "            embedded = pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        if use_packed is True:\n",
    "            output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        return output, hidden, attention_mask\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.longest_label = 100\n",
    "\n",
    "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
    "        emb = self.embedding(text_vec)\n",
    "        emb = self.dropout(emb)\n",
    "        seqlen = text_vec.size(1)\n",
    "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "        \n",
    "        decoder_hidden = decoder_hidden\n",
    "        output = []\n",
    "        attn_w_log = []\n",
    "\n",
    "        for i in range(seqlen):\n",
    "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
    "            \n",
    "            # compute attention at each time step\n",
    "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
    "            output.append(decoder_output_attended)\n",
    "            attn_w_log.append(attn_weights)\n",
    "            \n",
    "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
    "        scores = self.out(output)\n",
    "        \n",
    "        return scores, decoder_hidden, attn_w_log\n",
    "    \n",
    "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
    "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "        \n",
    "        batch_size = ys.size(0)\n",
    "        target_length = ys.size(1)\n",
    "        longest_label = max(target_length, self.longest_label)\n",
    "        \n",
    "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "        \n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
    "        decoder_input = torch.cat([starts, y_in], 1)\n",
    "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
    "        _, preds = decoder_output.max(dim=2)\n",
    "        \n",
    "        return decoder_output, preds, attn_w_log\n",
    "    \n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        input_dim = hidden_size\n",
    "\n",
    "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
    "\n",
    "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
    "\n",
    "        encoder_output_t = encoder_output.transpose(1,2)\n",
    "        \n",
    "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
    "\n",
    "        attention_scores.masked_fill_((~attention_mask), -10e5)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
    "\n",
    "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
    "\n",
    "        output = self.linear_out(combined).unsqueeze(1)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "class seq2seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic seq2seq model with attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, opts):\n",
    "\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        \n",
    "        self.decoder = DecoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_dec'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                )\n",
    "        \n",
    "        self.encoder = EncoderRNN(\n",
    "                                    vocab_size=self.opts['vocab_size'],\n",
    "                                    embed_size=self.opts['embedding_size'],\n",
    "                                    hidden_size=self.opts['hidden_size'],\n",
    "                                    num_layers=self.opts['num_layers_enc'],\n",
    "                                    dropout=self.opts['dropout'],\n",
    "                                    shared_lt=self.decoder.embedding\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "load_pretrained = True\n",
    "    \n",
    "if load_pretrained is True:\n",
    "    if current_device == 'cuda':\n",
    "        model_pt = torch.load('./chat_model_best_22.pt')\n",
    "    else:\n",
    "        model_pt = torch.load('./chat_model_best_22.pt', map_location=torch.device('cpu'))\n",
    "    opts = model_pt['opts']\n",
    "    \n",
    "    model = seq2seq(opts)\n",
    "    model.load_state_dict(model_pt['state_dict'])\n",
    "    model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement nucleus sampling here.\n",
    "# you must cite any code you use from other sources!\n",
    "import numpy as np\n",
    "\n",
    "def nucleus_sampling(model, batch, batch_size, prob_thre):\n",
    "    \"\"\"\n",
    "    batch: the input batch which we want to sample the output on\n",
    "    batch_size: the relative batch size\n",
    "    prob_thre: the probability threshold to choose top p next token\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text_vecs = batch['text_vecs'].to(current_device)\n",
    "        encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
    "        encoder_output, encoder_hidden, attention_mask = encoded\n",
    "\n",
    "        # First define __start__ as the starting point of our sampling\n",
    "        starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Start sampling\n",
    "        samp = [starts]\n",
    "        probs_selecting = []\n",
    "\n",
    "        # We want to stop when every sequence has 2 as their ends (2 == __end__)\n",
    "        finish_mask = torch.Tensor([0]*batch_size).byte().to(model.decoder.embedding.weight.device)\n",
    "        current_sequences = starts\n",
    "        _attn_w_log = []\n",
    "\n",
    "        def find_indices(sorted_prob, prob_thre): \n",
    "            # Given a sorted probs, return the number of probs and sum of probs that could add up to >= threshold     \n",
    "            sum_now = 0\n",
    "            count = 0\n",
    "            while sum_now < prob_thre:\n",
    "                sum_now += sorted_prob[count].item()\n",
    "                count += 1\n",
    "            return count, sum_now\n",
    "\n",
    "        for time_step in range(1000):\n",
    "            decoder_output, decoder_hidden, attn_w_log = model.decoder(current_sequences, decoder_hidden, encoded)\n",
    "            all_probs = torch.softmax(decoder_output, dim=-1)\n",
    "            new_samp_all = []\n",
    "            new_prob_list = []\n",
    "            for i in range(batch_size):\n",
    "                single_next = all_probs[i, :, :].squeeze()\n",
    "                _probs, _indices = torch.sort(single_next, descending=True)\n",
    "                count, sum_p = find_indices(_probs, prob_thre)\n",
    "                new_probs = _probs[:count] / sum_p\n",
    "                _indices = _indices[:count]\n",
    "                # Now sample an index from the new conditional distribution\n",
    "                new_pred = _indices[torch.multinomial(new_probs, 1)].view(-1, 1).long()\n",
    "                prob_for_this_token = _probs[new_pred.item()].view(-1, 1)\n",
    "                #new_pred = torch.Tensor([np.random.choice(_indices, p = _probs)]).view(-1, 1).long()\n",
    "                new_samp_all.append(new_pred)\n",
    "                new_prob_list.append(prob_for_this_token)\n",
    "\n",
    "            _samp = torch.cat(new_samp_all, dim=1).to(model.decoder.embedding.weight.device)\n",
    "            prob_batch = torch.cat(new_prob_list, dim=1).to(model.decoder.embedding.weight.device)\n",
    "            samp.append(_samp)\n",
    "            probs_selecting.append(prob_batch)\n",
    "            _attn_w_log.append(attn_w_log)\n",
    "\n",
    "            finish_mask += (_samp == 2).byte().view(-1)\n",
    "            \n",
    "            if not (torch.any(~finish_mask.bool())): # ~ is invert\n",
    "                break\n",
    "            \n",
    "            current_sequences = _samp\n",
    "        \n",
    "        samp = torch.cat(samp, dim=-1)\n",
    "        probs_selecting = torch.cat(probs_selecting, dim=-1)\n",
    "            \n",
    "    return samp, probs_selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1573601935583,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "SpkzyUlbxn-Q",
    "outputId": "1d415474-acc2-453b-b3fe-8d309f8a7260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__start__ i am from california , where are you from ? __end__\n",
      "-7.423248277044979\n"
     ]
    }
   ],
   "source": [
    "# A test run\n",
    "inputs = RETOK.findall(\"your persona: i live in texas.\\n hello , where are you ? ?\")\n",
    "\n",
    "test_batch = {\n",
    "    'text_vecs': torch.tensor([chat_dict.t2v(inputs)], dtype=torch.long, device=model.decoder.embedding.weight.device),\n",
    "    'text_lens': torch.tensor([len(inputs)], dtype=torch.long),\n",
    "    'use_packed': True,\n",
    "}\n",
    "\n",
    "sample, probs = nucleus_sampling(model, test_batch, 1, 0.1)#.tolist()\n",
    "print(chat_dict.v2t(*sample.tolist()))\n",
    "print(np.average(np.log(probs.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36564,
     "status": "ok",
     "timestamp": 1573591755797,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "athOySwpfeTj",
    "outputId": "2ad872b4-2018-4ad2-a024-a6945599763e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input is: your persona : i read twenty books a year . \n",
      " your persona : i ' m a stunt double as my second job . \n",
      " your persona : i only eat __unk__ . \n",
      " your persona : i was raised in a single parent household . \n",
      " hello what are doing today ?\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Section 2.1\n",
    "# Q2 and Q3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_decodings = 100\n",
    "prob_thre_list = [0.1, 0.5, 0.9]\n",
    "# Selecting a single input from the validation set\n",
    "index = 0\n",
    "valid_input = valid_dataset[index][0]\n",
    "print(f\"Our input is: {chat_dict.v2t(valid_input.tolist())}\")\n",
    "test_input = {\n",
    "    'text_vecs': valid_input.view(1, -1).to(model.decoder.embedding.weight.device),\n",
    "    'text_lens': torch.tensor([len(valid_input)], dtype=torch.long),\n",
    "    'use_packed': True,\n",
    "}\n",
    "\n",
    "# Compute the average log-probability of generated sequences for each value of prob_thre\n",
    "# Also store the samples\n",
    "log_p_dict = {}\n",
    "all_samples = {}\n",
    "for thre in prob_thre_list:\n",
    "    log_p_dict[thre] = []\n",
    "    all_samples[thre] = []\n",
    "    for i in range(number_decodings):\n",
    "        sample, probs = nucleus_sampling(model, test_input, 1, thre)\n",
    "        # Store the sample\n",
    "        all_samples[thre].append(*sample.tolist())\n",
    "        # Compute log_p and add them up\n",
    "        log_p_total = torch.mean(torch.log(probs))\n",
    "        log_p_dict[thre].append(log_p_total)\n",
    "\n",
    "avg_log_p = [np.mean(log_p_dict[i]) for i in log_p_dict]\n",
    "plt.plot(prob_thre_list, avg_log_p)\n",
    "plt.title(\"Average Log Probability vs. Probability threshold\")\n",
    "plt.xticks([0.1, 0.5, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1573591840279,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "U8W1aCZHfeTl",
    "outputId": "86bb9e72-b411-4537-df5e-8d9d621a20e1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAL9f9r9aI5Cfj14AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3's plot\n",
    "unique_token_count = []\n",
    "for thre in all_samples:\n",
    "    all_tokens = []\n",
    "    [all_tokens.extend(l) for l in all_samples[thre]]\n",
    "    unique_token_count.append(len(set(all_tokens)))\n",
    "\n",
    "# Plot\n",
    "plt.plot(prob_thre_list, unique_token_count)\n",
    "plt.title(\"Unique tokens count for different prob. threshold\")\n",
    "plt.xticks([0.1, 0.5, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HD6-tEpbIis"
   },
   "source": [
    "## 2.2 N-Gram blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3XokS7k02O5J"
   },
   "outputs": [],
   "source": [
    "model_name = './chat_model_best_22.pt'\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if current_device == 'cuda':\n",
    "    model_pt = torch.load(model_name)\n",
    "else:\n",
    "    model_pt = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "opts = model_pt['opts']\n",
    "\n",
    "model = seq2seq(opts)\n",
    "model.load_state_dict(model_pt['state_dict'])\n",
    "model.to(current_device)\n",
    "\n",
    "plot_cache = model_pt['plot_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TVszNFTbbIiu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    \"\"\"\n",
    "    This class serves to keep info about partial hypothesis and perform the beam step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        beam_size,\n",
    "        padding_token=0,\n",
    "        bos_token=1,\n",
    "        eos_token=2,\n",
    "        min_length=3,\n",
    "        min_n_best=3,\n",
    "        device='cpu',\n",
    "        # for iterbeam below\n",
    "        similarity_metric='hamming',\n",
    "        similarity_threshold=0,\n",
    "        N = 3\n",
    "    ):\n",
    "        \n",
    "        self.beam_size = beam_size\n",
    "        self.min_length = min_length\n",
    "        self.eos = eos_token\n",
    "        self.bos = bos_token\n",
    "        self.pad = padding_token\n",
    "        self.device = device\n",
    "        # recent score for each hypo in the beam\n",
    "        self.scores = None\n",
    "        # self.scores values per each time step\n",
    "        self.all_scores = [torch.Tensor([0.0] * beam_size).to(self.device)]\n",
    "        # backtracking id to hypothesis at previous time step\n",
    "        self.bookkeep = []\n",
    "        # output tokens at each time step\n",
    "        self.outputs = [\n",
    "            torch.Tensor(self.beam_size).long().fill_(self.bos).to(self.device)\n",
    "        ]\n",
    "        # keeps tuples (score, time_step, hyp_id)\n",
    "        self.finished = []\n",
    "        self.eos_top = False\n",
    "        self.eos_top_ts = None\n",
    "        self.n_best_counter = 0\n",
    "        self.min_n_best = min_n_best\n",
    "        self.partial_hyps = [[self.bos] for i in range(beam_size)]\n",
    "\n",
    "        # iterbeam related below\n",
    "        self.history_hyps = []\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.banned_tokens = set()\n",
    "        \n",
    "        ### N-gram blocking\n",
    "        self.N = N\n",
    "      \n",
    "    def get_output_from_current_step(self):\n",
    "        \"\"\"Get the output at the current step.\"\"\"\n",
    "        return self.outputs[-1]\n",
    "\n",
    "    def get_backtrack_from_current_step(self):\n",
    "        \"\"\"Get the backtrack at the current step.\"\"\"\n",
    "        return self.bookkeep[-1]\n",
    "    \n",
    "    ##################### ITER-BEAM BLOCKING PART START #####################\n",
    "    \n",
    "    def hamming_distance(self, t1, t2):\n",
    "        dist = 0\n",
    "        for tok1, tok2 in zip(t1,t2):\n",
    "            if tok1 != tok2:\n",
    "                dist += 1\n",
    "        return dist\n",
    "    \n",
    "    def edit_distance(self, t1, t2):\n",
    "        import editdistance\n",
    "        dist = editdistance.eval(t1, t2)\n",
    "        return dist\n",
    "                \n",
    "    def similarity_check(self, active_hyp, previous_hyps, metric='hamming', threshold=0):\n",
    "        #print('active_hyp', active_hyp)\n",
    "        banned_tokens = []\n",
    "        active_len = len(active_hyp)\n",
    "        for observed_hyp, _banned_tokens in previous_hyps.items():\n",
    "            if len(observed_hyp) != active_len:\n",
    "                continue\n",
    "            if metric == 'hamming':\n",
    "                dist = self.hamming_distance(observed_hyp, active_hyp)\n",
    "            if metric == 'edit':\n",
    "                dist = self.edit_distance(observed_hyp, active_hyp)\n",
    "            if dist <= threshold:\n",
    "                banned_tokens.extend(_banned_tokens)\n",
    "                    \n",
    "        return list(set(banned_tokens))\n",
    "\n",
    "    #######################################################\n",
    "    ########## Add ngram blocking function here ###########\n",
    "    #######################################################\n",
    "    \n",
    "    def get_ngram_blocking(self, text_vec, N = 3):\n",
    "        # print('Start n-gram blocking for ', text_vec)\n",
    "        banned = []\n",
    "        \n",
    "        if N == 1:  # special case: unigram\n",
    "            banned = [i for i in text_vec[:-1] if i == text_vec[-1]]\n",
    "        else:\n",
    "            history = tuple(text_vec[-N+1:])\n",
    "            for ngram in zip(*[text_vec[i:] for i in range(N)]):\n",
    "                if ngram[:-1] == history:\n",
    "                    banned.append(ngram[-1])\n",
    "\n",
    "        return list(set(banned)) \n",
    "    \n",
    "    ##################### ITER-BEAM BLOCKING PART END ########################\n",
    "    \n",
    "    def select_paths(self, logprobs, prior_scores, previous_hyps):\n",
    "        \"\"\"Select the next vocabulary item in these beams.\"\"\"\n",
    "        # beam search actually looks over all hypotheses together so we flatten\n",
    "        beam_scores = logprobs + prior_scores.unsqueeze(1).expand_as(logprobs)\n",
    "        # print('shape of beam score', beam_scores.size())\n",
    "        # iterbeam blocking part\n",
    "        current_length = len(self.all_scores)\n",
    "        # print('all scores length', len(self.all_scores))\n",
    "        if len(previous_hyps) > 0 and current_length > 0:\n",
    "            for hyp_id in range(beam_scores.size(0)):\n",
    "                active_hyp = tuple(self.partial_hyps[hyp_id])\n",
    "                banned_tokens = self.similarity_check(active_hyp, previous_hyps, metric=self.similarity_metric, threshold=self.similarity_threshold)\n",
    "                if len(banned_tokens) > 0:\n",
    "                    beam_scores[:, banned_tokens] = -10e5\n",
    "      \n",
    "        ############################################################ \n",
    "        ################# Insert n-gram blocking here ##############\n",
    "        ############################################################\n",
    "        if self.N:\n",
    "            for hyp_id in range(beam_scores.size(0)):\n",
    "                active_hyp = self.partial_hyps[hyp_id]\n",
    "                # print('hyp_id: ', hyp_id)\n",
    "                # print('active hyp ids: ', active_hyp)\n",
    "                # print('active hyp tokens: ',chat_dict.v2t(active_hyp))\n",
    "\n",
    "                if len(active_hyp) >= self.N + 1:\n",
    "                    banned_tokens = self.get_ngram_blocking(active_hyp, self.N)\n",
    "                    if len(banned_tokens) > 0:\n",
    "                        print(f'Beam {hyp_id}: banned ids are:  {banned_tokens}, tokens: {chat_dict.v2t(banned_tokens)}')\n",
    "                        beam_scores[hyp_id, banned_tokens] = -10e5\n",
    "\n",
    "        flat_beam_scores = beam_scores.view(-1)\n",
    "       \n",
    "        best_scores, best_idxs = torch.topk(flat_beam_scores, self.beam_size, dim=-1)\n",
    "\n",
    "        # change the best_scores here \n",
    "        voc_size = logprobs.size(-1)\n",
    "\n",
    "        # get the backtracking hypothesis id as a multiple of full voc_sizes\n",
    "        hyp_ids = best_idxs / voc_size\n",
    "        # get the actual word id from residual of the same division\n",
    "        tok_ids = best_idxs % voc_size\n",
    "        # print('tok_ids', chat_dict.v2t(tok_ids.cpu().tolist()))\n",
    "        return (hyp_ids, tok_ids, best_scores)\n",
    "    \n",
    "    def advance(self, logprobs, previous_hyps):\n",
    "        \"\"\"Advance the beam one step.\"\"\"\n",
    "        current_length = len(self.all_scores) - 1\n",
    "        if current_length < self.min_length:\n",
    "            # penalize all eos probs to make it decode longer\n",
    "            for hyp_id in range(logprobs.size(0)):\n",
    "                logprobs[hyp_id][self.eos] = -10e5\n",
    "\n",
    "        if self.scores is None:\n",
    "            logprobs = logprobs[0:1]  # we use only the first hyp now, since they are all same\n",
    "            self.scores = torch.zeros(1).type_as(logprobs).to(logprobs.device)\n",
    "            \n",
    "        hyp_ids, tok_ids, self.scores = self.select_paths(logprobs, self.scores, previous_hyps)\n",
    "\n",
    "        # clone scores here to avoid referencing penalized EOS in the future!\n",
    "        self.all_scores.append(self.scores.clone())\n",
    "        # print('self.all_scores', self.all_scores)\n",
    "        self.outputs.append(tok_ids)\n",
    "        self.bookkeep.append(hyp_ids)\n",
    "        self.partial_hyps = [\n",
    "            self.partial_hyps[hyp_ids[i]] + [tok_ids[i].item()]\n",
    "            for i in range(self.beam_size)\n",
    "        ]\n",
    "        self.history_hyps.extend(self.partial_hyps)\n",
    "        \n",
    "    \n",
    "        # print('history hyps ids: ', self.history_hyps)\n",
    "        # print('parital hyps at this step: ', self.partial_hyps)\n",
    "        # print('log prob size',logprobs.size(0), logprobs.size(1))\n",
    "        \n",
    "        #print('history hyps tokens: ', chat_dict.v2t(self.history_hyps.cpu().tolist()))\n",
    "\n",
    "        #  check new hypos for eos label, if we have some, add to finished\n",
    "        for hypid in range(self.beam_size):\n",
    "            if self.outputs[-1][hypid] == self.eos:\n",
    "                self.scores[hypid] = -10e5\n",
    "                #  this is finished hypo, adding to finished\n",
    "                eostail = _HypothesisTail(\n",
    "                    timestep=len(self.outputs) - 1,\n",
    "                    hypid=hypid,\n",
    "                    score=self.all_scores[-1][hypid],\n",
    "                    tokenid=self.eos,\n",
    "                )\n",
    "                self.finished.append(eostail)\n",
    "                self.n_best_counter += 1\n",
    "\n",
    "        if self.outputs[-1][0] == self.eos:\n",
    "            self.eos_top = True\n",
    "            if self.eos_top_ts is None:\n",
    "                self.eos_top_ts = len(self.outputs) - 1\n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"Return whether beam search is complete.\"\"\"\n",
    "        return self.eos_top and self.n_best_counter >= self.min_n_best\n",
    "\n",
    "    def get_top_hyp(self):\n",
    "        \"\"\"\n",
    "        Get single best hypothesis.\n",
    "        :return: hypothesis sequence and the final score\n",
    "        \"\"\"\n",
    "        return self._get_rescored_finished(n_best=1)[0]\n",
    "\n",
    "    def _get_hyp_from_finished(self, hypothesis_tail):\n",
    "        \"\"\"\n",
    "        Extract hypothesis ending with EOS at timestep with hyp_id.\n",
    "        :param timestep:\n",
    "            timestep with range up to len(self.outputs) - 1\n",
    "        :param hyp_id:\n",
    "            id with range up to beam_size - 1\n",
    "        :return:\n",
    "            hypothesis sequence\n",
    "        \"\"\"\n",
    "        hyp_idx = []\n",
    "        endback = hypothesis_tail.hypid\n",
    "        for i in range(hypothesis_tail.timestep, -1, -1):\n",
    "            hyp_idx.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=i,\n",
    "                    hypid=endback,\n",
    "                    score=self.all_scores[i][endback],\n",
    "                    tokenid=self.outputs[i][endback],\n",
    "                )\n",
    "            )\n",
    "            endback = self.bookkeep[i - 1][endback]\n",
    "\n",
    "        return hyp_idx\n",
    "\n",
    "    def _get_pretty_hypothesis(self, list_of_hypotails):\n",
    "        \"\"\"Return hypothesis as a tensor of token ids.\"\"\"\n",
    "        return torch.stack([ht.tokenid for ht in reversed(list_of_hypotails)])\n",
    "\n",
    "    def _get_rescored_finished(self, n_best=None, add_length_penalty=False):\n",
    "        \"\"\"\n",
    "        Return finished hypotheses according to adjusted scores.\n",
    "        Score adjustment is done according to the Google NMT paper, which\n",
    "        penalizes long utterances.\n",
    "        :param n_best:\n",
    "            number of finalized hypotheses to return\n",
    "        :return:\n",
    "            list of (tokens, score) pairs, in sorted order, where:\n",
    "              - tokens is a tensor of token ids\n",
    "              - score is the adjusted log probability of the entire utterance\n",
    "        \"\"\"\n",
    "        # if we never actually finished, force one\n",
    "        if not self.finished:\n",
    "            self.finished.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=len(self.outputs) - 1,\n",
    "                    hypid=0,\n",
    "                    score=self.all_scores[-1][0],\n",
    "                    tokenid=self.eos,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        rescored_finished = []\n",
    "        for finished_item in self.finished:\n",
    "            if add_length_penalty:\n",
    "                current_length = finished_item.timestep + 1\n",
    "                # these weights are from Google NMT paper\n",
    "                length_penalty = math.pow((1 + current_length) / 6, 0.65)\n",
    "            else:\n",
    "                length_penalty = 1\n",
    "            rescored_finished.append(\n",
    "                _HypothesisTail(\n",
    "                    timestep=finished_item.timestep,\n",
    "                    hypid=finished_item.hypid,\n",
    "                    score=finished_item.score / length_penalty,\n",
    "                    tokenid=finished_item.tokenid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Note: beam size is almost always pretty small, so sorting is cheap enough\n",
    "        srted = sorted(rescored_finished, key=attrgetter('score'), reverse=True)\n",
    "\n",
    "        if n_best is not None:\n",
    "            srted = srted[:n_best]\n",
    "\n",
    "        return [\n",
    "            (self._get_pretty_hypothesis(self._get_hyp_from_finished(hyp)), hyp.score)\n",
    "            for hyp in srted\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lTagb09bIix"
   },
   "source": [
    "## You present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kuNlVWtqbIix"
   },
   "outputs": [],
   "source": [
    "# check pdf to see what you expected to present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZluSsCyNoHuI"
   },
   "outputs": [],
   "source": [
    "def generate_with_beam(beam_size, min_n_best, model, batch, batch_size, previous_hyps=None, similarity_metric='hamming', similarity_threshold=0, verbose=False, N = 1):\n",
    "    \"\"\"\n",
    "    This function takes a model, batch, beam settings and performs decoding with a beam\n",
    "    \"\"\"\n",
    "    beams = [Beam(beam_size, min_n_best=min_n_best, eos_token=chat_dict.word2ind['__end__'], \n",
    "                  padding_token=chat_dict.word2ind['__null__'], \n",
    "                  bos_token=chat_dict.word2ind['__start__'], \n",
    "                  device=current_device, similarity_metric=similarity_metric, \n",
    "                  similarity_threshold=similarity_threshold, N = N) for _ in range(batch_size)]\n",
    "    \n",
    "    repeated_inds = torch.arange(batch_size).to(current_device).unsqueeze(1).repeat(1, beam_size).view(-1)\n",
    "\n",
    "    text_vecs = batch['text_vecs'].to(current_device)\n",
    "\n",
    "    encoder_states = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    encoder_states = reorder_encoder_states(encoder_states, repeated_inds)  # no actual reordering here, but repeating beam size times each sample in the minibatch\n",
    "    encoder_output, encoder_hidden, attention_mask = encoder_states\n",
    "    \n",
    "    incr_state = encoder_hidden  # we init decoder hidden with last encoder_hidden\n",
    "    \n",
    "    # 1 is a start token id\n",
    "    starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size*beam_size, 1).long()  # expand to batch_size * beam_size\n",
    "    decoder_input = starts\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ts in range(100):\n",
    "            if all((b.is_done() for b in beams)):\n",
    "                break\n",
    "            score, incr_state, attn_w_log = model.decoder(decoder_input, incr_state, encoder_states)\n",
    "            score = score[:, -1:, :]  # take last time step and eliminate the dimension\n",
    "            score = score.view(batch_size, beam_size, -1)\n",
    "            score = torch.log_softmax(score, dim=-1)\n",
    "         \n",
    "            for i, b in enumerate(beams):\n",
    "                if not b.is_done():\n",
    "                    # make mock previous_hyps if not used #\n",
    "                    if previous_hyps is None:\n",
    "                        previous_hyps = [{} for i in range(batch_size)]\n",
    "                    b.advance(score[i], previous_hyps[i])\n",
    "    \n",
    "            incr_state_inds = torch.cat([beam_size * i + b.get_backtrack_from_current_step() for i, b in enumerate(beams)])\n",
    "            incr_state = reorder_decoder_incremental_state(incr_state, incr_state_inds)\n",
    "            selection = torch.cat([b.get_output_from_current_step() for b in beams]).unsqueeze(-1)\n",
    "            #print('size of previous hyps', [b.get_output_from_current_step() for b in beams])\n",
    "            decoder_input = selection\n",
    "\n",
    "    beam_preds_scores = [list(b.get_top_hyp()) for b in beams]\n",
    "\n",
    "    if verbose:\n",
    "        for bi in range(batch_size):\n",
    "            print(f'batch {bi}')\n",
    "            for i in get_nbest_list_from_beam(beams[bi], chat_dict, n_best=min_n_best):\n",
    "                print(i)\n",
    "    \n",
    "    return beam_preds_scores, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "yFll2DU5ryIb",
    "outputId": "26c4c8e9-42eb-4c75-bd14-3999c8e52c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example from validation set\n",
      "Validation text:  your persona : i read twenty books a year . \n",
      " your persona : i ' m a stunt double as my second job . \n",
      " your persona : i only eat __unk__ . \n",
      " your persona : i was raised in a single parent household . \n",
      " hello what are doing today ? \n",
      " i am good , i just got off work and tired , i have two jobs . \n",
      " i just got done watching a horror movie \n",
      " i rather read , i ' ve read about 20 books this year . \n",
      " wow ! i do love a good horror movie . loving this cooler weather \n",
      " but a good movie is always good . \n",
      " yes ! my son is in junior high and i just started letting him watch them too \n",
      " i work in the movies as well . \n",
      " neat ! ! i used to work in the human services field\n",
      "Validation target:  yes it is neat , i stunt double , it is so much fun and hard work . __end__\n",
      "\n",
      "no ngram blocking\n",
      "batch 0\n",
      "('__start__ what do you do ? __end__', -5.292159080505371)\n",
      "('__start__ do you have any hobbies ? __end__', -5.544672966003418)\n",
      "('__start__ how about you ? __end__', -5.616154670715332)\n",
      "('__start__ do you have any pets ? __end__', -5.638843536376953)\n",
      "('__start__ . . . __end__', -5.7207183837890625)\n",
      "('__start__ do you work ? __end__', -5.9467267990112305)\n",
      "('__start__ do you have pets ? __end__', -6.6697998046875)\n",
      "('__start__ do you like music ? __end__', -6.675205230712891)\n",
      "('__start__ do you have kids ? __end__', -6.787569046020508)\n",
      "('__start__ . . . . __end__', -7.092815399169922)\n",
      "batch 1\n",
      "('__start__ what do you do ? __end__', -5.310277938842773)\n",
      "('__start__ do you have any hobbies ? __end__', -5.5536651611328125)\n",
      "('__start__ how about you ? __end__', -5.634095191955566)\n",
      "('__start__ do you have any pets ? __end__', -5.648006439208984)\n",
      "('__start__ . . . __end__', -5.741286277770996)\n",
      "('__start__ do you work ? __end__', -5.956805229187012)\n",
      "('__start__ do you have pets ? __end__', -6.680454254150391)\n",
      "('__start__ do you like music ? __end__', -6.687767028808594)\n",
      "('__start__ do you have kids ? __end__', -6.814296722412109)\n",
      "('__start__ . . . . __end__', -7.101889610290527)\n",
      "batch 2\n",
      "('__start__ what do you do ? __end__', -5.337267875671387)\n",
      "('__start__ do you have any hobbies ? __end__', -5.557065963745117)\n",
      "('__start__ do you have any pets ? __end__', -5.6860809326171875)\n",
      "('__start__ . . . __end__', -5.806824684143066)\n",
      "('__start__ and you ? __end__', -5.930785179138184)\n",
      "('__start__ do you work ? __end__', -5.994296073913574)\n",
      "('__start__ do you like music ? __end__', -6.683838844299316)\n",
      "('__start__ do you have pets ? __end__', -6.702122688293457)\n",
      "('__start__ . . . . __end__', -7.163710594177246)\n",
      "('__start__ what is your favorite food ? __end__', -7.2940568923950195)\n",
      "batch 3\n",
      "('__start__ what do you do ? __end__', -5.304720878601074)\n",
      "('__start__ do you have any hobbies ? __end__', -5.410431861877441)\n",
      "('__start__ do you have any pets ? __end__', -5.765810012817383)\n",
      "('__start__ do you work ? __end__', -5.948907852172852)\n",
      "('__start__ . . . __end__', -6.499058723449707)\n",
      "('__start__ do you like music ? __end__', -6.533461570739746)\n",
      "('__start__ do you have pets ? __end__', -6.654759407043457)\n",
      "('__start__ do you like to read ? __end__', -7.221473693847656)\n",
      "('__start__ what is your favorite food ? __end__', -7.249305725097656)\n",
      "('__start__ do you have any siblings ? __end__', -7.509661674499512)\n",
      "batch 4\n",
      "('__start__ how about you ? __end__', -4.127680778503418)\n",
      "('__start__ and you ? __end__', -4.697432518005371)\n",
      "('__start__ how are you ? __end__', -5.133681297302246)\n",
      "('__start__ do you have any hobbies ? __end__', -5.137869834899902)\n",
      "('__start__ what do you do ? __end__', -5.261124610900879)\n",
      "('__start__ do you have any pets ? __end__', -5.316582679748535)\n",
      "('__start__ do you work ? __end__', -5.651354789733887)\n",
      "('__start__ do you have pets ? __end__', -6.405787467956543)\n",
      "('__start__ what are you up to ? __end__', -6.4883928298950195)\n",
      "('__start__ do you like music ? __end__', -6.53204345703125)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "beam_size = 20\n",
    "beam_n_best = 10\n",
    "\n",
    "valid_loader_single = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=batch_size)\n",
    "\n",
    "valid_sample = next(iter(valid_loader_single))\n",
    "\n",
    "print('Single example from validation set')\n",
    "print('Validation text: ', chat_dict.v2t(valid_sample['text_vecs'][0].cpu().tolist()))\n",
    "print('Validation target: ', chat_dict.v2t(valid_sample['target_vecs'][0].cpu().tolist()))\n",
    "\n",
    "print('\\nno ngram blocking')\n",
    "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, \n",
    "                                              valid_sample, batch_size=batch_size, verbose=True, N = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2PaOdTnbIi5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = 1\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 6: banned ids are:  [4], tokens: .\n",
      "Beam 7: banned ids are:  [4], tokens: .\n",
      "Beam 0: banned ids are:  [17], tokens: do\n",
      "Beam 0: banned ids are:  [17], tokens: do\n",
      "Beam 0: banned ids are:  [17], tokens: do\n",
      "Beam 0: banned ids are:  [17], tokens: do\n",
      "Beam 1: banned ids are:  [17], tokens: do\n",
      "Beam 5: banned ids are:  [17], tokens: do\n",
      "Beam 5: banned ids are:  [17], tokens: do\n",
      "Beam 5: banned ids are:  [17], tokens: do\n",
      "Beam 5: banned ids are:  [17], tokens: do\n",
      "Beam 5: banned ids are:  [17], tokens: do\n",
      "batch 0\n",
      "('__start__ what do you do ? __end__', -5.292159080505371)\n",
      "('__start__ do you have any hobbies ? __end__', -5.544672966003418)\n",
      "('__start__ how about you ? __end__', -5.616154670715332)\n",
      "('__start__ do you have any pets ? __end__', -5.638843536376953)\n",
      "('__start__ do you work ? __end__', -5.9467267990112305)\n",
      "('__start__ do you have pets ? __end__', -6.6697998046875)\n",
      "('__start__ do you like music ? __end__', -6.675205230712891)\n",
      "('__start__ do you have kids ? __end__', -6.787569046020508)\n",
      "('__start__ what is your favorite food ? __end__', -7.193254470825195)\n",
      "('__start__ do you have any siblings ? __end__', -7.345789909362793)\n",
      "batch 1\n",
      "('__start__ what do you do ? __end__', -5.310277938842773)\n",
      "('__start__ do you have any hobbies ? __end__', -5.5536651611328125)\n",
      "('__start__ how about you ? __end__', -5.634095191955566)\n",
      "('__start__ do you have any pets ? __end__', -5.648006439208984)\n",
      "('__start__ do you work ? __end__', -5.956805229187012)\n",
      "('__start__ do you have pets ? __end__', -6.680454254150391)\n",
      "('__start__ do you like music ? __end__', -6.687767028808594)\n",
      "('__start__ do you have kids ? __end__', -6.814296722412109)\n",
      "('__start__ what is your favorite food ? __end__', -7.22746467590332)\n",
      "('__start__ do you have any siblings ? __end__', -7.362831115722656)\n",
      "batch 2\n",
      "('__start__ what do you do ? __end__', -5.337267875671387)\n",
      "('__start__ do you have any hobbies ? __end__', -5.557065963745117)\n",
      "('__start__ do you have any pets ? __end__', -5.6860809326171875)\n",
      "('__start__ and you ? __end__', -5.930785179138184)\n",
      "('__start__ do you work ? __end__', -5.994296073913574)\n",
      "('__start__ do you like music ? __end__', -6.683838844299316)\n",
      "('__start__ do you have pets ? __end__', -6.702122688293457)\n",
      "('__start__ do you have kids ? __end__', -6.920087814331055)\n",
      "('__start__ what is your favorite food ? __end__', -7.2940568923950195)\n",
      "('__start__ do you have any siblings ? __end__', -7.391822814941406)\n",
      "batch 3\n",
      "('__start__ what do you do ? __end__', -5.304720878601074)\n",
      "('__start__ do you have any hobbies ? __end__', -5.410431861877441)\n",
      "('__start__ do you have any pets ? __end__', -5.765810012817383)\n",
      "('__start__ do you work ? __end__', -5.948907852172852)\n",
      "('__start__ do you like music ? __end__', -6.533461570739746)\n",
      "('__start__ do you have pets ? __end__', -6.654759407043457)\n",
      "('__start__ do you like to read ? __end__', -7.221473693847656)\n",
      "('__start__ what is your favorite food ? __end__', -7.249305725097656)\n",
      "('__start__ do you have any siblings ? __end__', -7.509661674499512)\n",
      "('__start__ what is your favorite color ? __end__', -7.528674125671387)\n",
      "batch 4\n",
      "('__start__ how about you ? __end__', -4.127680778503418)\n",
      "('__start__ and you ? __end__', -4.697432518005371)\n",
      "('__start__ how are you ? __end__', -5.133681297302246)\n",
      "('__start__ do you have any hobbies ? __end__', -5.137869834899902)\n",
      "('__start__ what do you do ? __end__', -5.261124610900879)\n",
      "('__start__ do you have any pets ? __end__', -5.316582679748535)\n",
      "('__start__ do you work ? __end__', -5.651354789733887)\n",
      "('__start__ do you have pets ? __end__', -6.405787467956543)\n",
      "('__start__ what are you up to ? __end__', -6.4883928298950195)\n",
      "('__start__ do you like music ? __end__', -6.53204345703125)\n"
     ]
    }
   ],
   "source": [
    "print('ngram = 1')\n",
    "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, \n",
    "                                              valid_sample, batch_size=batch_size, verbose=True, N = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = 2\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 6: banned ids are:  [4], tokens: .\n",
      "Beam 7: banned ids are:  [4], tokens: .\n",
      "Beam 0: banned ids are:  [6], tokens: you\n",
      "Beam 0: banned ids are:  [6], tokens: you\n",
      "Beam 0: banned ids are:  [6], tokens: you\n",
      "Beam 0: banned ids are:  [6], tokens: you\n",
      "Beam 1: banned ids are:  [6], tokens: you\n",
      "Beam 5: banned ids are:  [6], tokens: you\n",
      "Beam 5: banned ids are:  [6], tokens: you\n",
      "Beam 5: banned ids are:  [6], tokens: you\n",
      "Beam 5: banned ids are:  [6], tokens: you\n",
      "Beam 5: banned ids are:  [6], tokens: you\n",
      "batch 0\n",
      "('__start__ what do you do ? __end__', -5.292159080505371)\n",
      "('__start__ do you have any hobbies ? __end__', -5.544672966003418)\n",
      "('__start__ how about you ? __end__', -5.616154670715332)\n",
      "('__start__ do you have any pets ? __end__', -5.638843536376953)\n",
      "('__start__ do you work ? __end__', -5.9467267990112305)\n",
      "('__start__ do you have pets ? __end__', -6.6697998046875)\n",
      "('__start__ do you like music ? __end__', -6.675205230712891)\n",
      "('__start__ do you have kids ? __end__', -6.787569046020508)\n",
      "('__start__ what is your favorite food ? __end__', -7.193254470825195)\n",
      "('__start__ do you have any siblings ? __end__', -7.345789909362793)\n",
      "batch 1\n",
      "('__start__ what do you do ? __end__', -5.310277938842773)\n",
      "('__start__ do you have any hobbies ? __end__', -5.5536651611328125)\n",
      "('__start__ how about you ? __end__', -5.634095191955566)\n",
      "('__start__ do you have any pets ? __end__', -5.648006439208984)\n",
      "('__start__ do you work ? __end__', -5.956805229187012)\n",
      "('__start__ do you have pets ? __end__', -6.680454254150391)\n",
      "('__start__ do you like music ? __end__', -6.687767028808594)\n",
      "('__start__ do you have kids ? __end__', -6.814296722412109)\n",
      "('__start__ what is your favorite food ? __end__', -7.22746467590332)\n",
      "('__start__ do you have any siblings ? __end__', -7.362831115722656)\n",
      "batch 2\n",
      "('__start__ what do you do ? __end__', -5.337267875671387)\n",
      "('__start__ do you have any hobbies ? __end__', -5.557065963745117)\n",
      "('__start__ do you have any pets ? __end__', -5.6860809326171875)\n",
      "('__start__ and you ? __end__', -5.930785179138184)\n",
      "('__start__ do you work ? __end__', -5.994296073913574)\n",
      "('__start__ do you like music ? __end__', -6.683838844299316)\n",
      "('__start__ do you have pets ? __end__', -6.702122688293457)\n",
      "('__start__ do you have kids ? __end__', -6.920087814331055)\n",
      "('__start__ what is your favorite food ? __end__', -7.2940568923950195)\n",
      "('__start__ do you have any siblings ? __end__', -7.391822814941406)\n",
      "batch 3\n",
      "('__start__ what do you do ? __end__', -5.304720878601074)\n",
      "('__start__ do you have any hobbies ? __end__', -5.410431861877441)\n",
      "('__start__ do you have any pets ? __end__', -5.765810012817383)\n",
      "('__start__ do you work ? __end__', -5.948907852172852)\n",
      "('__start__ do you like music ? __end__', -6.533461570739746)\n",
      "('__start__ do you have pets ? __end__', -6.654759407043457)\n",
      "('__start__ do you like to read ? __end__', -7.221473693847656)\n",
      "('__start__ what is your favorite food ? __end__', -7.249305725097656)\n",
      "('__start__ do you have any siblings ? __end__', -7.509661674499512)\n",
      "('__start__ what is your favorite color ? __end__', -7.528674125671387)\n",
      "batch 4\n",
      "('__start__ how about you ? __end__', -4.127680778503418)\n",
      "('__start__ and you ? __end__', -4.697432518005371)\n",
      "('__start__ how are you ? __end__', -5.133681297302246)\n",
      "('__start__ do you have any hobbies ? __end__', -5.137869834899902)\n",
      "('__start__ what do you do ? __end__', -5.261124610900879)\n",
      "('__start__ do you have any pets ? __end__', -5.316582679748535)\n",
      "('__start__ do you work ? __end__', -5.651354789733887)\n",
      "('__start__ do you have pets ? __end__', -6.405787467956543)\n",
      "('__start__ what are you up to ? __end__', -6.4883928298950195)\n",
      "('__start__ do you like music ? __end__', -6.53204345703125)\n"
     ]
    }
   ],
   "source": [
    "print('ngram = 2')\n",
    "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, \n",
    "                                              valid_sample, batch_size=batch_size, verbose=True, N = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = 3\n",
      "Beam 2: banned ids are:  [4], tokens: .\n",
      "Beam 2: banned ids are:  [4], tokens: .\n",
      "Beam 2: banned ids are:  [4], tokens: .\n",
      "Beam 4: banned ids are:  [4], tokens: .\n",
      "Beam 5: banned ids are:  [4], tokens: .\n",
      "batch 0\n",
      "('__start__ what do you do ? __end__', -5.292159080505371)\n",
      "('__start__ do you have any hobbies ? __end__', -5.544672966003418)\n",
      "('__start__ how about you ? __end__', -5.616154670715332)\n",
      "('__start__ do you have any pets ? __end__', -5.638843536376953)\n",
      "('__start__ . . . __end__', -5.7207183837890625)\n",
      "('__start__ do you work ? __end__', -5.9467267990112305)\n",
      "('__start__ do you have pets ? __end__', -6.6697998046875)\n",
      "('__start__ do you like music ? __end__', -6.675205230712891)\n",
      "('__start__ do you have kids ? __end__', -6.787569046020508)\n",
      "('__start__ what is your favorite food ? __end__', -7.193254470825195)\n",
      "batch 1\n",
      "('__start__ what do you do ? __end__', -5.310277938842773)\n",
      "('__start__ do you have any hobbies ? __end__', -5.5536651611328125)\n",
      "('__start__ how about you ? __end__', -5.634095191955566)\n",
      "('__start__ do you have any pets ? __end__', -5.648006439208984)\n",
      "('__start__ . . . __end__', -5.741286277770996)\n",
      "('__start__ do you work ? __end__', -5.956805229187012)\n",
      "('__start__ do you have pets ? __end__', -6.680454254150391)\n",
      "('__start__ do you like music ? __end__', -6.687767028808594)\n",
      "('__start__ do you have kids ? __end__', -6.814296722412109)\n",
      "('__start__ what is your favorite food ? __end__', -7.22746467590332)\n",
      "batch 2\n",
      "('__start__ what do you do ? __end__', -5.337267875671387)\n",
      "('__start__ do you have any hobbies ? __end__', -5.557065963745117)\n",
      "('__start__ do you have any pets ? __end__', -5.6860809326171875)\n",
      "('__start__ . . . __end__', -5.806824684143066)\n",
      "('__start__ and you ? __end__', -5.930785179138184)\n",
      "('__start__ do you work ? __end__', -5.994296073913574)\n",
      "('__start__ do you like music ? __end__', -6.683838844299316)\n",
      "('__start__ do you have pets ? __end__', -6.702122688293457)\n",
      "('__start__ do you have kids ? __end__', -6.920087814331055)\n",
      "('__start__ what is your favorite food ? __end__', -7.2940568923950195)\n",
      "batch 3\n",
      "('__start__ what do you do ? __end__', -5.304720878601074)\n",
      "('__start__ do you have any hobbies ? __end__', -5.410431861877441)\n",
      "('__start__ do you have any pets ? __end__', -5.765810012817383)\n",
      "('__start__ do you work ? __end__', -5.948907852172852)\n",
      "('__start__ . . . __end__', -6.499058723449707)\n",
      "('__start__ do you like music ? __end__', -6.533461570739746)\n",
      "('__start__ do you have pets ? __end__', -6.654759407043457)\n",
      "('__start__ do you like to read ? __end__', -7.221473693847656)\n",
      "('__start__ what is your favorite food ? __end__', -7.249305725097656)\n",
      "('__start__ do you have any siblings ? __end__', -7.509661674499512)\n",
      "batch 4\n",
      "('__start__ how about you ? __end__', -4.127680778503418)\n",
      "('__start__ and you ? __end__', -4.697432518005371)\n",
      "('__start__ how are you ? __end__', -5.133681297302246)\n",
      "('__start__ do you have any hobbies ? __end__', -5.137869834899902)\n",
      "('__start__ what do you do ? __end__', -5.261124610900879)\n",
      "('__start__ do you have any pets ? __end__', -5.316582679748535)\n",
      "('__start__ do you work ? __end__', -5.651354789733887)\n",
      "('__start__ do you have pets ? __end__', -6.405787467956543)\n",
      "('__start__ what are you up to ? __end__', -6.4883928298950195)\n",
      "('__start__ do you like music ? __end__', -6.53204345703125)\n"
     ]
    }
   ],
   "source": [
    "print('ngram = 3')\n",
    "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, \n",
    "                                              valid_sample, batch_size=batch_size, verbose=True, N = 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezklc-zwfeTv"
   },
   "source": [
    "# Part 3 Interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8t1J0lkfeTy"
   },
   "outputs": [],
   "source": [
    "# implement logic for interactive chat here\n",
    "# the easiest is to just use python input functionality\n",
    "\n",
    "def transfer(input_text):\n",
    "    # Transfer an input to a tensor\n",
    "    _inp_tokened = RETOK.findall(input_text.lower())\n",
    "    _inp_tokened_id = chat_dict.t2v(_inp_tokened)\n",
    "    tensor_input = torch.tensor(_inp_tokened_id, dtype=torch.long)\n",
    "    return tensor_input\n",
    "\n",
    "def make_it_batch(model, input_tensor):\n",
    "    # Make a input tensor a batch\n",
    "    device_now = model.decoder.embedding.weight.device\n",
    "    return {\n",
    "        'text_vecs': input_tensor.view(1, -1).to(device_now),\n",
    "        'text_lens': torch.tensor([len(input_tensor)], dtype=torch.long),\n",
    "        'use_packed': True}\n",
    "\n",
    "def logging(chat_history):\n",
    "    print(\"\\n\")\n",
    "    print(\"Here's the full dialog history\")\n",
    "    print(chat_dict.v2t(chat_history.tolist()))\n",
    "    return None\n",
    "\n",
    "# Now we start to implement chat function\n",
    "def chat(model, chat_history, start = True):\n",
    "    new_input = input(\"What do you want to say next\\n\")\n",
    "    if new_input == \"\": # If the user don't want to continue the chat\n",
    "        logging(chat_history)\n",
    "        return None\n",
    "    else:\n",
    "        if start:\n",
    "            chat_history = transfer(chat_history)\n",
    "        new_line_id = torch.tensor(chat_dict.t2v([\"\\n\"]), dtype=torch.long)\n",
    "        # Add user's input to chat history\n",
    "        # TODO\n",
    "        chat_history = torch.cat([chat_history, new_line_id, transfer(new_input)])\n",
    "        # Now do the sampling\n",
    "        input_for_samp = make_it_batch(model, chat_history)\n",
    "        output, probs = nucleus_sampling(model, input_for_samp, 1, 0.5)\n",
    "        output = output.squeeze()[1:-1]\n",
    "        print(f\"The model says: {chat_dict.v2t(output.tolist())} \\n\")\n",
    "        new_chat = torch.cat([chat_history, new_line_id, output])\n",
    "        return chat(model, new_chat, start = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rK0ryIsXfeT0"
   },
   "source": [
    "## You present here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 111113,
     "status": "ok",
     "timestamp": 1573602192619,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "MFSRxmAXamsQ",
    "outputId": "21c1e134-db8d-4302-d9a4-f21971973e3d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to say next\n",
      "hi, where do you live?\n",
      "The model says: i live in the mountains so i ' m a country trainer \n",
      "\n",
      "What do you want to say next\n",
      "do you like climb mountains?\n",
      "The model says: i do not . i am not sure about them \n",
      "\n",
      "What do you want to say next\n",
      "where are you from?\n",
      "The model says: i am from new york city \n",
      "\n",
      "What do you want to say next\n",
      "how many children do you have?\n",
      "The model says: i have three kids , and you ? \n",
      "\n",
      "What do you want to say next\n",
      "I do not have any kids. do you like school?\n",
      "The model says: i do . what do you do for a living ? \n",
      "\n",
      "What do you want to say next\n",
      "i drive cars. do you like cars?\n",
      "The model says: i do . i ' m a huge fan of guns \n",
      "\n",
      "What do you want to say next\n",
      "\n",
      "\n",
      "\n",
      "Here's the full dialog history\n",
      "your persona : i love disneyland and mickey mouse . \n",
      " your persona : i love to spend time with my family . \n",
      " your persona : i ' m a baby delivery nurse . \n",
      " your persona : i walk three miles every day . \n",
      " \n",
      " hi , where do you live ? \n",
      " i live in the mountains so i ' m a country trainer \n",
      " do you like climb mountains ? \n",
      " i do not . i am not sure about them \n",
      " where are you from ? \n",
      " i am from new york city \n",
      " how many children do you have ? \n",
      " i have three kids , and you ? \n",
      " __unk__ do not have any kids . do you like school ? \n",
      " i do . what do you do for a living ? \n",
      " i drive cars . do you like cars ? \n",
      " i do . i ' m a huge fan of guns\n"
     ]
    }
   ],
   "source": [
    "# Dialogue sample\n",
    "# First pick a persona\n",
    "index = 20\n",
    "sample = valid_dataset[index][0]\n",
    "persona_list = chat_dict.v2t(sample.tolist()).split(\"\\n\")\n",
    "persona = \"\"\n",
    "for item in persona_list:\n",
    "    if \"your persona\" in item:\n",
    "        persona += item\n",
    "        persona += \"\\n\"\n",
    "\n",
    "# If we want to self input persona, uncooment next line        \n",
    "# persona = \"your persona: i am from china.\\nyour persona: i currently live in jersey.\\nyour persona: i come from a small town.\\nyour persona: my favorite drink is coke.\\nyour persona: i am very bad at homework.\"\n",
    "chat(model, persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 89981,
     "status": "ok",
     "timestamp": 1573602325233,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "lkwXHUVJfeT3",
    "outputId": "8be969f8-c750-455b-df18-d761659430a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your persona : i have two dogs . \n",
      " your persona : i like to work on vintage cars . \n",
      " your persona : my favorite music is country . \n",
      " your persona : i own two vintage mustangs . \n",
      "\n",
      "What do you want to say next\n",
      "how are you?\n",
      "The model says: i am good . just got back from my shift at a beach . \n",
      "\n",
      "What do you want to say next\n",
      "do you have any dogs?\n",
      "The model says: no , i am a dog person . \n",
      "\n",
      "What do you want to say next\n",
      "how many dogs do you have?\n",
      "The model says: i have a husky . we have a couple dogs . \n",
      "\n",
      "What do you want to say next\n",
      "do you like music?\n",
      "The model says: i like all music . i love to listen to rock music . \n",
      "\n",
      "What do you want to say next\n",
      "do you drive cars?\n",
      "The model says: i do . i have been everywhere . \n",
      "\n",
      "What do you want to say next\n",
      "do you have any vintage car?\n",
      "The model says: i have a job at a small town . \n",
      "\n",
      "What do you want to say next\n",
      "\n",
      "\n",
      "\n",
      "Here's the full dialog history\n",
      "your persona : i have two dogs . \n",
      " your persona : i like to work on vintage cars . \n",
      " your persona : my favorite music is country . \n",
      " your persona : i own two vintage mustangs . \n",
      " \n",
      " how are you ? \n",
      " i am good . just got back from my shift at a beach . \n",
      " do you have any dogs ? \n",
      " no , i am a dog person . \n",
      " how many dogs do you have ? \n",
      " i have a husky . we have a couple dogs . \n",
      " do you like music ? \n",
      " i like all music . i love to listen to rock music . \n",
      " do you drive cars ? \n",
      " i do . i have been everywhere . \n",
      " do you have any vintage car ? \n",
      " i have a job at a small town .\n"
     ]
    }
   ],
   "source": [
    "# Dialogue sample\n",
    "index = 12\n",
    "sample = valid_dataset[index][0]\n",
    "persona_list = chat_dict.v2t(sample.tolist()).split(\"\\n\")\n",
    "persona = \"\"\n",
    "for item in persona_list:\n",
    "    if \"your persona\" in item:\n",
    "        persona += item\n",
    "        persona += \"\\n\"\n",
    "print(persona)\n",
    "\n",
    "chat(model, persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 89774,
     "status": "ok",
     "timestamp": 1573602433548,
     "user": {
      "displayName": "Xiao Li",
      "photoUrl": "",
      "userId": "17628369663427228897"
     },
     "user_tz": 300
    },
    "id": "-yrTqhAqAHqf",
    "outputId": "fc193c48-c497-44fa-94e9-88c5489de3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your persona : i ' m a little girl . \n",
      " your persona : i ' ve superpowers . \n",
      " your persona : i like to make time stop . \n",
      " your persona : i live in the cloud . \n",
      "\n",
      "What do you want to say next\n",
      "how are you?\n",
      "The model says: i am doing good , thanks . how are you ? \n",
      "\n",
      "What do you want to say next\n",
      "i am ok. where do you live?\n",
      "The model says: i live in the suburbs . i love to surf . \n",
      "\n",
      "What do you want to say next\n",
      "are you boy or girl?\n",
      "The model says: i ' m not sure what that ' s . \n",
      "\n",
      "What do you want to say next\n",
      "do you like superpowers?\n",
      "The model says: yes , i ' ve not . i ' m very shy . \n",
      "\n",
      "What do you want to say next\n",
      "so you are a shy girl.\n",
      "The model says: yes , i am in high school . \n",
      "\n",
      "What do you want to say next\n",
      "do you have any hobby?\n",
      "The model says: no , i don ' t . i like to hike . \n",
      "\n",
      "What do you want to say next\n",
      "\n",
      "\n",
      "\n",
      "Here's the full dialog history\n",
      "your persona : i ' m a little girl . \n",
      " your persona : i ' ve superpowers . \n",
      " your persona : i like to make time stop . \n",
      " your persona : i live in the cloud . \n",
      " \n",
      " how are you ? \n",
      " i am doing good , thanks . how are you ? \n",
      " i am ok . where do you live ? \n",
      " i live in the suburbs . i love to surf . \n",
      " are you boy or girl ? \n",
      " i ' m not sure what that ' s . \n",
      " do you like superpowers ? \n",
      " yes , i ' ve not . i ' m very shy . \n",
      " so you are a shy girl . \n",
      " yes , i am in high school . \n",
      " do you have any hobby ? \n",
      " no , i don ' t . i like to hike .\n"
     ]
    }
   ],
   "source": [
    "# Dialogue sample\n",
    "index = 35\n",
    "sample = valid_dataset[index][0]\n",
    "persona_list = chat_dict.v2t(sample.tolist()).split(\"\\n\")\n",
    "persona = \"\"\n",
    "for item in persona_list:\n",
    "    if \"your persona\" in item:\n",
    "        persona += item\n",
    "        persona += \"\\n\"\n",
    "print(persona)\n",
    "\n",
    "chat(model, persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQfIzySYAjrh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4Q1.2Q2.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
