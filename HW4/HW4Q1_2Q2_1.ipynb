{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW4Q1.2Q2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drhZPAoRbIiF",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4: Conversation Modeling and decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-sdmiwWWIud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy2p9OqRWNOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHSF2IQst2qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import ChatDictionary, ChatDataset, pad_tensor, argsort, batchify, _HypothesisTail, reorder_encoder_states, reorder_decoder_incremental_state, get_nbest_list_from_beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCcRr49JbIiH",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-J6pfDBbIiJ",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Attention visulization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTr1RmhhbIiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### set up the model and complete the corresponding task\n",
        "\n",
        "### the pretrained model was trained in ~2 hours, i.e. you can expect attention maps\n",
        "### to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6zydA1TbIiQ",
        "colab_type": "text"
      },
      "source": [
        "### You present here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6R5ESlvbIiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is some example attention map here, \n",
        "# *make sure you add text tokens on the axes to make it readable!*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vk1LNBZbIiV",
        "colab_type": "text"
      },
      "source": [
        "![Imgur](https://i.imgur.com/xodciCU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPdfiasbIiW",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Encoder Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6agDLBMcOMNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHewoRTnUWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d21a809-8926-4de6-b0a8-0d9cd60d5baa"
      },
      "source": [
        "import os\n",
        "\n",
        "### persona chat dataset\n",
        "if not os.path.exists('./dict'):\n",
        "    !wget \"https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\" -O './dict'\n",
        "if not os.path.exists('./train.jsonl'):\n",
        "    !wget \"https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\" -O './train.jsonl'\n",
        "if not os.path.exists('./valid.jsonl'):\n",
        "    !wget \"https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\" -O './valid.jsonl'\n",
        "\n",
        "if not os.path.exists('./chat_model_best_22.pt'):\n",
        "    !wget \"https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\" -O './chat_model_best_22.pt'\n",
        "\n",
        "chat_dict = ChatDictionary('./dict')\n",
        "train_dataset = ChatDataset('./train.jsonl', chat_dict)\n",
        "valid_dataset = ChatDataset('./valid.jsonl', chat_dict, 'valid')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-12 14:29:52--  https://nyu.box.com/shared/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/sj9f87tofpicll89xbc154pmbztu5q4h [following]\n",
            "--2019-11-12 14:29:52--  https://nyu.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h [following]\n",
            "--2019-11-12 14:29:52--  https://nyu.app.box.com/public/static/sj9f87tofpicll89xbc154pmbztu5q4h\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!2BLHLX93vRJ6lcfSqM9zz0Cc1B_Skf20NKvhYAx03yatyoO51KLhyTF6wrDe4_KFxv6hwRGjNN1Kcfs9g3ZmqNJDblAwl8La9-2KurpP5awyC23_K9P0InM6_WnslqIXJRRUPC7fBCAqWx3CV5ngBJ-U3rgD_RyZVVCsKGI3CF_jkJSdFiPcChyQyKqqY7lS3YLN2lV5T7un4-rXVnKGoJp3-J-sh3kzYG5whtiP8xVSyzboAy9hV3D5oRki15PDkdf6uQAS4khytnEM5f6kQSJrDzzizIDXpuOiW3nKmtsyqbGvf3j9tcmhvMQeiwm36DOgvcrSzRKVusal7ThE6oFqZPUNplRbIKs-3S7TyPGPBzLimCP8MKFiJlwOudjXA__F5N-OGicv2OIKz6cKEz5_7WRSUtL5EyfKhohiI1gkLm-kQIJwDpgESLC6QVEBUspjII6bsbf6DPbNMlOWur6QMprycFbtSa-wuGnvqdG72Sh-kHnaR78ja-5ktwxOUvMNzfJ5CHv2eV2ALZn1W62mw5lCt0IuqAabpVULHNQcLGjMOIBmk_Tk57egWtqc6QfZBeJRLTb96zh1DTI2Aw0kUJbyD8K57_WhBLX7p-x6u1oQkwG0-cKWIw6_ALRnZdFY74XC5MhasJFDoKOlLsX-20bktMuV1TDIYXVRNd3QxG3Es23Xdqzmq47XL-bqG6sQnNesPWo250jAUQ6TkA6YCRvGT2HC592RZlcSiE6NWkQo60f3NkoJ8KZ_7gVJnXjeU6q_RE-NPTtNjfxP898UR2WhPXqMGuMDaarDWGW5jceC4NFwlseHXJogfsMWFT_lD8agldyUy5pzWwTT47P3A7ji5K4nWzJsbZWJG-fHMAhuneX0fH_ijOZZPKXx5jvshR5o4jummCRmlClIcxzVF_YBOakdSZ7jj60T6z09cZ3m9tuXeO3KBQUbzFlOg4Ki8fqjo9HzONYA_ek0LFRXG8OSEwWbjYqiIPT31EmiQVzdcewoB2weErVxfTJIo7rDCprkkH2qPuNIloNUcresRBclKwjzV9RZYG_kyvr1ZHBsptyfleOv0bDUerV3-uby3MX7AAwBPmg7RHs9PU-AhcdIxBj7ONHMCIhgkMiWqLUCqFE18GASkvwemdgH9HPOVUtL7BWZI1VHBb4UdXcnIkasMzWIe2vsIzJx2zPzjwuTHvE_OzjYLD7nv9G4X2DO9gTzrE3GemeTD5TaQhBJ6b9FPIfhFkvLnLdmWgVoBBbzOKbuv3CdPzgNyFkCIARBnyGw0yT2Bso4Xt9p7GX-g80NJGcIvIkrRWtKkVvh1jn3MeT4ZI6FkBPTYDW39rBe_4zfuTYtsjee99nC3YPvGrq7hpyJXd1DFHkVc20bAA../download [following]\n",
            "--2019-11-12 14:29:53--  https://public.boxcloud.com/d/1/b1!2BLHLX93vRJ6lcfSqM9zz0Cc1B_Skf20NKvhYAx03yatyoO51KLhyTF6wrDe4_KFxv6hwRGjNN1Kcfs9g3ZmqNJDblAwl8La9-2KurpP5awyC23_K9P0InM6_WnslqIXJRRUPC7fBCAqWx3CV5ngBJ-U3rgD_RyZVVCsKGI3CF_jkJSdFiPcChyQyKqqY7lS3YLN2lV5T7un4-rXVnKGoJp3-J-sh3kzYG5whtiP8xVSyzboAy9hV3D5oRki15PDkdf6uQAS4khytnEM5f6kQSJrDzzizIDXpuOiW3nKmtsyqbGvf3j9tcmhvMQeiwm36DOgvcrSzRKVusal7ThE6oFqZPUNplRbIKs-3S7TyPGPBzLimCP8MKFiJlwOudjXA__F5N-OGicv2OIKz6cKEz5_7WRSUtL5EyfKhohiI1gkLm-kQIJwDpgESLC6QVEBUspjII6bsbf6DPbNMlOWur6QMprycFbtSa-wuGnvqdG72Sh-kHnaR78ja-5ktwxOUvMNzfJ5CHv2eV2ALZn1W62mw5lCt0IuqAabpVULHNQcLGjMOIBmk_Tk57egWtqc6QfZBeJRLTb96zh1DTI2Aw0kUJbyD8K57_WhBLX7p-x6u1oQkwG0-cKWIw6_ALRnZdFY74XC5MhasJFDoKOlLsX-20bktMuV1TDIYXVRNd3QxG3Es23Xdqzmq47XL-bqG6sQnNesPWo250jAUQ6TkA6YCRvGT2HC592RZlcSiE6NWkQo60f3NkoJ8KZ_7gVJnXjeU6q_RE-NPTtNjfxP898UR2WhPXqMGuMDaarDWGW5jceC4NFwlseHXJogfsMWFT_lD8agldyUy5pzWwTT47P3A7ji5K4nWzJsbZWJG-fHMAhuneX0fH_ijOZZPKXx5jvshR5o4jummCRmlClIcxzVF_YBOakdSZ7jj60T6z09cZ3m9tuXeO3KBQUbzFlOg4Ki8fqjo9HzONYA_ek0LFRXG8OSEwWbjYqiIPT31EmiQVzdcewoB2weErVxfTJIo7rDCprkkH2qPuNIloNUcresRBclKwjzV9RZYG_kyvr1ZHBsptyfleOv0bDUerV3-uby3MX7AAwBPmg7RHs9PU-AhcdIxBj7ONHMCIhgkMiWqLUCqFE18GASkvwemdgH9HPOVUtL7BWZI1VHBb4UdXcnIkasMzWIe2vsIzJx2zPzjwuTHvE_OzjYLD7nv9G4X2DO9gTzrE3GemeTD5TaQhBJ6b9FPIfhFkvLnLdmWgVoBBbzOKbuv3CdPzgNyFkCIARBnyGw0yT2Bso4Xt9p7GX-g80NJGcIvIkrRWtKkVvh1jn3MeT4ZI6FkBPTYDW39rBe_4zfuTYtsjee99nC3YPvGrq7hpyJXd1DFHkVc20bAA../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190210 (186K) [application/octet-stream]\n",
            "Saving to: ‘./dict’\n",
            "\n",
            "./dict              100%[===================>] 185.75K  1.16MB/s    in 0.2s    \n",
            "\n",
            "2019-11-12 14:29:54 (1.16 MB/s) - ‘./dict’ saved [190210/190210]\n",
            "\n",
            "--2019-11-12 14:29:55--  https://nyu.box.com/shared/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl [following]\n",
            "--2019-11-12 14:29:55--  https://nyu.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl [following]\n",
            "--2019-11-12 14:29:55--  https://nyu.app.box.com/public/static/aqp0jyjaixjmukm5asasivq2bcfze075.jsonl\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!qJ5H7G0Z6so4V6XkrPU_DpXS2z47Ra8AaHd3G0YIdqzX0MnbbBq_NPxzJXQxOGlMfYV_j0lzVenpeXc7TC9gmwE2ZfU9J5F7DCjgup5PQwvrPOl-6iVYl509LBXKBwGgACPzzmLTZRFH1Vi5BKHBbmv42C6bjdfxolssSHzXwlg0enIZWihV1dxAzSJ1f0vUw1ptqTJYjKUwz4hvoyfPEAHoSX922l4hvnFqsUWYVK0ilsGfm5pkY0on33tMZMGtAst8O1PosiJ6QSZtkF2EXycg7GweepUPj2GuyfmvSPjhEN1mxK9emYmADGNiiEG3CyNycAdR4Ri4kPVYUenEpRqu_1y1bEvzgDGmWzGIZpdn8xJnS2vFtre3B9qley98J7M_y5w1pnemAErlayelGCs-05b3YE0ftE1QHfo-smuODWp33oQFn7ybbHxk9YjTmadyLBvpdmitEYMxNnEDu_EAfclLwpoMWw6E_RvjzdOzfxGnX3wZ0dgz-XmfBVSOX_5agiXyWexr7sXHH0yXaDOY9Ts7xtf0CTkZ_Xe7HtTPemMsMm_ECoK-cDK-MW64ovaUEZ6pFNfuaVoqHvY-woeAjs20f0URNvGUzFKjucGU3Z_rFaS95swBNnxrlM60dgctm0Slerk-Y-_6GvPRWdS4pVfogWO0C3JrfmlRAbPw4Q8F5uH2oZTcFVPPfDVI00usVVP683Iv2f8CAy3ftXwVtWaZefI3HZnd2c5C59oHni4ubHveXzhF_JrGZflAxThk5mjdRiyNTu0Vm9F1OG923EhJWZ4ZD1zjNnmDk18RAwcY-5oAm7yuTjIhn1zPgmNOFCEBRbktZUzJ_M-FITQwT0uCg0DxY7gIc0H-pPpB3yZ2DaCDxV90K3B_QqjXtn4ZCtj-khyPb2jEqJd9Q5laEszxbODH2zE4xEj_dWWDWDgGnwL5XBVrtLoTJNT9C6tKFsEm_d0YFCzzfUAZkZDdQ61BGA7Eilxid0kQHhB8MqbZxtLMmDaqJBV8uiAK7c0GzQ54mixt6uqnihkbunIaKufqL2Asty7Hcjo4hXiON5kUCIjZLpCdSc9XkmpOUlNo6NEBCeMXvvkn1YJcp7sK8sYn58W75KHmQbU00U6Yt2Jp_t0Xd5kLCc5IAbU7hjS07tD-bvP0OhA-bdKUghk4d5W8fOIOUfKKO9dY9oTyMDIquDxd5sNPbJORctlAE8_TlsbXHZPcEYfwp2ZzNyqhCeHy9z6slghGNtjFAnz0w02zTXhq4DJYfVNEGBwuqEqzd6Z5xw5xshoSIz7o5peImuICuSJ2wBvPxNl9zHduupbUNkrzCHUssuD6VCw-1nQWqjNV3NwvuSv4DrUnaWrbSub5ndRC37CPaIxZfRY9C7zlPNViZZMr7w../download [following]\n",
            "--2019-11-12 14:29:56--  https://public.boxcloud.com/d/1/b1!qJ5H7G0Z6so4V6XkrPU_DpXS2z47Ra8AaHd3G0YIdqzX0MnbbBq_NPxzJXQxOGlMfYV_j0lzVenpeXc7TC9gmwE2ZfU9J5F7DCjgup5PQwvrPOl-6iVYl509LBXKBwGgACPzzmLTZRFH1Vi5BKHBbmv42C6bjdfxolssSHzXwlg0enIZWihV1dxAzSJ1f0vUw1ptqTJYjKUwz4hvoyfPEAHoSX922l4hvnFqsUWYVK0ilsGfm5pkY0on33tMZMGtAst8O1PosiJ6QSZtkF2EXycg7GweepUPj2GuyfmvSPjhEN1mxK9emYmADGNiiEG3CyNycAdR4Ri4kPVYUenEpRqu_1y1bEvzgDGmWzGIZpdn8xJnS2vFtre3B9qley98J7M_y5w1pnemAErlayelGCs-05b3YE0ftE1QHfo-smuODWp33oQFn7ybbHxk9YjTmadyLBvpdmitEYMxNnEDu_EAfclLwpoMWw6E_RvjzdOzfxGnX3wZ0dgz-XmfBVSOX_5agiXyWexr7sXHH0yXaDOY9Ts7xtf0CTkZ_Xe7HtTPemMsMm_ECoK-cDK-MW64ovaUEZ6pFNfuaVoqHvY-woeAjs20f0URNvGUzFKjucGU3Z_rFaS95swBNnxrlM60dgctm0Slerk-Y-_6GvPRWdS4pVfogWO0C3JrfmlRAbPw4Q8F5uH2oZTcFVPPfDVI00usVVP683Iv2f8CAy3ftXwVtWaZefI3HZnd2c5C59oHni4ubHveXzhF_JrGZflAxThk5mjdRiyNTu0Vm9F1OG923EhJWZ4ZD1zjNnmDk18RAwcY-5oAm7yuTjIhn1zPgmNOFCEBRbktZUzJ_M-FITQwT0uCg0DxY7gIc0H-pPpB3yZ2DaCDxV90K3B_QqjXtn4ZCtj-khyPb2jEqJd9Q5laEszxbODH2zE4xEj_dWWDWDgGnwL5XBVrtLoTJNT9C6tKFsEm_d0YFCzzfUAZkZDdQ61BGA7Eilxid0kQHhB8MqbZxtLMmDaqJBV8uiAK7c0GzQ54mixt6uqnihkbunIaKufqL2Asty7Hcjo4hXiON5kUCIjZLpCdSc9XkmpOUlNo6NEBCeMXvvkn1YJcp7sK8sYn58W75KHmQbU00U6Yt2Jp_t0Xd5kLCc5IAbU7hjS07tD-bvP0OhA-bdKUghk4d5W8fOIOUfKKO9dY9oTyMDIquDxd5sNPbJORctlAE8_TlsbXHZPcEYfwp2ZzNyqhCeHy9z6slghGNtjFAnz0w02zTXhq4DJYfVNEGBwuqEqzd6Z5xw5xshoSIz7o5peImuICuSJ2wBvPxNl9zHduupbUNkrzCHUssuD6VCw-1nQWqjNV3NwvuSv4DrUnaWrbSub5ndRC37CPaIxZfRY9C7zlPNViZZMr7w../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93166014 (89M) [application/octet-stream]\n",
            "Saving to: ‘./train.jsonl’\n",
            "\n",
            "./train.jsonl       100%[===================>]  88.85M  13.8MB/s    in 7.1s    \n",
            "\n",
            "2019-11-12 14:30:04 (12.5 MB/s) - ‘./train.jsonl’ saved [93166014/93166014]\n",
            "\n",
            "--2019-11-12 14:30:05--  https://nyu.box.com/shared/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl [following]\n",
            "--2019-11-12 14:30:05--  https://nyu.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl [following]\n",
            "--2019-11-12 14:30:05--  https://nyu.app.box.com/public/static/eg4ivddtqib2hkf1k8rkxnmzmo0cq27p.jsonl\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!2Zdr82lDCtOfmBerUxvisxZ5yybqhw9ugjF-0u9RJOBqATcMlMcLXiaT7zvw3kPHKlK10-LeQjgyPdai7V16Gd7hAdNViV3n5tvD23xS-QjimxZo2U47oan8l54nwWsC9lgnz1Q-yFY8KeFQaDnqVI3xkaTHXZ2a8DgRviSZTP9PiL6W8nKxTRj8qYTg4aMHXbMftbFnhwCesYufCYinIFlrQFB_wNM0c70kN2DH5gn2CD5c897SvuBfPzjBkCAWFRrjmPIXnwlrWum1VIYkj_JS-KWmBsz1p7xuBB1Yh7UjOx8CQGQvOJSuYq5u5n89aogVsrgZxAlcLAC5CcMozNYURVtQ7GllFJt0TbCZoXNyorr6UaST2-r2AOWm1mCHBLw3HRj_wTpEmPdpYTJGdCjz_2u1EWWggmZt4UygnlNyj5GOhNIs7s1Y2ijkuQmqCuDzrKjC_bLlUwVRr385WGacSbR7ux0ZkcigZOX9PL69XFCGoq-u0lYyqJFrfIaJr5d-i_5ocQ1BMx2J9bZe5s8ElN6gWx4VIXsXKBiWBIb7tM1A5QqbUIQ3irN9Cz0zQMSihQT1NOdppn4I3Ffr_oxK-vgghdOwuPQ4BAUcuZegXawyevlAWBjfKRAKCAzr0IULfgHUjvjGpKOiwQ_ITlnwHwvqBNX8dWXZT2oDdUXOBVaaYuBOAbnjV87ZulbLw4wAJJ04kbAhHKZKtxQpVdzbwGpA70NmRyX8faI65atJzdodPdepsXQDAy6WY8s5BN5xBqqpOhvR4SCdYp0JeJgk1xWDgpKFcTKKgkPrPOpCkJoOoxPWJG9I7LsKKHU1iqsxK883YFOfvk18luK6Dl8pgP87RApb6fECnvaE8T17DyWPLsqBm13MMnHQwetEDzAZTA3BqiL5cfqLdd6kebcID4ujyy4IE9h6kPSkGnlqVOIOeJRC2qXhowqdoUtHcBEm6o7Y-pu3IWWLaJHPHwBSuChJkzrgJE8lHdhl-2a3Tx9UYFD7doIlmAUkvwi0XA4NsoOubMuDuAoyRQ2loQI8Hjyg_GiXBkGm_RgExvlAM8IJ7LAp60u7RHLrwYLyNRXXn3CRjBwvQZ_RxFKDJcIC3imMClVdOe1wVv5gRixbFNDDkCtSkyY-mKniWK0QlfhdvjtnYTCZOt-oGRAXTKomuixMZ_vhpO_r5IKgjyYKbbEWp8es_B47-O84qqfmxmdoqjyRd4bPdIjDmhIgscLfwWnv4dk1f8O0oOLS6GSXpQqysDWK3JxBDdfc0Ze9EMsOLgIrjnb9pIlezhXGSfJuhxHhynBbgGOPFRWteRvIFv9O3-agD_NodbqLJxFnusg1zGZtghcQLOF_yviNmiAC_pV1t0rd7RUiL9DzfqqpBR4YlmPzpFbM/download [following]\n",
            "--2019-11-12 14:30:06--  https://public.boxcloud.com/d/1/b1!2Zdr82lDCtOfmBerUxvisxZ5yybqhw9ugjF-0u9RJOBqATcMlMcLXiaT7zvw3kPHKlK10-LeQjgyPdai7V16Gd7hAdNViV3n5tvD23xS-QjimxZo2U47oan8l54nwWsC9lgnz1Q-yFY8KeFQaDnqVI3xkaTHXZ2a8DgRviSZTP9PiL6W8nKxTRj8qYTg4aMHXbMftbFnhwCesYufCYinIFlrQFB_wNM0c70kN2DH5gn2CD5c897SvuBfPzjBkCAWFRrjmPIXnwlrWum1VIYkj_JS-KWmBsz1p7xuBB1Yh7UjOx8CQGQvOJSuYq5u5n89aogVsrgZxAlcLAC5CcMozNYURVtQ7GllFJt0TbCZoXNyorr6UaST2-r2AOWm1mCHBLw3HRj_wTpEmPdpYTJGdCjz_2u1EWWggmZt4UygnlNyj5GOhNIs7s1Y2ijkuQmqCuDzrKjC_bLlUwVRr385WGacSbR7ux0ZkcigZOX9PL69XFCGoq-u0lYyqJFrfIaJr5d-i_5ocQ1BMx2J9bZe5s8ElN6gWx4VIXsXKBiWBIb7tM1A5QqbUIQ3irN9Cz0zQMSihQT1NOdppn4I3Ffr_oxK-vgghdOwuPQ4BAUcuZegXawyevlAWBjfKRAKCAzr0IULfgHUjvjGpKOiwQ_ITlnwHwvqBNX8dWXZT2oDdUXOBVaaYuBOAbnjV87ZulbLw4wAJJ04kbAhHKZKtxQpVdzbwGpA70NmRyX8faI65atJzdodPdepsXQDAy6WY8s5BN5xBqqpOhvR4SCdYp0JeJgk1xWDgpKFcTKKgkPrPOpCkJoOoxPWJG9I7LsKKHU1iqsxK883YFOfvk18luK6Dl8pgP87RApb6fECnvaE8T17DyWPLsqBm13MMnHQwetEDzAZTA3BqiL5cfqLdd6kebcID4ujyy4IE9h6kPSkGnlqVOIOeJRC2qXhowqdoUtHcBEm6o7Y-pu3IWWLaJHPHwBSuChJkzrgJE8lHdhl-2a3Tx9UYFD7doIlmAUkvwi0XA4NsoOubMuDuAoyRQ2loQI8Hjyg_GiXBkGm_RgExvlAM8IJ7LAp60u7RHLrwYLyNRXXn3CRjBwvQZ_RxFKDJcIC3imMClVdOe1wVv5gRixbFNDDkCtSkyY-mKniWK0QlfhdvjtnYTCZOt-oGRAXTKomuixMZ_vhpO_r5IKgjyYKbbEWp8es_B47-O84qqfmxmdoqjyRd4bPdIjDmhIgscLfwWnv4dk1f8O0oOLS6GSXpQqysDWK3JxBDdfc0Ze9EMsOLgIrjnb9pIlezhXGSfJuhxHhynBbgGOPFRWteRvIFv9O3-agD_NodbqLJxFnusg1zGZtghcQLOF_yviNmiAC_pV1t0rd7RUiL9DzfqqpBR4YlmPzpFbM/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6044784 (5.8M) [application/octet-stream]\n",
            "Saving to: ‘./valid.jsonl’\n",
            "\n",
            "./valid.jsonl       100%[===================>]   5.76M  7.60MB/s    in 0.8s    \n",
            "\n",
            "2019-11-12 14:30:07 (7.60 MB/s) - ‘./valid.jsonl’ saved [6044784/6044784]\n",
            "\n",
            "--2019-11-12 14:30:08--  https://nyu.box.com/shared/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt [following]\n",
            "--2019-11-12 14:30:09--  https://nyu.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt [following]\n",
            "--2019-11-12 14:30:09--  https://nyu.app.box.com/public/static/24zsynuks8nzg7530tgakzh8o62id9xa.pt\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!EspraUXCgnCwgwoofyoBmkoTskKYStKUeFiDTVp4O0w5AV3KgzctNxqpgL741AlGt7XVTzfOmKVnmJRgaRgc9ljDp3lYGWCP3e8s539IAFHGuSArz3Qw3RrWeTZdRNKTkmqT9XF6_z-Zy19pc7GVoxh-BoMvrdsorl2SBQePIGPvOCDZqo4PidGUt-04OR5HlxxhnrMApstI41AlTyX5RB-EZbZVYtsIRi4DsYssRFsl8gFPn7IxpxUNrCPl0oOuWgU-D8sHX9jvZLppvgGTWTH1w9DXsEDhiX5gGv-4Blv31pOiIs4B9ELPG24ydwN5X2GSJJj7iwNiexjsREz9L9SD9pKvXbrbR3rYUL1CNRwXAiX2eFaF_A2vNtFFSRVrigkAbi6q5RbGbksxrkm5d55eEMeIDhsm5RASuTk3IrtCDpyY7DhmkLLYpTIr4XQPHJcbwfDVTUpRcd1fYCLSFjGRlHn3hl_tIzGXUKNjJk3p9bW3jz9NTVvJkA7Uotgdm_wk6V3bD7AUxGTPn6uuSSHbApkQV_FBC2FgVKQd01DSF8cYM3_2A5lnTthkToxfE-SMVgmk8p4WcnJ5pl6sgPcy340C_Cz1nTOvcwDqGsLuYyez-a8pHn0aYtKQQlxybRkUnhtdn2LWJe-QUQY361qSeRhdlafOBb12YhkTkNz8BVD3i65BvpD7bMOccaxUoCMea3FpDO9YQs0L_rY0DWqF-mGoQKU2vKGsyWBRx2udakjcJ2Hrgzd_ZsAQMCk5MGjeR8BuLgctITqiiQQIquR1F1wu1FZO1l8K6xy7LRpZQNAOjNTPCzrBNSaRKAJkPV5jia9Fo3uTPJMrbtxxgBI9yk64-wUfgDsoBt4as-9P01m2Xturksj2a473Bn78UKkEUM0s_4xcp1245MeHxfMBrryGZk9rPDOdODxK04FB_jOvyUlJZntSF8GRF-QKdnRgn8ZDqouz5TAKSFc6V2GVi-n4ywCFBJ64gHOG5OqoFtazLvt-RRcMjqVajsG7iLsoQlRno9tzSUBjbRuzt4S1O-7afzvV_fg9b87Q3_JI5vAsoTJ--CQPzjUJ__4axqsRG_XJUzVGalBV8c89iC-0cTit_9amvIEcYUx_DL2cWMpKGtmoTfuZ7vFiesWlKRGgJj5HZsyKdnyYnUpstfNjPZO3O5u7pqN7AoBzJpKg-kEHm32R66IXyl_5oNi-a9OBwd8mVpl40O5Jhq41fIH11TE3mdCCLyXtm2PBfpDBBTTFHZLQeVk7KVfQERtCV1r4h7Qc-OszYgOnl5LwUKeqAtp9oRdjWQ8gj1Bz_pf3kJ3TDOmqNB5-wCvsrXYdQUtpC2tu0Qs6ijnVEHdYLXzu7uW_KEYaVqLKU-i3Br1bLn4sIsMPENRsc1dgA594gQnxckl4/download [following]\n",
            "--2019-11-12 14:30:10--  https://public.boxcloud.com/d/1/b1!EspraUXCgnCwgwoofyoBmkoTskKYStKUeFiDTVp4O0w5AV3KgzctNxqpgL741AlGt7XVTzfOmKVnmJRgaRgc9ljDp3lYGWCP3e8s539IAFHGuSArz3Qw3RrWeTZdRNKTkmqT9XF6_z-Zy19pc7GVoxh-BoMvrdsorl2SBQePIGPvOCDZqo4PidGUt-04OR5HlxxhnrMApstI41AlTyX5RB-EZbZVYtsIRi4DsYssRFsl8gFPn7IxpxUNrCPl0oOuWgU-D8sHX9jvZLppvgGTWTH1w9DXsEDhiX5gGv-4Blv31pOiIs4B9ELPG24ydwN5X2GSJJj7iwNiexjsREz9L9SD9pKvXbrbR3rYUL1CNRwXAiX2eFaF_A2vNtFFSRVrigkAbi6q5RbGbksxrkm5d55eEMeIDhsm5RASuTk3IrtCDpyY7DhmkLLYpTIr4XQPHJcbwfDVTUpRcd1fYCLSFjGRlHn3hl_tIzGXUKNjJk3p9bW3jz9NTVvJkA7Uotgdm_wk6V3bD7AUxGTPn6uuSSHbApkQV_FBC2FgVKQd01DSF8cYM3_2A5lnTthkToxfE-SMVgmk8p4WcnJ5pl6sgPcy340C_Cz1nTOvcwDqGsLuYyez-a8pHn0aYtKQQlxybRkUnhtdn2LWJe-QUQY361qSeRhdlafOBb12YhkTkNz8BVD3i65BvpD7bMOccaxUoCMea3FpDO9YQs0L_rY0DWqF-mGoQKU2vKGsyWBRx2udakjcJ2Hrgzd_ZsAQMCk5MGjeR8BuLgctITqiiQQIquR1F1wu1FZO1l8K6xy7LRpZQNAOjNTPCzrBNSaRKAJkPV5jia9Fo3uTPJMrbtxxgBI9yk64-wUfgDsoBt4as-9P01m2Xturksj2a473Bn78UKkEUM0s_4xcp1245MeHxfMBrryGZk9rPDOdODxK04FB_jOvyUlJZntSF8GRF-QKdnRgn8ZDqouz5TAKSFc6V2GVi-n4ywCFBJ64gHOG5OqoFtazLvt-RRcMjqVajsG7iLsoQlRno9tzSUBjbRuzt4S1O-7afzvV_fg9b87Q3_JI5vAsoTJ--CQPzjUJ__4axqsRG_XJUzVGalBV8c89iC-0cTit_9amvIEcYUx_DL2cWMpKGtmoTfuZ7vFiesWlKRGgJj5HZsyKdnyYnUpstfNjPZO3O5u7pqN7AoBzJpKg-kEHm32R66IXyl_5oNi-a9OBwd8mVpl40O5Jhq41fIH11TE3mdCCLyXtm2PBfpDBBTTFHZLQeVk7KVfQERtCV1r4h7Qc-OszYgOnl5LwUKeqAtp9oRdjWQ8gj1Bz_pf3kJ3TDOmqNB5-wCvsrXYdQUtpC2tu0Qs6ijnVEHdYLXzu7uW_KEYaVqLKU-i3Br1bLn4sIsMPENRsc1dgA594gQnxckl4/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69267718 (66M) [application/octet-stream]\n",
            "Saving to: ‘./chat_model_best_22.pt’\n",
            "\n",
            "./chat_model_best_2 100%[===================>]  66.06M  14.5MB/s    in 5.1s    \n",
            "\n",
            "2019-11-12 14:30:15 (13.0 MB/s) - ‘./chat_model_best_22.pt’ saved [69267718/69267718]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 131438/131438 [00:14<00:00, 9276.29it/s] \n",
            "100%|██████████| 7801/7801 [00:00<00:00, 9384.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjMy68c7nVHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=batchify, batch_size=64)\n",
        "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unu5CAhBbIiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### add transformer encoder as optional encoder in seq2seq model.\n",
        "\n",
        "# code below can help you to start it, but feel free to start from scratch\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "class EncoderRNN(nn.Module):\n",
        "    \"\"\"Encodes the input context.\"\"\"\n",
        "\n",
        "    # def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx=0, dropout=0, shared_lt=None):\n",
        "    #     super().__init__()\n",
        "    #     self.vocab_size = vocab_size\n",
        "    #     self.embed_size = embed_size\n",
        "    #     self.hidden_size = hidden_size\n",
        "    #     self.num_layers = num_layers\n",
        "    #     self.dropout = nn.Dropout(p=dropout)\n",
        "    #     self.pad_idx = pad_idx\n",
        "        \n",
        "    #     if shared_lt is None:\n",
        "    #         self.embedding = nn.Embedding(self.vocab_size, self.embed_size, pad_idx)\n",
        "    #     else:\n",
        "    #         self.embedding = shared_lt\n",
        "            \n",
        "    #     self.gru = nn.GRU(\n",
        "    #         self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "    #     )\n",
        "        \n",
        "        \n",
        "    # def forward(self, text_vec, text_lens, hidden=None, use_packed=True):\n",
        "    #     embedded = self.embedding(text_vec)\n",
        "    #     attention_mask = text_vec.ne(self.pad_idx)\n",
        "\n",
        "    #     embedded = self.dropout(embedded)\n",
        "    #     if use_packed is True:\n",
        "    #         embedded = pack_padded_sequence(embedded, text_lens, batch_first=True)\n",
        "    #     output, hidden = self.gru(embedded, hidden)\n",
        "    #     if use_packed is True:\n",
        "    #         output, output_lens = pad_packed_sequence(output, batch_first=True)\n",
        "        \n",
        "    #     return output, hidden, attention_mask\n",
        "\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, dropout, num_layers,shared_lt, max_len = 7500, nhead=4, pad_idx= 0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        self.nhead = nhead\n",
        "        self.num_layers = num_layers\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.position_embed = nn.Embedding(num_embeddings = self.max_len, embedding_dim = self.embed_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(self.embed_size, self.nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, self.num_layers)\n",
        "        self.token_embed = shared_lt\n",
        "        self.dropout = nn.Dropout(p = self.dropout)\n",
        "\n",
        "    def _generate_mask(self, sentence_len):\n",
        "        \"Mask out subsequent positions.\"\n",
        "        attn_shape = (sentence_len, sentence_len)\n",
        "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "        return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "    def forward(self, text_vec, text_lens, hidden=None, use_packed=True): \n",
        "        # text_vec: batch x length \n",
        "        pos = torch.arange(text_vec.size(1), device = text_vec.device).expand(text_vec.size(0),-1).view(-1, text_vec.size(1))\n",
        "        pos_embedded = self.position_embed(pos)\n",
        "        tok_embedded = self.token_embed(text_vec)\n",
        "        embedded = pos_embedded + tok_embedded  # apply pos embedding\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attention_mask_pos = self._generate_mask(text_vec.size(1))\n",
        "        attention_mask_pad = text_vec.eq(self.pad_idx)\n",
        "        \n",
        "        output = self.transformer(embedded.transpose(0,1), \n",
        "                                  src_key_padding_mask=attention_mask_pad.to(text_vec.device),\n",
        "                                  mask = attention_mask_pos.to(text_vec.device))\n",
        "        output.transpose_(0,1)\n",
        "        #print('output shape', output.size())\n",
        "        hidden_encoder = torch.mean(output, dim = 1, keepdim=True).transpose(0,1).expand(2,-1,-1).contiguous() # 2 because decoder num_layers \n",
        "        #print('hidden layer shape', hidden_encoder.size())\n",
        "        return output, hidden_encoder, attention_mask_pad\n",
        "    \n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    \"\"\"Generates a sequence of tokens in response to context.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size, 0)\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            self.embed_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
        "        )\n",
        "        \n",
        "        self.attention = AttentionLayer(self.hidden_size, self.embed_size)\n",
        "\n",
        "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self.longest_label = 100\n",
        "\n",
        "    def forward(self, text_vec, decoder_hidden, encoder_states):\n",
        "        emb = self.embedding(text_vec)\n",
        "        emb = self.dropout(emb)\n",
        "        seqlen = text_vec.size(1)\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        decoder_hidden = decoder_hidden\n",
        "        output = []\n",
        "        attn_w_log = []\n",
        "\n",
        "        for i in range(seqlen):\n",
        "            decoder_output, decoder_hidden = self.gru(emb[:,i,:].unsqueeze(1), decoder_hidden)\n",
        "            \n",
        "            # compute attention at each time step\n",
        "            decoder_output_attended, attn_weights = self.attention(decoder_output, decoder_hidden, encoder_output, attention_mask)\n",
        "            output.append(decoder_output_attended)\n",
        "            attn_w_log.append(attn_weights)\n",
        "            \n",
        "        output = torch.cat(output, dim=1).to(text_vec.device)\n",
        "        scores = self.out(output)\n",
        "        \n",
        "\n",
        "        return scores, decoder_hidden, attn_w_log\n",
        "    \n",
        "    def decode_forced(self, ys, encoder_states, xs_lens):\n",
        "        encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "        \n",
        "        batch_size = ys.size(0)\n",
        "        target_length = ys.size(1)\n",
        "        longest_label = max(target_length, self.longest_label)\n",
        "        \n",
        "        starts = torch.Tensor([1]).long().to(self.embedding.weight.device).expand(batch_size, 1).long()  # expand to batch size\n",
        "        \n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        y_in = ys.narrow(1, 0, ys.size(1) - 1)\n",
        "        decoder_input = torch.cat([starts, y_in], 1)\n",
        "        decoder_output, decoder_hidden, attn_w_log = self.forward(decoder_input, encoder_hidden, encoder_states)\n",
        "        _, preds = decoder_output.max(dim=2)\n",
        "        \n",
        "        return decoder_output, preds, attn_w_log\n",
        "    \n",
        "    \n",
        "class AttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, embedding_size):\n",
        "        super().__init__()\n",
        "        input_dim = hidden_size\n",
        "\n",
        "        self.linear_out = nn.Linear(hidden_size+input_dim, input_dim, bias=False)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, decoder_output, decoder_hidden, encoder_output, attention_mask):\n",
        "\n",
        "        batch_size, seq_length, hidden_size = encoder_output.size()\n",
        "\n",
        "        encoder_output_t = encoder_output.transpose(1,2)\n",
        "        \n",
        "        attention_scores = torch.bmm(decoder_output, encoder_output_t).squeeze(1)\n",
        "\n",
        "        attention_scores.masked_fill_((attention_mask), -10e5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        mix = torch.bmm(attention_weights.unsqueeze(1), encoder_output)\n",
        "\n",
        "        combined = torch.cat((decoder_output.squeeze(1), mix.squeeze(1)), dim=1)\n",
        "\n",
        "        output = self.linear_out(combined).unsqueeze(1)\n",
        "        output = self.tanh(output)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "class seq2seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic seq2seq model with attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, opts):\n",
        "\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        \n",
        "        self.decoder = DecoderRNN(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    embed_size=self.opts['embedding_size'],\n",
        "                                    hidden_size=self.opts['hidden_size'],\n",
        "                                    num_layers=self.opts['num_layers_dec'],\n",
        "                                    dropout=self.opts['dropout'],\n",
        "                                )\n",
        "        \n",
        "        self.encoder = EncoderRNN(\n",
        "                                    vocab_size=self.opts['vocab_size'],\n",
        "                                    embed_size=self.opts['embedding_size'],\n",
        "                                    hidden_size=self.opts['hidden_size'],\n",
        "                                    num_layers=self.opts['num_layers_enc'],\n",
        "                                    dropout=self.opts['dropout'],\n",
        "                                    shared_lt=self.decoder.embedding,\n",
        "                                    # ### Transformer\n",
        "                                    # max_len = self.opts['max_len'],\n",
        "                                    # nhead = self.opts['nhead'],\n",
        "    \n",
        "        )\n",
        "        \n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        \n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5thvHRqpbIih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "load_pretrained = False\n",
        "    \n",
        "if load_pretrained is True:\n",
        "    if current_device == 'cuda':\n",
        "        model_pt = torch.load('./transformer_best_6.pt')\n",
        "    else:\n",
        "        model_pt = torch.load('./transformer_best_6.pt', map_location=torch.device('cpu'))\n",
        "    opts = model_pt['opts']\n",
        "    \n",
        "    model = seq2seq(opts)\n",
        "    model.load_state_dict(model_pt['state_dict'])\n",
        "    model.to(current_device)\n",
        "    \n",
        "else:\n",
        "    \n",
        "    opts = {}\n",
        "\n",
        "    opts['vocab_size'] = len(chat_dict)\n",
        "    opts['hidden_size'] = 256\n",
        "    opts['embedding_size'] = 256\n",
        "    opts['num_layers_enc'] = 2\n",
        "    opts['num_layers_dec'] = 2\n",
        "    opts['dropout'] = 0.3\n",
        "    opts['encoder_shared_lt'] = True\n",
        "    \n",
        "    ### Transformer \n",
        "    # opts['max_len'] = 7500\n",
        "    # opts['nhead'] = 4\n",
        "    \n",
        "    \n",
        "    model = seq2seq(opts)\n",
        "    model.to(current_device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqH2fQYOnpUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.001, amsgrad=True, weight_decay = 0.01) # AdamW is Adam + weight decay\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAVeA9MdbIic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check pdf to see what you expected to present"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "be866175-013c-4a93-e123-10f5ce9934df",
        "id": "-Hl2YnSK8cT7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "plot_cache = []\n",
        "\n",
        "best_val_loss = 100\n",
        "\n",
        "for epoch in range(15):\n",
        "    \n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    sum_tokens = 0\n",
        "    \n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text_vecs = batch['text_vecs'].to('cuda')\n",
        "        target_vecs = batch['target_vecs'].to('cuda')\n",
        " \n",
        "        encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
        "        \n",
        "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
        "        \n",
        "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
        "        \n",
        "        loss = criterion(scores, target_vecs.view(-1))\n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
        "        loss /= num_tokens\n",
        "        \n",
        "        sum_tokens += num_tokens\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            avg_train_loss = sum_loss/sum_tokens\n",
        "            print(\"iter {} train loss = {}\".format(i, sum_loss/sum_tokens))\n",
        "\n",
        "    val_loss = 0\n",
        "    val_tokens = 0\n",
        "    for i, batch in enumerate(valid_loader):\n",
        "        model.eval()\n",
        "        \n",
        "        text_vecs = batch['text_vecs'].to('cuda')\n",
        "        target_vecs = batch['target_vecs'].to('cuda')\n",
        "        \n",
        "        encoded = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
        "        #encoded = model.encoder(text_vecs)\n",
        "        \n",
        "        decoder_output, preds, attn_w_log = model.decoder.decode_forced(target_vecs, encoded, batch['text_lens'])\n",
        "        \n",
        "        scores = decoder_output.view(-1, decoder_output.size(-1))\n",
        "        \n",
        "        loss = criterion(scores, target_vecs.view(-1))\n",
        "        \n",
        "        num_tokens = target_vecs.ne(0).long().sum().item()\n",
        "        \n",
        "        val_tokens += num_tokens\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    avg_val_loss = val_loss/val_tokens\n",
        "    val_perp = np.exp(avg_val_loss)\n",
        "    scheduler.step(avg_val_loss)\n",
        "        \n",
        "    print(\"Epoch {} valid loss = {}\".format(epoch, avg_val_loss))\n",
        "    print(\"Epoch {} valid perp = {}\".format(epoch, val_perp))\n",
        "    \n",
        "    plot_cache.append( (avg_train_loss, avg_val_loss) )\n",
        "    \n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        \n",
        "        torch.save({\n",
        "        'state_dict': model.state_dict(),\n",
        "        'opts': opts,\n",
        "        'plot_cache': plot_cache,\n",
        "            }, f'./transformer_best_{epoch}.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0 train loss = 9.8188107823919\n",
            "iter 100 train loss = 5.862565773542912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNtN_PT4nu_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = './transformer_best_0.pt'\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if current_device == 'cuda':\n",
        "    model_pt = torch.load(model_name)\n",
        "else:\n",
        "    model_pt = torch.load(model_name, map_location=torch.device('cpu'))\n",
        "opts = model_pt['opts']\n",
        "\n",
        "model = seq2seq(opts)\n",
        "model.load_state_dict(model_pt['state_dict'])\n",
        "model.to(current_device)\n",
        "\n",
        "plot_cache = model_pt['plot_cache']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAxmvNFgbIia",
        "colab_type": "text"
      },
      "source": [
        "## You present here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpOU_fRsn3oB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "795484c6-7eee-4706-8522-6d10d6dc319f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "plot_cache = model_pt['plot_cache']\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
        "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Loss curves')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqyYIP7L/3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8gEwljBn533",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "fb4d2257-98cb-4937-b386-d626ec0b73ec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves')\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMsAlqVAGuCQVygCXpEIZ4JJUqP8PgUY2MKF5A6wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vWrwPgbIik",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HD6-tEpbIis",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 N-Gram blocking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XokS7k02O5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = './chat_model_best_22.pt'\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if current_device == 'cuda':\n",
        "    model_pt = torch.load(model_name)\n",
        "else:\n",
        "    model_pt = torch.load(model_name, map_location=torch.device('cpu'))\n",
        "opts = model_pt['opts']\n",
        "\n",
        "model = seq2seq(opts)\n",
        "model.load_state_dict(model_pt['state_dict'])\n",
        "model.to(current_device)\n",
        "\n",
        "plot_cache = model_pt['plot_cache']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVszNFTbbIiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from operator import attrgetter\n",
        "\n",
        "\n",
        "class Beam(object):\n",
        "    \"\"\"\n",
        "    This class serves to keep info about partial hypothesis and perform the beam step\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        beam_size,\n",
        "        padding_token=0,\n",
        "        bos_token=1,\n",
        "        eos_token=2,\n",
        "        min_length=3,\n",
        "        min_n_best=3,\n",
        "        device='cpu',\n",
        "        # for iterbeam below\n",
        "        similarity_metric='hamming',\n",
        "        similarity_threshold=0,\n",
        "        N = 3\n",
        "    ):\n",
        "        \n",
        "        self.beam_size = beam_size\n",
        "        self.min_length = min_length\n",
        "        self.eos = eos_token\n",
        "        self.bos = bos_token\n",
        "        self.pad = padding_token\n",
        "        self.device = device\n",
        "        # recent score for each hypo in the beam\n",
        "        self.scores = None\n",
        "        # self.scores values per each time step\n",
        "        self.all_scores = [torch.Tensor([0.0] * beam_size).to(self.device)]\n",
        "        # backtracking id to hypothesis at previous time step\n",
        "        self.bookkeep = []\n",
        "        # output tokens at each time step\n",
        "        self.outputs = [\n",
        "            torch.Tensor(self.beam_size).long().fill_(self.bos).to(self.device)\n",
        "        ]\n",
        "        # keeps tuples (score, time_step, hyp_id)\n",
        "        self.finished = []\n",
        "        self.eos_top = False\n",
        "        self.eos_top_ts = None\n",
        "        self.n_best_counter = 0\n",
        "        self.min_n_best = min_n_best\n",
        "        self.partial_hyps = [[self.bos] for i in range(beam_size)]\n",
        "\n",
        "        # iterbeam related below\n",
        "        self.history_hyps = []\n",
        "        self.similarity_metric = similarity_metric\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.banned_tokens = set()\n",
        "        \n",
        "        ### N-gram blocking\n",
        "        self.N = N\n",
        "      \n",
        "    def get_output_from_current_step(self):\n",
        "        \"\"\"Get the output at the current step.\"\"\"\n",
        "        return self.outputs[-1]\n",
        "\n",
        "    def get_backtrack_from_current_step(self):\n",
        "        \"\"\"Get the backtrack at the current step.\"\"\"\n",
        "        return self.bookkeep[-1]\n",
        "    \n",
        "    ##################### ITER-BEAM BLOCKING PART START #####################\n",
        "    \n",
        "    def hamming_distance(self, t1, t2):\n",
        "        dist = 0\n",
        "        for tok1, tok2 in zip(t1,t2):\n",
        "            if tok1 != tok2:\n",
        "                dist += 1\n",
        "        return dist\n",
        "    \n",
        "    def edit_distance(self, t1, t2):\n",
        "        import editdistance\n",
        "        dist = editdistance.eval(t1, t2)\n",
        "        return dist\n",
        "                \n",
        "    def similarity_check(self, active_hyp, previous_hyps, metric='hamming', threshold=0):\n",
        "        #print('active_hyp', active_hyp)\n",
        "        banned_tokens = []\n",
        "        active_len = len(active_hyp)\n",
        "        for observed_hyp, _banned_tokens in previous_hyps.items():\n",
        "            if len(observed_hyp) != active_len:\n",
        "                continue\n",
        "            if metric == 'hamming':\n",
        "                dist = self.hamming_distance(observed_hyp, active_hyp)\n",
        "            if metric == 'edit':\n",
        "                dist = self.edit_distance(observed_hyp, active_hyp)\n",
        "            if dist <= threshold:\n",
        "                banned_tokens.extend(_banned_tokens)\n",
        "                    \n",
        "        return list(set(banned_tokens))\n",
        "\n",
        "    #######################################################\n",
        "    ########## Add ngram blocking function here ###########\n",
        "    #######################################################\n",
        "    \n",
        "    def get_ngram_blocking(self, text_vec, N = 3):\n",
        "        # print('Start n-gram blocking for ', text_vec)\n",
        "        banned = []\n",
        "        \n",
        "        if N == 1:  # special case: unigram\n",
        "          banned = [i for i in text_vec[:-1] if i == text_vec[-1]]\n",
        "        else:\n",
        "          history = tuple(text_vec[-N+1:])\n",
        "          for ngram in zip(*[text_vec[i:] for i in range(N)]):\n",
        "              if ngram[:-1] == history:\n",
        "                  banned.append(ngram[-1])\n",
        "    \n",
        "        return list(set(banned)) \n",
        "    \n",
        "    ##################### ITER-BEAM BLOCKING PART END ########################\n",
        "    \n",
        "    def select_paths(self, logprobs, prior_scores, previous_hyps):\n",
        "        \"\"\"Select the next vocabulary item in these beams.\"\"\"\n",
        "        # beam search actually looks over all hypotheses together so we flatten\n",
        "        beam_scores = logprobs + prior_scores.unsqueeze(1).expand_as(logprobs)\n",
        "        # print('shape of beam score', beam_scores.size())\n",
        "        # iterbeam blocking part\n",
        "        current_length = len(self.all_scores)\n",
        "        # print('all scores length', len(self.all_scores))\n",
        "        if len(previous_hyps) > 0 and current_length > 0:\n",
        "            for hyp_id in range(beam_scores.size(0)):\n",
        "                active_hyp = tuple(self.partial_hyps[hyp_id])\n",
        "                banned_tokens = self.similarity_check(active_hyp, previous_hyps, metric=self.similarity_metric, threshold=self.similarity_threshold)\n",
        "                if len(banned_tokens) > 0:\n",
        "                    beam_scores[:, banned_tokens] = -10e5\n",
        "      \n",
        "        ############################################################ \n",
        "        ################# Insert n-gram blocking here ##############\n",
        "        ############################################################\n",
        "\n",
        "        for hyp_id in range(beam_scores.size(0)):\n",
        "            active_hyp = self.partial_hyps[hyp_id]\n",
        "            # print('hyp_id: ', hyp_id)\n",
        "            # print('active hyp ids: ', active_hyp)\n",
        "            # print('active hyp tokens: ',chat_dict.v2t(active_hyp))\n",
        "\n",
        "            if len(active_hyp) >= self.N + 1:\n",
        "              banned_tokens = self.get_ngram_blocking(active_hyp, self.N)\n",
        "              if len(banned_tokens) > 0:\n",
        "              #  print(f'banned ids are:  {banned_tokens}, tokens: {chat_dict.v2t(banned_tokens)}')\n",
        "                beam_scores[hyp_id, banned_tokens] = -10e5\n",
        "\n",
        "        flat_beam_scores = beam_scores.view(-1)\n",
        "       \n",
        "        best_scores, best_idxs = torch.topk(flat_beam_scores, self.beam_size, dim=-1)\n",
        "\n",
        "        # change the best_scores here \n",
        "        voc_size = logprobs.size(-1)\n",
        "\n",
        "        # get the backtracking hypothesis id as a multiple of full voc_sizes\n",
        "        hyp_ids = best_idxs / voc_size\n",
        "        # get the actual word id from residual of the same division\n",
        "        tok_ids = best_idxs % voc_size\n",
        "        # print('tok_ids', chat_dict.v2t(tok_ids.cpu().tolist()))\n",
        "        return (hyp_ids, tok_ids, best_scores)\n",
        "    \n",
        "    def advance(self, logprobs, previous_hyps):\n",
        "        \"\"\"Advance the beam one step.\"\"\"\n",
        "        current_length = len(self.all_scores) - 1\n",
        "        if current_length < self.min_length:\n",
        "            # penalize all eos probs to make it decode longer\n",
        "            for hyp_id in range(logprobs.size(0)):\n",
        "                logprobs[hyp_id][self.eos] = -10e5\n",
        "\n",
        "        if self.scores is None:\n",
        "            logprobs = logprobs[0:1]  # we use only the first hyp now, since they are all same\n",
        "            self.scores = torch.zeros(1).type_as(logprobs).to(logprobs.device)\n",
        "            \n",
        "        hyp_ids, tok_ids, self.scores = self.select_paths(logprobs, self.scores, previous_hyps)\n",
        "\n",
        "        # clone scores here to avoid referencing penalized EOS in the future!\n",
        "        self.all_scores.append(self.scores.clone())\n",
        "        # print('self.all_scores', self.all_scores)\n",
        "        self.outputs.append(tok_ids)\n",
        "        self.bookkeep.append(hyp_ids)\n",
        "        self.partial_hyps = [\n",
        "            self.partial_hyps[hyp_ids[i]] + [tok_ids[i].item()]\n",
        "            for i in range(self.beam_size)\n",
        "        ]\n",
        "        self.history_hyps.extend(self.partial_hyps)\n",
        "        \n",
        "    \n",
        "        # print('history hyps ids: ', self.history_hyps)\n",
        "        # print('parital hyps at this step: ', self.partial_hyps)\n",
        "        # print('log prob size',logprobs.size(0), logprobs.size(1))\n",
        "        \n",
        "        #print('history hyps tokens: ', chat_dict.v2t(self.history_hyps.cpu().tolist()))\n",
        "\n",
        "        #  check new hypos for eos label, if we have some, add to finished\n",
        "        for hypid in range(self.beam_size):\n",
        "            if self.outputs[-1][hypid] == self.eos:\n",
        "                self.scores[hypid] = -10e5\n",
        "                #  this is finished hypo, adding to finished\n",
        "                eostail = _HypothesisTail(\n",
        "                    timestep=len(self.outputs) - 1,\n",
        "                    hypid=hypid,\n",
        "                    score=self.all_scores[-1][hypid],\n",
        "                    tokenid=self.eos,\n",
        "                )\n",
        "                self.finished.append(eostail)\n",
        "                self.n_best_counter += 1\n",
        "\n",
        "        if self.outputs[-1][0] == self.eos:\n",
        "            self.eos_top = True\n",
        "            if self.eos_top_ts is None:\n",
        "                self.eos_top_ts = len(self.outputs) - 1\n",
        "    \n",
        "    def is_done(self):\n",
        "        \"\"\"Return whether beam search is complete.\"\"\"\n",
        "        return self.eos_top and self.n_best_counter >= self.min_n_best\n",
        "\n",
        "    def get_top_hyp(self):\n",
        "        \"\"\"\n",
        "        Get single best hypothesis.\n",
        "        :return: hypothesis sequence and the final score\n",
        "        \"\"\"\n",
        "        return self._get_rescored_finished(n_best=1)[0]\n",
        "\n",
        "    def _get_hyp_from_finished(self, hypothesis_tail):\n",
        "        \"\"\"\n",
        "        Extract hypothesis ending with EOS at timestep with hyp_id.\n",
        "        :param timestep:\n",
        "            timestep with range up to len(self.outputs) - 1\n",
        "        :param hyp_id:\n",
        "            id with range up to beam_size - 1\n",
        "        :return:\n",
        "            hypothesis sequence\n",
        "        \"\"\"\n",
        "        hyp_idx = []\n",
        "        endback = hypothesis_tail.hypid\n",
        "        for i in range(hypothesis_tail.timestep, -1, -1):\n",
        "            hyp_idx.append(\n",
        "                _HypothesisTail(\n",
        "                    timestep=i,\n",
        "                    hypid=endback,\n",
        "                    score=self.all_scores[i][endback],\n",
        "                    tokenid=self.outputs[i][endback],\n",
        "                )\n",
        "            )\n",
        "            endback = self.bookkeep[i - 1][endback]\n",
        "\n",
        "        return hyp_idx\n",
        "\n",
        "    def _get_pretty_hypothesis(self, list_of_hypotails):\n",
        "        \"\"\"Return hypothesis as a tensor of token ids.\"\"\"\n",
        "        return torch.stack([ht.tokenid for ht in reversed(list_of_hypotails)])\n",
        "\n",
        "    def _get_rescored_finished(self, n_best=None, add_length_penalty=False):\n",
        "        \"\"\"\n",
        "        Return finished hypotheses according to adjusted scores.\n",
        "        Score adjustment is done according to the Google NMT paper, which\n",
        "        penalizes long utterances.\n",
        "        :param n_best:\n",
        "            number of finalized hypotheses to return\n",
        "        :return:\n",
        "            list of (tokens, score) pairs, in sorted order, where:\n",
        "              - tokens is a tensor of token ids\n",
        "              - score is the adjusted log probability of the entire utterance\n",
        "        \"\"\"\n",
        "        # if we never actually finished, force one\n",
        "        if not self.finished:\n",
        "            self.finished.append(\n",
        "                _HypothesisTail(\n",
        "                    timestep=len(self.outputs) - 1,\n",
        "                    hypid=0,\n",
        "                    score=self.all_scores[-1][0],\n",
        "                    tokenid=self.eos,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        rescored_finished = []\n",
        "        for finished_item in self.finished:\n",
        "            if add_length_penalty:\n",
        "                current_length = finished_item.timestep + 1\n",
        "                # these weights are from Google NMT paper\n",
        "                length_penalty = math.pow((1 + current_length) / 6, 0.65)\n",
        "            else:\n",
        "                length_penalty = 1\n",
        "            rescored_finished.append(\n",
        "                _HypothesisTail(\n",
        "                    timestep=finished_item.timestep,\n",
        "                    hypid=finished_item.hypid,\n",
        "                    score=finished_item.score / length_penalty,\n",
        "                    tokenid=finished_item.tokenid,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Note: beam size is almost always pretty small, so sorting is cheap enough\n",
        "        srted = sorted(rescored_finished, key=attrgetter('score'), reverse=True)\n",
        "\n",
        "        if n_best is not None:\n",
        "            srted = srted[:n_best]\n",
        "\n",
        "        return [\n",
        "            (self._get_pretty_hypothesis(self._get_hyp_from_finished(hyp)), hyp.score)\n",
        "            for hyp in srted\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lTagb09bIix",
        "colab_type": "text"
      },
      "source": [
        "## You present here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuNlVWtqbIix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check pdf to see what you expected to present\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZluSsCyNoHuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_with_beam(beam_size, min_n_best, model, batch, batch_size, previous_hyps=None, similarity_metric='hamming', similarity_threshold=0, verbose=False, N = 1):\n",
        "    \"\"\"\n",
        "    This function takes a model, batch, beam settings and performs decoding with a beam\n",
        "    \"\"\"\n",
        "    beams = [Beam(beam_size, min_n_best=min_n_best, eos_token=chat_dict.word2ind['__end__'], \n",
        "                  padding_token=chat_dict.word2ind['__null__'], \n",
        "                  bos_token=chat_dict.word2ind['__start__'], \n",
        "                  device=current_device, similarity_metric=similarity_metric, \n",
        "                  similarity_threshold=similarity_threshold, N = N) for _ in range(batch_size)]\n",
        "    \n",
        "    repeated_inds = torch.arange(batch_size).to(current_device).unsqueeze(1).repeat(1, beam_size).view(-1)\n",
        "\n",
        "    text_vecs = batch['text_vecs'].to(current_device)\n",
        "\n",
        "    encoder_states = model.encoder(text_vecs, batch['text_lens'], use_packed=batch['use_packed'])\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    encoder_states = reorder_encoder_states(encoder_states, repeated_inds)  # no actual reordering here, but repeating beam size times each sample in the minibatch\n",
        "    encoder_output, encoder_hidden, attention_mask = encoder_states\n",
        "    \n",
        "    incr_state = encoder_hidden  # we init decoder hidden with last encoder_hidden\n",
        "    \n",
        "    # 1 is a start token id\n",
        "    starts = torch.Tensor([1]).long().to(model.decoder.embedding.weight.device).expand(batch_size*beam_size, 1).long()  # expand to batch_size * beam_size\n",
        "    decoder_input = starts\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ts in range(100):\n",
        "            if all((b.is_done() for b in beams)):\n",
        "                break\n",
        "            score, incr_state, attn_w_log = model.decoder(decoder_input, incr_state, encoder_states)\n",
        "            score = score[:, -1:, :]  # take last time step and eliminate the dimension\n",
        "            score = score.view(batch_size, beam_size, -1)\n",
        "            score = torch.log_softmax(score, dim=-1)\n",
        "         \n",
        "            for i, b in enumerate(beams):\n",
        "                if not b.is_done():\n",
        "                    # make mock previous_hyps if not used #\n",
        "                    if previous_hyps is None:\n",
        "                        previous_hyps = [{} for i in range(batch_size)]\n",
        "                    b.advance(score[i], previous_hyps[i])\n",
        "    \n",
        "            incr_state_inds = torch.cat([beam_size * i + b.get_backtrack_from_current_step() for i, b in enumerate(beams)])\n",
        "            incr_state = reorder_decoder_incremental_state(incr_state, incr_state_inds)\n",
        "            selection = torch.cat([b.get_output_from_current_step() for b in beams]).unsqueeze(-1)\n",
        "            #print('size of previous hyps', [b.get_output_from_current_step() for b in beams])\n",
        "            decoder_input = selection\n",
        "\n",
        "    beam_preds_scores = [list(b.get_top_hyp()) for b in beams]\n",
        "\n",
        "    if verbose:\n",
        "        for bi in range(batch_size):\n",
        "            print(f'batch {bi}')\n",
        "            for i in get_nbest_list_from_beam(beams[bi], chat_dict, n_best=min_n_best):\n",
        "                print(i)\n",
        "    \n",
        "    return beam_preds_scores, beams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFll2DU5ryIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "26c4c8e9-42eb-4c75-bd14-3999c8e52c59"
      },
      "source": [
        "batch_size = 1\n",
        "beam_size = 20\n",
        "beam_n_best = 10\n",
        "\n",
        "valid_loader_single = DataLoader(valid_dataset, shuffle=False, collate_fn=batchify, batch_size=batch_size)\n",
        "\n",
        "valid_sample = next(iter(valid_loader_single))\n",
        "\n",
        "print('Single example from validation set')\n",
        "print('Validation text: ', chat_dict.v2t(valid_sample['text_vecs'][0].cpu().tolist()))\n",
        "print('Validation target: ', chat_dict.v2t(valid_sample['target_vecs'][0].cpu().tolist()))\n",
        "\n",
        "\n",
        "print('ngram = 1')\n",
        "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, verbose=True, N = 1)\n",
        "\n",
        "print('ngram = 2')\n",
        "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, verbose=True, N = 2)\n",
        "\n",
        "print('ngram = 3')\n",
        "beam_preds_scores, beams = generate_with_beam(beam_size, beam_n_best, model, valid_sample, batch_size=batch_size, verbose=True, N = 3)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Single example from validation set\n",
            "Validation text:  your persona : i read twenty books a year . \n",
            " your persona : i ' m a stunt double as my second job . \n",
            " your persona : i only eat __unk__ . \n",
            " your persona : i was raised in a single parent household . \n",
            " hello what are doing today ?\n",
            "Validation target:  i am good , i just got off work and tired , i have two jobs . __end__\n",
            "ngram = 1\n",
            "batch 0\n",
            "('__start__ i am good how are you __end__', -5.82402229309082)\n",
            "(\"__start__ i ' m good how are you __end__\", -6.111946105957031)\n",
            "('__start__ hi how are you today __end__', -6.116390228271484)\n",
            "('__start__ i am good how are you ? __end__', -6.397331237792969)\n",
            "('__start__ hi how are you doing __end__', -6.450916290283203)\n",
            "(\"__start__ i ' m good how are you ? __end__\", -6.650628089904785)\n",
            "(\"__start__ i ' m good . how are you ? __end__\", -7.28643798828125)\n",
            "(\"__start__ i ' m doing well , how are you ? __end__\", -7.779935836791992)\n",
            "(\"__start__ i ' m doing great . how are you ? __end__\", -7.984478950500488)\n",
            "(\"__start__ i ' m doing well . how are you ? __end__\", -8.081779479980469)\n",
            "ngram = 2\n",
            "batch 0\n",
            "('__start__ hi how are you today __end__', -6.074948310852051)\n",
            "('__start__ i am good how are you __end__', -6.224578857421875)\n",
            "('__start__ hi how are you doing __end__', -6.358716011047363)\n",
            "(\"__start__ i ' m good how are you __end__\", -6.630590438842773)\n",
            "('__start__ hi how are you doing ? __end__', -6.773155212402344)\n",
            "('__start__ hi how are you today ? __end__', -6.77985954284668)\n",
            "('__start__ i am good how are you ? __end__', -6.785019874572754)\n",
            "(\"__start__ i ' m good how are you ? __end__\", -7.146596908569336)\n",
            "(\"__start__ i ' m doing well , how are you ? __end__\", -8.104902267456055)\n",
            "(\"__start__ i ' m doing great . how are you ? __end__\", -8.402371406555176)\n",
            "ngram = 3\n",
            "batch 0\n",
            "('__start__ hi how are you today __end__', -6.074948310852051)\n",
            "('__start__ i am good how are you __end__', -6.224578857421875)\n",
            "('__start__ hi how are you doing __end__', -6.358716011047363)\n",
            "(\"__start__ i ' m good how are you __end__\", -6.630590438842773)\n",
            "('__start__ hi how are you doing ? __end__', -6.773155212402344)\n",
            "('__start__ hi how are you today ? __end__', -6.77985954284668)\n",
            "('__start__ i am good how are you ? __end__', -6.785019874572754)\n",
            "(\"__start__ i ' m good how are you ? __end__\", -7.146596908569336)\n",
            "(\"__start__ i ' m doing well , how are you ? __end__\", -8.104902267456055)\n",
            "(\"__start__ i ' m doing great . how are you ? __end__\", -8.402371406555176)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2PaOdTnbIi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}