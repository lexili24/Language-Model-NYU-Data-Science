{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of lm_homework.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HCzN-nn8BsK",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNWcR__B8d-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "24b79dfd-87d6-482b-edee-b51bdae999e6"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P3D72q38BsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import operator\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZXhR1La8BsS",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyYA19ji8BsU",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZfFvEXa8BsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V50F-WEd8Bsc",
        "colab_type": "code",
        "outputId": "e6005b00-70b4-445a-c095-0a79fcb1b448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJp7NFs8Bsk",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "820qKy6U8Bsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlaXszzj8Bsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# delta = 0.0005\n",
        "# for n in [2, 3, 4]:\n",
        "#     lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm.estimate(datasets['train'])\n",
        "\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7cWyvDm8BtA",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5imstdWS8BtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwE6KP0P8BtF",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj8Z2ly_8BtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXlT_7WV8BtK",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f59E088X8BtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler,DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDZSVhLr8BtP",
        "colab_type": "text"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXP5LO2P8BtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>') # validation token is not seen in the training dataset\n",
        "                                # <unk> is in the original training dataset. \n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkqJe4HI8BtU",
        "colab_type": "code",
        "outputId": "3715156c-b7fe-42f5-cb0f-80f512704c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "train_dict=Dictionary(datasets) # excluding validation dataset #109\n",
        "#all_dict=Dictionary(datasets, include_valid = True)\n",
        "\n",
        "# example\n",
        "rand_int = np.random.randint(1, len(datasets['valid']))\n",
        "print(' '.join(datasets['valid'][rand_int]))\n",
        "encoded = train_dict.encode_token_seq(datasets['valid'][rand_int])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = train_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "# checking\n",
        "print('length of train_dict is ', len(train_dict)) # that's because <unk> is already in the dataset \n",
        "print('unique words in training dataset is ', len(vocab))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:19<00:00, 561.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "He was succeeded as AOC <unk> by Air Vice Marshal <unk> Murdoch .\n",
            "\n",
            " encoded - [858, 118, 5931, 14, 15272, 3, 28, 6074, 3048, 8170, 3, 30459, 39]\n",
            "\n",
            " decoded - ['He', 'was', 'succeeded', 'as', 'AOC', '<unk>', 'by', 'Air', 'Vice', 'Marshal', '<unk>', 'Murdoch', '.']\n",
            "length of train_dict is  33178\n",
            "unique words in training dataset is  33175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhkIjBnj8BtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given the dictionary from above, now write a function that tokenize the all datasets into id's\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets\n",
        "\n",
        "# Given a tokenzied dataset with ngram defined, slice the input sequences into n-grams \n",
        "# [0,1,2,3,4,5], 2 -> [0,1], [1,2], [2,3], [3,4], [4,5]\n",
        "def slice_sequences_given_order(tokenized_dataset_with_spec, ngram_order=2):\n",
        "    sliced_datasets = {}\n",
        "    for split, dataset in tokenized_dataset_with_spec.items():\n",
        "        _list_of_sliced_ngrams = []\n",
        "        for seq in tqdm(dataset):\n",
        "            ngrams = [seq[i:i+ngram_order] for i in range(len(seq)-ngram_order+1)]\n",
        "            _list_of_sliced_ngrams.extend(ngrams)\n",
        "        sliced_datasets[split] = _list_of_sliced_ngrams\n",
        "\n",
        "    return sliced_datasets\n",
        "\n",
        "# # Now we create a dataset\n",
        "class NgramDataset(Dataset):\n",
        "    def __init__(self, sliced_dataset_split):\n",
        "        super().__init__()\n",
        "\n",
        "        # for each sample: [:-1] is input, [-1] is target\n",
        "        self.sequences = [torch.tensor(i, dtype=torch.long) for i in sliced_dataset_split]\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        sample = self.sequences[i]\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "def batchify(list_minibatch):\n",
        "    inp_list = [i[:-1] for i in list_minibatch]\n",
        "    tar_list = [i[-1] for i in list_minibatch]\n",
        "\n",
        "    inp_tensor = torch.stack(inp_list, dim=0) # list of tensors and create a new tensor a u-dimension specified by dim\n",
        "    tar_tensor = torch.stack(tar_list, dim=0)\n",
        "    # cat: take a list of tensors and use existing dimension to concatent. Cannot create a new u-dimension speicfied. \n",
        "\n",
        "    return inp_tensor, tar_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYuFOMTg8Btd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY8VffsL8Btg",
        "colab_type": "code",
        "outputId": "5944ccc2-5b2f-43b6-d9cc-ee0302016ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "#  returns dictionary of three items with 'train', 'valid' and 'test' with lists of token ids \n",
        "tokenized_ngram = tokenize_dataset(datasets, train_dict, ngram_order=2)\n",
        "\n",
        "# returns dictionary of three and lists of sliced n-grams\n",
        "sliced_ngram = slice_sequences_given_order(tokenized_ngram, ngram_order=2)\n",
        "\n",
        "# check that the sentence is encoded with (n-1)<bos> and can be decoded back to tokens \n",
        "decoded_with_spec = train_dict.decode_idx_seq(tokenized_ngram['train'][3010])\n",
        "print(f'\\n decoded with spec - {decoded_with_spec}')\n",
        "\n",
        "ngram_datasets = {}\n",
        "ngram_loaders = {}\n",
        "for split, dataset_sliced in sliced_ngram.items():\n",
        "    if split == 'train':\n",
        "        shuffle_ = True\n",
        "    else:\n",
        "        shuffle_ = False\n",
        "    dataset_ = NgramDataset(dataset_sliced)\n",
        "    ngram_datasets[split] = dataset_\n",
        "    ngram_loaders[split] = DataLoader(dataset_, batch_size=2048, shuffle=shuffle_, collate_fn=batchify)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 125192.03it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 115692.12it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 115844.51it/s]\n",
            "100%|██████████| 78274/78274 [00:03<00:00, 24533.29it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 131990.11it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 138521.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " decoded with spec - ['<bos>', 'The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu1jlFox8Btk",
        "colab_type": "code",
        "outputId": "3e66c0b8-3176-47c4-87d7-bd35751e2889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('The length of original training dataset is: ', len(datasets['train']))\n",
        "print('The length of tokenzied training dataset is: ', len(tokenized_ngram['train']))\n",
        "print('Slided_ngram for training dataset now has length of ', len(sliced_ngram['train']))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of original training dataset is:  78274\n",
            "The length of tokenzied training dataset is:  78274\n",
            "Slided_ngram for training dataset now has length of  2003028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJBw4I88Bto",
        "colab_type": "code",
        "outputId": "765a237d-6370-4526-bdaf-38bf3e480a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(tokenized_ngram['valid'][10])\n",
        "print(train_dict.decode_idx_seq(tokenized_ngram['valid'][10]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 504, 1506, 7106, 741, 459, 20, 140, 98, 432, 19366, 10, 150, 15605, 13, 260, 3748, 955, 1826, 1384, 1145, 98, 1722, 4303, 39, 1]\n",
            "['<bos>', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSWY6ayI8Bt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "    \n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors]) # t.size(-1) changes the type from torch.Size to int.\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:  # dim = 0: cat on row, dim = 1: add on column \n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = persona_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM1MWdTo8BuD",
        "colab_type": "code",
        "outputId": "02e6d6df-1dfc-473f-937c-9d7ca9c11a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tensor_dataset = {}\n",
        "\n",
        "# split: train, valid, test\n",
        "for split, listoflists in tokenized_ngram.items():\n",
        "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "tensor_dataset['train'][24]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  0, 282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39]]),\n",
              " tensor([[282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39,   1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRvBF-uB8BuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders = {}\n",
        "batch_size = 32\n",
        "for split, dataset in tensor_dataset.items():\n",
        "    loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1xV4hU28BuO",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model: RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlERvjpg8BuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import RNNBase, RNN\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Linear, functional\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUJELhUY8BuU",
        "colab_type": "code",
        "outputId": "bb6cde18-0816-43eb-d524-74533a6ffe39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example embedding layer\n",
        "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
        "lookup.weight.size()\n",
        "# train_dict = vocab size + 4"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33178, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHnJ96W8BuY",
        "colab_type": "code",
        "outputId": "a44bc308-f851-4293-9476-23e41e08d140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Example input\n",
        "input_ = train_dict.encode_token_seq('hello world'.split(' '))\n",
        "print(f'discrete input: {[input_]}')\n",
        "\n",
        "input_continious = lookup(torch.tensor([input_], dtype=torch.long))\n",
        "print(f'continious input size: {input_continious.size()}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discrete input: [[3, 693]]\n",
            "continious input size: torch.Size([1, 2, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHTC5cXk8Buc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "# before nn.RNN is hidden\n",
        "# Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
        "# RNN natrually takes multi sentence inputs and outputs hidden_size \n",
        "rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "projection = nn.Linear(options['hidden_size'], options['num_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvwxE89D8Buf",
        "colab_type": "code",
        "outputId": "ac0e839d-fbca-4b1e-f9ec-d3dd41df9e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lookup"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(33178, 64, padding_idx=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGE_fb7i8Bui",
        "colab_type": "code",
        "outputId": "65f71e3a-e661-4690-c0fb-7a462a62b0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rnn"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9XC3i-w8Buq",
        "colab_type": "code",
        "outputId": "14db14b4-339f-4ee8-ece2-a22052369202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "projection"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=128, out_features=33178, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvqpivh8Buu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        # before nn.RNN is hidden\n",
        "        # Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
        "        # RNN natrually takes multi sentence inputs and outputs hidden_size \n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        # project of outputs \n",
        "        # rnn_outputs: tupple with second element being last hidden state. \n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYq0aW18Bux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell need to be moved forward in oder to prevent error from occuring.\n",
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 64\n",
        "    hidden_size = 128 # output of dimension \n",
        "    num_layers = 2\n",
        "    rnn_dropout = 0.1\n",
        "    input_size = lookup.weight.size(1)\n",
        "    vocab_size = lookup.weight.size(0)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'rnn_dropout': rnn_dropout,\n",
        "    }\n",
        "\n",
        "    \n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGnsOgMI8Bu0",
        "colab_type": "code",
        "outputId": "ed9c6a52-49b8-4823-8ada-5b9726d89c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# check model\n",
        "model"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLanguageModel(\n",
              "  (lookup): Embedding(33178, 64, padding_idx=2)\n",
              "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pka57Cz78Bu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the following two chunks out "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOgwouSJ8Bu9",
        "colab_type": "code",
        "outputId": "2dc086ea-937d-4d11-c1b6-902b0d37a19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "source": [
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(100):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4347\n",
            "Step 100 avg train loss = 7.7255\n",
            "Step 200 avg train loss = 6.9448\n",
            "Step 300 avg train loss = 6.7479\n",
            "Step 400 avg train loss = 6.6020\n",
            "Step 500 avg train loss = 6.5238\n",
            "Step 600 avg train loss = 6.4444\n",
            "Step 700 avg train loss = 6.3687\n",
            "Step 800 avg train loss = 6.3088\n",
            "Step 900 avg train loss = 6.2642\n",
            "Step 1000 avg train loss = 6.2030\n",
            "Step 1100 avg train loss = 6.1762\n",
            "Step 1200 avg train loss = 6.1313\n",
            "Step 1300 avg train loss = 6.1102\n",
            "Step 1400 avg train loss = 6.0654\n",
            "Step 1500 avg train loss = 6.0286\n",
            "Step 1600 avg train loss = 6.0137\n",
            "Step 1700 avg train loss = 5.9976\n",
            "Step 1800 avg train loss = 5.9683\n",
            "Step 1900 avg train loss = 5.9564\n",
            "Step 2000 avg train loss = 5.9412\n",
            "Step 2100 avg train loss = 5.9246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-a6274d42989d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfw_BFDp8BvB",
        "colab_type": "code",
        "outputId": "95abf217-cf3c-485e-c073-d684766f7b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of RNN baseline model')\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XeYVdW5+PHvO733wjSYofcyjoiC\nBQUVo4KJMRh7LNFobow3RXPzSzTlxpjEkptEo9FYoqCxd2PBgoqAiHSQMsAUmMr0Puv3x9ozHIbp\nhVPm/TzPfnbf5z17zrxnn7XXXkuMMSillPJdfu4OQCml1NDSRK+UUj5OE71SSvk4TfRKKeXjNNEr\npZSP00SvlFI+ThO98loiMldEvhKRahFZ4u542oiIEZGxx/D1Mp3XDHDm3xCRK47V6w8GETlNRPJ6\nue3tIvKvoY7Jl2ii9yAikisidU7iOigij4pIhLPufRGpd9aViMjzIpLirHtURH7j3ujd4lfAX4wx\nEcaYFzuu7HA+D7ieT2f9o06CnO2ybKyIGJf5tvOe4bJsgYjkDt3bGhhjzCJjzGPujkN5Dk30nuc8\nY0wEkA3kAD93WXeTs248EAPc44b4aLty9ACjgM09bNN2PmcCs4DbOqwvA3r6kqwB/l+/IlTKA2ii\n91DGmHzgDWBqJ+vKgOc6W9cTEZknIp+IyCER2S8iVzrL3xeRa1y2u1JEVrrMGxG5UUS+Ar4SkftF\n5I8djv2SiNziTKeKyHMiUiwie0Tkv1y2my0ia0Wk0vnlcnc38V4rIjtFpExEXhaRVGf5LmA08Ipz\nxR7c3fs2xhwA3sImfFePAdNF5NRudv8zcLGIjOnuNTo4R0R2O7++/iAifk7cY0TkPREpddY9KSIx\nLu/3pyKSLyJVIrJdRM5wlvuJyK0issvZ9xkRievshV3/lm1/RxH5o4iUO3+LRS7bRovIwyJS6Lzu\nb0TEv4vj3i4i/xaRfznxbRSR8SJym4gUOZ+nM122T3X+ZmXO3/Bal3Whzi+qchHZAhzf4bW6/Pyo\nvtNE76GcooJzgC86WZcAfKOzdT0ccxT2y+P/gERs0lvfh0MsAU4AJgPLgG+JiDjHjgXOBJY7Se0V\n4EsgDTgDuFlEznKOcx9wnzEmChgDPNNFvKcDvwMuAlKAvcByAGPMGGAfzhW7Maahh/eeDiwCdnZY\nVQv8L/DbbnbPBx4C7ujuNTq4APuLLBtYDHynLRTse0oFJgEZwO1OjBOAm4DjjTGRwFlArrPf97Hn\n/1Rn33Lgr72M5QRgO5AA3AU83PZ3Ax4FmoGx2F88ZwLXdHKMNucBTwCx2M/fW9g8koYtSvu7y7bL\ngTwn3guB/3X+pgC/xP7txzjvs/2eQi8+P6qvjDE6eMiA/aeuBg5hk9rfgFBn3fvYpHQIm3ieBBKd\ndY8Cv+nF8W8DXuhi3fvANS7zVwIrXeYNcLrLvGAT7SnO/LXAe870CcC+Tl77n870h9ikmdBDvA8D\nd7nMRwBNQKbL+VrQi/NZ5cT/LhDjsv5RbLFNsPNeFmETnul4XrBfjBXAFGABkNvN6xrgbJf57wHv\ndrHtEuALZ3osUOQcP7DDdluBM1zmU5xzEQBkOq8Z0PFv6fwdd7rsF+ZsOwJIBhraPmPO+ouBFV3E\nejvwtsv8ec759XfmI51jx2C/wFqASJftfwc86kzv7nCOrgPyevn5uR34l7v/X71p0Ct6z7PEGBNj\njBlljPmeMabOZd1/OevSjDGXGGOK+3jsDGDXAGLb3zZh7H/ccmxiAPg29ssHbNl5qlM8dEhEDgE/\nwyYWgKux9xm2icgaETm3i9dLxX7htb1mNVCKvcrrrSXGXh2fBkzEXtUewdhfA792hk455/ov2KvW\n3tjvMr0X+14QkWQRWe4Uk1QC/2qLyRizE7gZm8iKnO1SnWOMAl5wOZ9bsYk0mZ4dcHkftc5khHPM\nQKDQ5bh/B5K6OdZBl+k6oMQY0+Iy33bsVKDMGFPV4Ty0/e1SOfoctenp86P6SBP98LIf+1O5MzXY\nq702IzrZpmNTp8uAC50ioROw9w3aXmeP86XUNkQaY84BMMZ8ZYy5GJtQfg88KyLhnbxeAfafHgBn\nm3jsL5o+McZ8gL2C/2MXm/wTeyX69W4O8wdgPnBcL14yw2V6JPa9gC0mMsA0Y4uuLsX+OmqL8ylj\nzDzs+zbY8wP2nC7qcE5DjL2X01/7sVf0CS7HjDLGTBnAMdsUAHEiEumybCSH/3aFHH2OXOPq8vOj\n+k4Tve/wF5EQlyGok22eBBaIyEUiEiAi8SLSdnNyPfB1EQkTWwf86p5e0BjzBVAC/AN4yxhzyFm1\nGqhybiyGioi/iEwVkeMBRORSEUk0xrRii6IAWjt5iWXAVSIy07nZ+r/AZ8aY3F6dkaPdCywUkRmd\nvJdmbLnxT7va2Xl/fwJ+0ovX+rGIxDr3Wn4APO0sj8QWd1SISBrw47YdRGSCiJzuvNd67BVy23l5\nAPit86WKiCSKyOJexNElY0wh8B/gTyIS5dzwHSPd35ju7bH3A58Av3M+j9Oxn6m2+u/PALc55ygd\new+iTbefH9V3muh9x63YxNA2vNdxA2PMPuwN3v/GVitcD7QlvXuARuxP88c4XAzTk6ewZcpPubxO\nC3Au9mbvHg5/GUQ7m5wNbBaRauyN2aUdiqjajvMOtlrjc9grwDHA0l7GdRSn+OVx4BddbLLMeZ3u\n3IctMunJS8Dn2HP8GvZ+A9h7E9nY8v7XgOdd9gkG7sSerwPYXzxt1UHvA14G/iMiVcAq7K+ogboc\nCAK2YG/wPost/x8MF2PvHxQALwC/dP6mYM/DXuzn4z/YG7xArz4/qo/EFrUqpZTyVXpFr5RSPk4T\nvVJK+ThN9Eop5eM00SullI/ziMapEhISTGZmprvDUEopr/L555+XGGMSe9rOIxJ9ZmYma9eudXcY\nSinlVURkb89badGNUkr5vB4TvfNU22oR+VJENovIHc7yR53mQ9c7w0xnuYjIn51mSTeISPZQvwml\nlFJd603RTQO21cJqEQkEVorIG866Hxtjnu2w/SJgnDOcANzP4DzBp5RSqh96TPROK4XVzmygM3T3\nOO1i4HFnv1UiEiMiKU67GkqpYaqpqYm8vDzq6+vdHYrXCQkJIT09ncDAwH7t36ubsWJ7nPkc2172\nX40xn4nIDdhGln6Bbef7Vqe51zSObH40z1lW2OGY12HboGbkSNeG65RSvigvL4/IyEgyMzM53O+J\n6okxhtLSUvLy8sjKyurXMXp1M9YY02KMmQmkA7NFZCq2saWJ2C7A4uim1b8ujvmgMSbHGJOTmNhj\n7SCllJerr68nPj5ek3wfiQjx8fED+iXUp1o3TjOtK7A9wxQaqwHblvdsZ7N8jmxnOp1+tB+ulPI9\nmuT7Z6DnrTe1bhLF6bxYREKBhdiegVKcZYLtDm2Ts8vLwOVO7Zs5QMVQlc9vP1DFnW9so7K+aSgO\nr5RSPqE3V/QpwAoR2QCswfYZ+SrwpIhsBDZiu0L7jbP969j+IHdiO1T+3qBH7dhXVssDH+xiV1F1\nzxsrpYa10tJSZs6cycyZMxkxYgRpaWnt842Njb06xlVXXcX27duHNM709HQOHTrU84Z90JtaNxuw\nvcN3XH56J5u31dK5ceCh9SwrwfY+l1taw6yRscfiJZVSXio+Pp7169cDcPvttxMREcGPfvSjI7Zp\n70zbr/Nr4H/+859DHudQ8OonY0fGheEnsKe4xt2hKKW81M6dO5k8eTKXXHIJU6ZMobCwkOuuu46c\nnBymTJnCr351uD/4efPmsX79epqbm4mJieHWW29lxowZnHjiiRQVFR117J///OdcccUVzJkzh3Hj\nxvHII48A8M477zB//nwWLVrEhAkTuPHGGxnKTqA8oq2b/goK8CM9NozdJZrolfImd7yymS0FlYN6\nzMmpUfzyvP71a75t2zYef/xxcnJyALjzzjuJi4ujubmZ+fPnc+GFFzJ58uQj9qmoqODUU0/lzjvv\n5JZbbuGRRx7h1ltvPerYGzdu5JNPPqGyspLs7Gy+9rWvAfDZZ5+xZcsWMjIyWLhwIS+99BJLlizp\nV/w98eorerDFN3s00SulBmDMmDHtSR5g2bJlZGdnk52dzdatW9myZctR+4SGhrJo0SIAjjvuOHJz\nczs99pIlSwgJCSEpKYlTTjmFNWvWADBnzhwyMzPx9/dn6dKlrFy5cvDfmMOrr+jBJvo1uWUYY7Tq\nllJeor9X3kMlPDy8ffqrr77ivvvuY/Xq1cTExHDppZd2Woc9KCiofdrf35/m5uZOj90xL7XNd7V8\nKHj9Ff3oxHBqG1soqmpwdyhKKR9QWVlJZGQkUVFRFBYW8tZbbw3oeC+++CINDQ0UFxfz0Ucftf9y\nWLVqFfv27aOlpYVnnnmGefPmDUb4nfKJK3qAPSU1JEeFuDkapZS3y87OZvLkyUycOJFRo0Yxd+7c\nAR1v6tSpnHrqqZSWlnLHHXeQnJzMxo0bmT17Ntdffz27du1iwYIFnH/++YP0Do7mU4l+zuh4N0ej\nlPIGt99+e/v02LFj26tdgi1CeeKJJzrdz7Uc3bWu+9KlS1m6dGmn+8yaNYvHHnvsqOXR0dG8+OKL\nRy3Py8vrMf6+8vqim9ToUIIC/PSGrFJKdcHrr+j9/ITM+DB2a116pZSH+c1vftPp8gULFrBgwYJj\nFofXX9FDWxVLbQZBKaU64yOJPoJ9ZbU0t7S6OxSllPI4PpHoRyeE09RiKDikPdcopVRHPpHosxJt\nzZvdWnyjlFJH8Y1E71LFUimlOjN//vyjHn669957ueGGG7rdLyIiAoCCggIuvPDCTrc57bTTWLt2\n7YBjfP/99zn33HMHfJyOfCLRx4cHERkcoIleKdWliy++mOXLlx+xbPny5Vx88cW92j81NZVnn312\nKEIbcj6R6EWErERt3Ewp1bULL7yQ1157rb2TkdzcXAoKCjj55JOprq7mjDPOIDs7m2nTpvHSSy8d\ntX9ubi5Tp04FoK6ujqVLlzJp0iQuuOAC6urqOn3NzMxMfvKTnzBt2jRmz57Nzp07Abjyyiu5/vrr\nycnJYfz48bz66qtD9K4tr69H3yYrIZy1ueXuDkMp1Rtv3AoHNg7uMUdMg0V3drk6Li6O2bNn88Yb\nb7B48WKWL1/ORRddhIgQEhLCCy+8QFRUFCUlJcyZM4fzzz+/y4bG7r//fsLCwti6dSsbNmwgOzu7\ny9eNjo5m48aNPP7449x8883tST03N5fVq1eza9cu5s+f3/4lMBR84ooebKIvqKijvqnF3aEopTyU\na/GNa7GNMYaf/exnTJ8+nQULFpCfn8/Bgwe7PM6HH37IpZdeCsD06dOZPn16t6/ZNv7000/bl190\n0UX4+fkxbtw4Ro8ezbZt2wb8/rriU1f0xth+ZMcnR7o7HKVUd7q58h5Kixcv5oc//CHr1q2jtraW\n4447DoAnn3yS4uJiPv/8cwIDA8nMzOy0aeL+cP1V0NV0Z/ODyWeu6Ecn2Dvj2hSCUqorERERzJ8/\nn+985ztH3IStqKggKSmJwMBAVqxYwd69e7s9zimnnMJTTz0FwKZNm9iwYUOX2z799NPt4xNPPLF9\n+b///W9aW1vZtWsXu3fvZsKECQN5a93ymSv6zIQwQKtYKqW6d/HFF3PBBRccUQPnkksu4bzzzmPa\ntGnk5OQwceLEbo9xww03cNVVVzFp0iQmTZrU/sugM+Xl5UyfPp3g4GCWLVvWvnzkyJHMnj2byspK\nHnjgAUJChq6ZdRnKDml7KycnxwxGHdSc37zD6RMTuevCGYMQlVJqMG3dupVJkya5O4xjKjMzk7Vr\n15KQkHDE8iuvvJJzzz23y3r5nens/InI58aYnC52aeczRTdgm0LQK3qllDqSzxTdgL0h++62IneH\noZRSAF12GP7oo48e0zh86oo+KzGckuoGKuub3B2KUqoTnlBU7I0Get58K9E7bd7kavGNUh4nJCSE\n0tJSTfZ9ZIyhtLR0QDdrfa7oBmzNm+npMW6ORinlKj09nby8PIqLi90ditcJCQkhPT293/v3mOhF\nJAT4EAh2tn/WGPNLEckClgPxwOfAZcaYRhEJBh4HjgNKgW8ZY3L7HWEfjIwLQ0Tr0ivliQIDA8nK\nynJ3GMNSb4puGoDTjTEzgJnA2SIyB/g9cI8xZixQDlztbH81UO4sv8fZ7pgICfQnLSZUa94opZSL\nHhO9sdp69Ah0BgOcDrS12fkYsMSZXuzM46w/Q4by2d4OshLCyS3VRK+UUm16dTNWRPxFZD1QBLwN\n7AIOGWOanU3ygDRnOg3YD+Csr8AW7xwToxPC2VNcozd8lFLK0atEb4xpMcbMBNKB2UD3zwf3gohc\nJyJrRWTtYN6cyUoIp6qhmZLqxkE7plJKebM+Va80xhwCVgAnAjEi0nYzNx3Id6bzgQwAZ3009qZs\nx2M9aIzJMcbkJCYm9i/6kq/gvd9C0+FW5jK1W0GllDpCj4leRBJFJMaZDgUWAluxCb+toYYrgLYu\nWV525nHWv2eGqhyl5Cv48C7IP9xOTlsrlnu0o3CllAJ6d0WfAqwQkQ3AGuBtY8yrwE+BW0RkJ7YM\n/mFn+4eBeGf5LcCtgx+2Y9SJgEDux+2L0mJDCfQXdusVvVJKAb2oR2+M2QDM6mT5bmx5fcfl9cA3\nByW6noTGQvJU2Hs40fv7CaPiw/XpWKWUcnh/EwiZc2H/amg+fPM1S1uxVEqpdt6f6EfNheY6KPii\nfdHohHByS2tpadUqlkop5QOJ/iQ73ruyfVFWQjiNza0UHKpzU1BKKeU5vD/RhydA4sQjbshqFUul\nlDrM+xM92OKb/Z9Bi31Qd7QmeqWUaucbiT5zLjRWQ+GXACRGBhMe5K+JXiml8JVEP2qeHTvl9CJC\nVqLWvFFKKfCVRB+ZDPFjYe8n7YuyEiI00SulFL6S6MGW0+/9FFpbAFvzJq+8lobmFjcHppRS7uU7\niT5zHjRUwMFNAGQlhNFqYH9ZrZsDU0op9/KdRN9Wn96pZpnlNG6m3QoqpYY730n00ekQM6q93Zus\neFvFUnubUkoNd76T6MEW3+z9GFpbiQ4LJD48SG/IKqWGPd9K9KPmQl05FG8F7A1ZLbpRSg13vpXo\nM+facXs5vdalV0op30r0MaMgKr29nD4zIZyiqgaqG5p72FEppXyXbyV6EXtVv/djMKa9zRvthEQp\nNZz5VqIHW82yphhKviIrURs3U0opH0z0h9u9yYzXRK+UUr6X6OPHQEQy5H5MSKA/aTGhmuiVUsOa\n7yV6EafdG1tOn5UQzm5N9EqpYcz3Ej3YG7JVhVC2m8yEMPYUV2OM9h+rlBqefDPRt5fTf0JWQgSV\n9c0UVze4NyallHIT30z0iRMgLAH2fsyskTEAfLqr1M1BKaWUe/hmohex1SxzP2ZGegxx4UG8v73Y\n3VEppZRVXwG7VsCHf4Bd7w35ywUM+Su4y6i5sPVl/Cv3c+r4RN7fXkRLq8HfT9wdmVJqOGlphqIt\nkL8W8pyhZAfg3Dc8+b9hzOlDGoLvJnqXdm9Om3AKL3yRz5d5h8geGeveuJRSvqWxFmqKoKYEqouc\n6WI7fWATFK6HJqcDpNA4SM+Bqd+w47RsCB36nOS7iT5pCoTEwN6VnLrwG/gJvL+tSBO9Uqrvmhuh\neJvtwe7ARjtU5NmE3ljd+T7B0ZAwDrIvh7QcSD8OYrNs0fIx1mOiF5EM4HEgGftb40FjzH0icjtw\nLdBW+P0zY8zrzj63AVcDLcB/GWPeGoLYu+fn115OHxMWRPbIWFZsL+aWMycc81CUUl6kvgIKvrBX\n422JvXg7tDbZ9QGhkDwZ0o6DiCQIT7RD23REkq0MEhji3vfhojdX9M3Afxtj1olIJPC5iLztrLvH\nGPNH141FZDKwFJgCpALviMh4Y8yx76V71FzY/jpUFjB/YhJ/eGs7RZX1JEV5zh9AKeUBSnfBjjdh\n+xuw9xNoS1cRI2DENBi30I5HTIe40eDn7954+6jHRG+MKQQKnekqEdkKpHWzy2JguTGmAdgjIjuB\n2cCngxBv37SV0+/9hNMmLOQPb23n/R3FXJSTccxDUUp5kNYW2L8adrxhk3vJDrs8cRLM/YHtrW7E\ndIhIdG+cg6RPZfQikgnMAj4D5gI3icjlwFrsVX859ktglctueXTyxSAi1wHXAYwcObIfoffCiOkQ\nFAm5K5k89RskRwXz/vYiTfRKDSctTVCZD+V74dBe2zHRV/+BujLwC7QXhDlXw4SzITbT3dEOiV4n\nehGJAJ4DbjbGVIrI/cCvseX2vwb+BHynt8czxjwIPAiQk5MzNO0T+PnDyDmw92NEhPkTknhtQyFN\nLa0E+vvmIwRKDTstTVBZYG+OVuw/nNAP7bPTlfmHi2LA1nIZdyaMPxvGngEh0e6L/RjpVaIXkUBs\nkn/SGPM8gDHmoMv6h4BXndl8wPWSOd1Z5h6Z8+Cdt6FoK6dNSGL5mv18vrecOaPj3RaSUqoXjLE1\nWmpL7VB18HAyr8g7PFQV0l4nvU1kiu1xbuQciB0FMSPtfOwoiM7wujL2gepNrRsBHga2GmPudlme\n4pTfA1wAbHKmXwaeEpG7sTdjxwGrBzXqvpj5bfjkz/D8dcy97E0C/YUV24o00SvlCRqqYevL9iGi\ntoReW2bHdWXQ0nj0Pv5BEJ1uhzHzD09Hp9skHp3hUTVePEFvrujnApcBG0VkvbPsZ8DFIjIT+1Wa\nC3wXwBizWUSeAbZga+zc6JYaN20ikuD8/4Pl3yby0z9wfObZrNhexG3nTHJbSEoNa8bA/s/giydg\n84v2qj001lZNDIuHuCxb5zw0zs63DeGJEJNhqy76adFrX/Sm1s1KoLMa/q93s89vgd8OIK7BNfFr\nkH0FrLyXb2dP4aZdYeSV15IeG+buyJQaPioL4ctlsP5JKN0JgeEw9QKYdRlknOCWB4mGC999Mraj\ns/4Xcj/irB2/JJI7eH97MZfOGeXuqJTyXS3Ntghm/yr44l+w8x0wrTDyJJh3C0xeDMER7o5yWBg+\niT44Ai54kIBHzuKP4U/wzLZMTfRK9VVri33svzIfKvKh+qCdrym2bb3UlEBtiZ2vKz+8X2QqzPsh\nzLzEdvepjqnhk+gBMo5HTv0JZ73/O97aPYP6pmxCAofX3XeletTSDPmf26Ey3xkK7FBVCK3NR+8T\nGuc0BZAASZMg7OTD8/FjIeuUYVfTxZMMr0QPcPKPqNj4Or8s+QfrN13InFkz3B2RUu5XnmvbRd/5\nLuz5CBoq7PKAUIhKtcOouYeno9LsODLF3ij1H36pxJsMv7+OfwAhFz1M89/mkvzuzTDjXb2Dr4af\n+krI/cgm913vQdluuzw6A6Ysse2jj5prr8j1JqnXG36JHghOHseT8TfynbK7YdVf4aTvuzskpYZO\nSxMUbYWCdbZVxoIv4OBmWwQTGAaZJ8MJ19vkHj9WE7sPGpaJHiAg53LeeuNDznznV8jo+TBiqrtD\nUmrgWluhZPvhhJ6/zjaz29Jg14dEQ+os23DX6PmQMRsCgt0bsxpywzbRz5+YzOKXr+Hk0P8h7Plr\n4doV+jSdcq/aMpuI+3LTsqUJCjfA3o9t87r7PoX6Q3ZdUASkzIDZ19rknjrLNrGrV+zDzrBN9Blx\nYcQlpfLXoB/y46Kfw7t3wNm/c3dYaripyION/4YNz9h+Rf0C7dOfbe2ytI8z7TgowtaG2fuJTe77\nV0NTjT1W3BiYdB6MPNF2ipEwTmu6KGAYJ3qA+RMSeeiTsdx8wtUErvqbbUPjjF/qzVk1tOoOwZaX\nbHLfu9IuS59tP3sNlYdbX9z6qq2T3imB5Ckw6xLbk9rIkyAy+Zi9BeVdhnmiT+Khj/bwwehbWCCt\n8PG9tgOCrz8IwZHuDk/5kuYG2PEWbHzGjlsa7Y3P+f8D0y60RSqdaai2Sb8t+deV2yKYjBMgLO7Y\nvgfltYZ1os/JjCMiOIB3d5Sz4IJ77BXSGz+Fh8+Ci5fZn8pK9VZrq32gqHwPlO2xddPLnXHxDmis\ngvAkOP4amPZNm7B7Ki8PjrCfy+Qpx+IdKB81rBN9UIAf88Ym8P72Igwgs6+1j2f/+0p4aD5861/2\nZ7FSHTXV2bLyfatsE7tlu+xVd1vtFgDxt+XtsVkw41swYRFknaYPF6ljbth/4uZPTOTNzQfYfrCK\niSOibF3ia96DZd+Cx86Hc++B7MvcHaZyt5oSm9T3fWqb2C1YD61Ndl3CeDuMP8sm9bgs2yVddAb4\nB7o1bKVAEz2nTUgCYMW2YpvoARLGwjXvwLPfgZdvguJtsPBXWoPB17W22ka62svE99knRvNW22Z1\nAfyDIS0bTroJMubYeuhaVq483LBP9MlRIUxJjWLFtiJuOM2lVb3QWPj2v+E//wOf/sUm+wsfGRb9\nS/q01hanzHy7vfFennu4f9FD+48segGIGGET+6zLbLXF1Jn6gJHyOsM+0YOtfXP/B7sor2kkNjzo\n8Ar/AFj0e0icCK//CB48DRbcDhPP0yqYnq65AUp32adEi12G0p1HJvOweNufaPJUmHDO4brrMaNs\n+XpgqPveg1KDRBM9cPbUEfxlxU7e3HyAi2ePPHqDnKtsGeyrN8Mzl8OIabZa3Piz9SnDY62uHEp3\n29otbe2gVxe5tInuzLc9HQqA2ASeMAHGngGJE+x04nj9haaGBU30wJTUKEYnhPPKlwWdJ3qAzLnw\nvVWw8Vn44E5YttQ+fTj/ZzDmDE34g6mx1paNl+60tVlKd9np0p22x6KOQmJs38DhiZA0GbJOtfNx\no50bpeP0ylwNa5roARHh3Bmp/N97X1FUWU9SVBdt3vj522pyU79h+7784C741zfsTbnT/8d2rqC6\nZgzUV9ir8apC24do23TVgcMdW1QVHrlfZIp9uGjSeXYcN8a2hR6RZDuKDgjq/PWUUoAm+nbnz0jh\nz+9+xWsbC7lqblb3G/sH2CqX079le7L/8I/w2Hm2udd5P4SRcyAo/NgE7skaqmxd8/1rbJXEvDUd\nilQcobE2mUem2Cvy2Ez7PEP8WHtVrv2KKjUgYoxxdwzk5OSYtWvXujsMFt33EaGBfjz/vbl927Gp\nHj5/FD76E9QUgfjZhJWWDWm0wNQ+AAAY80lEQVQ5togncaJvPyhjjK3Bsn+1k9RX2zbPTatdnzgJ\nMo635yFyxOHEHjlCi1WU6icR+dwYk9PTdj6cefruvBkp3PXmdvaX1ZIRF9b7HQNDYM71kH257bWn\nrb/Nra/AusedbcIgZaZN/qmzbMJLGOe+qnqtrVBVYJNzfcWR64768je23LyuzDale9S43I7bWlEM\nioT04+CUH9t65mk5EBpzLN6VUqoTmuhdnDc9lbve3M6rGwqPrFPfW0Fh9unI8WfZeWPsTcX8dYeT\n/+qHDlfvEz9bNJE40WWYYG8gDrRtfGOgsdqWg7e3veIy7vi4fq+JTdqhcfZBocgU2w5LaBzEj7b3\nK5Im6cNlSnkQTfQuMuLCmDUyhpe/LOhfou9IxClrHgPTv2mXNTfaB3WKtzl1u7fZYfsbYFqc/fwg\nItm2PR4U7jIOPzwfHOHc3Dxkm72tKz88XX/IXqW3Nh8ZT1CEfUS/4+P6oXGd1BrqMB8YZhN7XzvG\nUEq5nSb6Ds6fkcodr2xhZ1EVY5OGoKnigCDbbWHHrgubG21VwqKt9gugIs8WhTQ6Q/WBw9ONNfZq\nHWzVwtCYw+OYUS7zsfYLIy7LJnXt6FmpYUkTfQdfm5bCr17dwitfFvLDhcewTfqAIFvkkTSpd9u3\nlaNr4lZK9aDH5/hFJENEVojIFhHZLCI/cJbHicjbIvKVM451louI/FlEdorIBhHJHuo3MZiSokKY\nkxXPK18W4Ak1krokokleKdUrvWmwpRn4b2PMZGAOcKOITAZuBd41xowD3nXmARYB45zhOuD+QY96\niJ0/M5XdJTVsLqh0dyhKKTVgPSZ6Y0yhMWadM10FbAXSgMXAY85mjwFLnOnFwOPGWgXEiEjKoEc+\nhM6eMoIAP+GVLwvcHYpSSg1Yn5pgFJFMYBbwGZBsjGl7Vv0A0NYzcRqw32W3PGeZ14gND+KU8Ym8\nuqGQ1lYPLr5RSqle6HWiF5EI4DngZmPMEWUaxhZm9ykjish1IrJWRNYWFxf3Zddj4rwZKeQfquOL\n/eXuDkUppQakV4leRAKxSf5JY8zzzuKDbUUyzrjIWZ4PZLjsnu4sO4Ix5kFjTI4xJicxMbG/8Q+Z\nhZNHEBzgx8vrtfhGKeXdelPrRoCHga3GmLtdVr0MXOFMXwG85LL8cqf2zRygwqWIx2tEBAdwxqQk\nXttYSHNLq7vDUUqpfuvNFf1c4DLgdBFZ7wznAHcCC0XkK2CBMw/wOrAb2Ak8BHxv8MM+Ns6bnkpJ\ndSOrdpe5OxSllOq3Hh+YMsas5Kjn4dud0cn2BrhxgHF5hPkTk4gIDuCVLwuYNy7B3eEopVS/aMen\n3QgJ9OfMycm8samQxmYtvlFKeSdN9D04b0YqlfXNfLjD82oGKaVUb2ii78G8cQnEhAXyygatfaOU\n8k6a6HsQ6O/HoqkpvL3lIHWNLe4ORyml+kwTfS+cNyOF2sYW3t120N2hKKVUn2mi74UTsuJJigzW\ntm+UUl5JE30v+PsJX5uewoptxewqrnZ3OEop1Sea6Hvp+lPHEBbsz83L12tVS6WUV9FE30vJUSHc\n+fVpbMyv4J53drg7HKWU6jVN9H1w9tQUvpWTwQMf7GLV7lJ3h6OUUr2iib6PfnHeZEbFhXHL0+up\nqGtydzhKKdUjTfR9FB4cwL1LZ3GwqoGfv7jJs/uVVUopNNH3y8yMGG4+YxyvfFnAi+uPampfKaU8\niib6fvre/LEcnxnLL17czP6yWneHo5RSXdJE30/+fsLdF80E4IdPr9fOSZRSHksT/QBkxIXxqyVT\nWLu3nPvf3+XucJRSqlOa6Adoycw0zp+Ryr3vfsX6/YfcHY5SSh1FE/0AiQi/XjKVEVEh3Lz8C2oa\nmt0dklJKHUET/SCIDg3k7otmsLesll+8tFmrXCqlPIom+kFywuh4vj9/LM+ty+P7y76gvknbrldK\neYYeOwdXvffDheMJCw7g929uY19ZLQ9dnkNyVIi7w1JKDXN6RT+IRITrTx3Dg5flsLOomvP/spKN\neRXuDkspNcxpoh8CCycn89wNJxHg58c3//4Jr20odHdISqlhTBP9EJmUEsVLN81lSmo0Nz61jvve\n+Upv0iql3EIT/RBKiAjmqWtP4OvZadzzzg69SauUcgu9GTvEggP8+dM3ZzAhOZI79SatUsoN9Ir+\nGBARvuvcpN1VVM3iv3zM9gNV7g5LKTVMaKI/hhZOTubZG06i1Ri++cAnrMktc3dISqlhoMdELyKP\niEiRiGxyWXa7iOSLyHpnOMdl3W0islNEtovIWUMVuLealBLFczecREJEMJf+4zPe3nLQ3SEppXxc\nb67oHwXO7mT5PcaYmc7wOoCITAaWAlOcff4mIv6DFayvyIgL49kbTmJiShTffWIty1fvc3dISikf\n1mOiN8Z8CPS2jGExsNwY02CM2QPsBGYPID6fFRcexLJrT+CU8Ync+vxG/vyuVr9USg2NgZTR3yQi\nG5yinVhnWRqw32WbPGfZUUTkOhFZKyJri4uLBxCG9woLCuChy3P4enYad7+9g1+8tJmWVk32SqnB\n1d9Efz8wBpgJFAJ/6usBjDEPGmNyjDE5iYmJ/QzD+wX6+/Gnb87gu6eO5olVe/n+snVa114pNaj6\nVY/eGNN+B1FEHgJedWbzgQyXTdOdZaobIsJtiyaRGBHMb17bSmn1ah66IoeokEB3h6aU8gH9uqIX\nkRSX2QuAtho5LwNLRSRYRLKAccDqgYU4fFxz8mjuWzqTdfvKueiBTymsqHN3SEopH9Cb6pXLgE+B\nCSKSJyJXA3eJyEYR2QDMB34IYIzZDDwDbAHeBG40xmg5RB8snpnGI1ceT155HUv++jGbC7T1S6XU\nwIgn1PTIyckxa9eudXcYHmXbgUq+8881VNQ18ZdvZzN/YpK7Q1JKeRgR+dwYk9PTdvpkrIeaOCKK\nF26cS2ZCOFc/toYnVu11d0hKKS+lid6DJUeF8Mx3T+S0CUn8vxc38dvXttCq1S+VUn2kid7DhQcH\n8OBlx3H5iaN46KM9fO/JddQ16m0PpVTvaaL3AgH+ftxx/hT+37mTeWvLAS5+aBUl1Q3uDksp5SU0\n0XsJEeHqeVncf8lxbDtQyQV/+5gdB7WpY6VUzzTRe5mzp45g+XUnUtfYwln3fsjSBz/lmTX7qaxv\ncndoSikPpdUrvVRRZT3L1+znhS/y2VNSQ3CAHwsmJ3PBzDROnZBIoL9+hyvl63pbvVITvZczxvBl\nXgUvrMvjlQ2FlNU0EhsWyHkzUlkyK41ZGTGIiLvDVEoNAU30w1BTSysf7ijm+S/yeWfLQRqaW5k4\nIpIfnTmBMyYlacJXysdooh/mKuubeH1DIQ98sIvc0lqOGxXLj8+awJzR8e4OTSk1SDTRK8Be5f97\nbR73vbuDg5UNnDI+kZ+cNYGpadHuDk0pNUCa6NUR6ptaePzTXP72/i4O1Tbxtekp3LJwPGMSI9wd\nmlKqnzTRq05V1jfxjw9384+Ve2hobuXC7HRuXjiOlOhQd4emlOojbdRMdSoqJJBbzpzAhz+Zz2Vz\nRvHCF/ksvPtDlq3ep33WKuWjNNEPUwkRwdx+/hTeueVUpqVFc9vzG7nin2soOKSdnSjlazTRD3Mj\n48N48poT+PXiKazNLeOsez7k6TV6da+UL9FEr/DzEy47MZM3f3AKk1Oj+OlzG7nyn2u0K0OlfIQm\netVuZHwYy66dwx3nT2H1njLOvOdDnlm7X6/ulfJyAe4OQHkWPz/hipMyOW1CIj9+dgM/eXYDb2ws\n5PITM2lqaaWxpZXGZmdwphuaW2lqaWVMYgRnTEoiLEg/Vkp5Ev2PVJ0aFR/O8mvn8Ninufz+zW2s\n2F7cq/1CA/1ZMDmZ82ekcsr4BIID/Ic2UKVUjzTRqy75+QlXzc3ia9NSyDtUR5C/H0EBfofHAYfn\n/f2EtbnlvLKhgDc2FvLKlwVEhgRw9pQRnDcjlZPGxBOgLWoq5Rb6wJQadE0trazcWcIrXxbwn80H\nqW5oJj48iHOmpfDtE0YyKSXK3SEq5RP0yVjlEeqbWnh/ezGvbCjg3a0HqW9q5WvTUrh5wTjGJUe6\nOzylvFpvE70W3aghFRLoz9lTR3D21BFU1Dbxj5W7eWTlHl7fVMiSmWn84IxxZCaEuztMpXyaXtGr\nY66sppG/f7CLxz7NpanF8I3sNL5/+jgy4sLcHZpSXkWLbpTHK6qq5/73d/HkZ/ZJ3G8dn8FN88cx\nIjrE3aEp5RU00SuvUVhRx19X7OTpNfsREc6dnsKSmWlaU0epHgxaoheRR4BzgSJjzFRnWRzwNJAJ\n5AIXGWPKxfZVdx9wDlALXGmMWddTEJroFcD+slru/2AXr3xZQFV9MwkRwZw7PYXFM1OZqX3fKnWU\nwUz0pwDVwOMuif4uoMwYc6eI3ArEGmN+KiLnAN/HJvoTgPuMMSf0FIQmeuWqrabOS+vzeXdbEY3N\nrWTGh3H+zDQWz0zVzlKUcgxq0Y2IZAKvuiT67cBpxphCEUkB3jfGTBCRvzvTyzpu193xNdGrrlTW\nN/HmpgO8tD6fT3aVYgxMS4vmpLHxTEuLZmpqNCPjwvDz06t9NfwMdfXKZJfkfQBIdqbTgP0u2+U5\ny45K9CJyHXAdwMiRI/sZhvJ1USGBXJSTwUU5GRRV1vPylwW8trGQR1buoanFXqREBgcwJS2KqanR\nTE2LZmpaFFkJEfhr8lcKGIR69MYYIyJ9vqNrjHkQeBDsFf1A41C+LykqhGtOHs01J4+msbmVHQer\n2JRfwaaCCjblV/LEqr00NLcCEBbkT0ZsGKkxIaTEhJIWE0pKdAipMaGkRocyIjqEoAC90auGh/4m\n+oMikuJSdFPkLM8HMly2S3eWKTWoggL8nKv36PZlzS2t7CyuZlN+JZsLKthfVkdhRR3r9x+ivLbp\niP1FbC9bo+LCGBkfRmZ8OKOccWZ8ONFhgcf6LSk1ZPqb6F8GrgDudMYvuSy/SUSWY2/GVvRUPq/U\nYAnw92PiiCgmjojiwuPSj1hX19hCQUUdhYfqKaioo+CQHfaV1fLprlKeX3fk9UhMWCCj4sPJjA9j\nwohIJqdEMTk1iqRIreOvvE+PiV5ElgGnAQkikgf8EpvgnxGRq4G9wEXO5q9ja9zsxFavvGoIYlaq\nz0KD/BmTGNFljZ36phb2ldWSW1LD3tJacktr2FdWy9rccl5aX9C+XUJEMJNTo5iUYpP/lFS9H6A8\nnz4wpVQPKuqa2FZYyZbCSrYU2PGOg1XtN4ODA/zISghnTGIEoxPDyUoIZ7QzHRWiRUBq6GijZkoN\nkujQQE4YHc8Jo+PblzU2t7KruJotBZVsLaxkd0kNmwsqeHPzAVpaD188JUQEMzohnDFJ4UxKiWJy\nShSTUqIID9Z/PXXs6KdNqX4ICvBjkpO0XTU2t7KvrJbdxdXsLqlhT3ENu0uqeXPTAZattjWPRSAr\nPpzJqVFMSY1mSqot/0+ICHbHW1HDgCZ6pQZRUIAfY5MiGJt05L0AYwwHKuvZnF/J5gJbK2j9/kO8\nuuFwXYWkyGAyE8JtTSCnNtCo+HBGxoURGxaoTUCoftNEr9QxICKkRIeSEh3KgsnJ7csrapvYXFjh\nFAFVsa+shg92FFNU1XDE/pHBAWTEhZEeG0pUaCARwQFEBAcQHhxAREgAEcH+RAQHEh7sT1x4EGMT\nI7RBONVOE71SbhQdFshJYxI4aUzCEcvrGlvYX17L3tJa9pbWsL+slr1ltjZQdX0zVQ3N1DQ009pF\nXYrQQH+mp0cza2Qs2SNjyB4Vq0VDw5gmeqU8UGiQP+OTIxnfTXeLxhjqmlqobmimur6ZmoYWqhqa\nKKpsYP3+Q3yxr5x/fLSbZufbYGRcWHvSn5IaTVx4EFEhAUSFBhKoV/8+TRO9Ul5KRAgLCiAsKICk\nDt8HS2alAfb5gI35FazbW866feV8vKuUF12eC2gTFuRPdGggUSGBRIUGEB0aSHRoEMlRwSRHhTiD\nnU6MDNYvBi+jiV4pHxYS6M/xmXEcnxkH2F8B+Yfq2HGwioq6Jipqm6isb6ayromKuiYq6+244FA9\nm/IrKa5uOKK6KNhaQ/HhwSRHBZMYGUxceBBxYUHEhgcRFx5EbJgdx4UHEhceTFiQP63G0NJqaG2F\nlrZpZ9zSaogKDSQ6tP/PHLS2GrYdqCI82J9R8doHcUea6JUaRkSE9Ngw0mN71z9va6uhtKaRg5X1\nztDAwcp6iqrqOVBRT3F1A18drKasppG6ppYBxTYuKYKczDhyRsVyfGYcGXGhXdY0akvsn+4uZdXu\nUlbvKaOizrZndPrEJK6Zl8WJY+K1ppJDn4xVSg2K+qYWymsbKatppLymibLaRsprGqluaMbfT/AX\nwc9P8Bfw92ubtuOiynrW7i3n873lVNU3A5AYGczxmbEcNyqO4zNjCfDzY5WT2D9zSewj48KYMzqO\nE7LiySuv4/FPcymtaWRyShTXnJzFudNTfbalUu0zVinldVpbDTuKqliTW87nuWWsyS0n/1DdEdu0\nJfY5ztPKaTGhR6yvb2rhxS/y+cfKPewsqiY5KpgrT8ri27NH9tgqaWuroaKuicAAPyK84OllTfRK\nKZ9QWFHH2txymlpamZ0V16dipw++Kubhj/awcmcJYUH+XJSTQfaoWEqqGiipbqC4bVzdQElVIyXV\nDe21lCKDAxgRHcKI6BBSokMYER3qjO18emyY278MNNErpZRjS0ElD6/cw8tf5rc3RhfgJyREBJMQ\nGURCRDCJEcEkRAaTEBFMU0srByrqKayoc8b2fkTHdJkYGUxWfDiZCWFkJoQ707ZPg9Ag//btjDE0\nNLdS09Bsq8O2VYltbCY9NqzbarTd0USvlFIdlFQ3UF7TSEJEMNGhgX3qa7ippZWiqgYOVNRRcKie\n/eW17CmuIbe0hj0ltZRUH/k084ioEAL8hWrn4ba2L5iOvnvqaG5bNKlf70dbr1RKqQ4SIoL7/YRw\noL8faU63lMeNOnp9VX0Te0tr2VNSQ25JDbmltRjMEc1VRIYEEB7U1myFHVKih74zG030Sik1CCJD\nAo/q3tJT+GadI6WUUu000SullI/TRK+UUj5OE71SSvk4TfRKKeXjNNErpZSP00SvlFI+ThO9Ukr5\nOI9oAkFEioG9/dw9ASgZxHCOBY352PC2mL0tXtCYj5WuYh5ljEnsaWePSPQDISJre9PWgyfRmI8N\nb4vZ2+IFjflYGWjMWnSjlFI+ThO9Ukr5OF9I9A+6O4B+0JiPDW+L2dviBY35WBlQzF5fRq+UUqp7\nvnBFr5RSqhua6JVSysd5daIXkbNFZLuI7BSRW90dT2+ISK6IbBSR9SLikf0nisgjIlIkIptclsWJ\nyNsi8pUzjnVnjK66iPd2Ecl3zvN6ETnHnTF2JCIZIrJCRLaIyGYR+YGz3JPPc1cxe+S5FpEQEVkt\nIl868d7hLM8Skc+cvPG0iAS5O9Y23cT8qIjscTnHM/t0YGOMVw6AP7ALGA0EAV8Ck90dVy/izgUS\n3B1HDzGeAmQDm1yW3QXc6kzfCvze3XH2EO/twI/cHVs3MacA2c50JLADmOzh57mrmD3yXAMCRDjT\ngcBnwBzgGWCps/wB4AZ3x9qLmB8FLuzvcb35in42sNMYs9sY0wgsBxa7OSafYIz5ECjrsHgx8Jgz\n/Riw5JgG1Y0u4vVoxphCY8w6Z7oK2Aqk4dnnuauYPZKxqp3ZQGcwwOnAs85yTzvHXcU8IN6c6NOA\n/S7zeXjwh86FAf4jIp+LyHXuDqYPko0xhc70ASDZncH00k0issEp2vGYIpCORCQTmIW9evOK89wh\nZvDQcy0i/iKyHigC3saWAhwyxjQ7m3hc3ugYszGm7Rz/1jnH94hIn3o49+ZE763mGWOygUXAjSJy\nirsD6itjf1d6er3c+4ExwEygEPiTe8PpnIhEAM8BNxtjKl3Xeep57iRmjz3XxpgWY8xMIB1bCjDR\nzSH1qGPMIjIVuA0b+/FAHPDTvhzTmxN9PpDhMp/uLPNoxph8Z1wEvID98HmDgyKSAuCMi9wcT7eM\nMQedf5hW4CE88DyLSCA2YT5pjHneWezR57mzmL3hXBtjDgErgBOBGBEJcFZ5bN5wiflsp9jMGGMa\ngH/Sx3PszYl+DTDOuYMeBCwFXnZzTN0SkXARiWybBs4ENnW/l8d4GbjCmb4CeMmNsfSoLVk6LsDD\nzrOICPAwsNUYc7fLKo89z13F7KnnWkQSRSTGmQ4FFmLvK6wALnQ287Rz3FnM21y+/AV7T6FP59ir\nn4x1qnHdi62B84gx5rduDqlbIjIaexUPEAA85Ykxi8gy4DRs06gHgV8CL2JrK4zENil9kTHGI26A\ndhHvadiiBIOt6fRdl7JvtxORecBHwEag1Vn8M2yZt6ee565ivhgPPNciMh17s9Ufe1H7jDHmV87/\n4XJsEcgXwKXOlbLbdRPze0AitlbOeuB6l5u2PR/XmxO9Ukqpnnlz0Y1SSqle0ESvlFI+ThO9Ukr5\nOE30Sinl4zTRK6WUj9NEr5RSPk4TvVJK+bj/D9KCQx6R/S7RAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODfcbzQ8BvG",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS8rQKQ98BvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import LSTM\n",
        "# input_size, hidden_size, num_layers. \n",
        "# Optional: bias = False, dropout = 0 (probability of dropout)\n",
        "# bidirectional = False. \n",
        "rnn = nn.LSTM(10, 20, 2)\n",
        "\n",
        "# input: tensor containing feature of the input sequence \n",
        "# shape: seq_len, batch, input_size\n",
        "input = torch.randn(5,3,10)\n",
        "\n",
        "#h0: tensor contain hidden state for t = seq_len\n",
        "# shape: num_layers * num*directions, batch, hidden_size\n",
        "h0 = torch.randn(2,3,20)\n",
        "\n",
        "#c0: tensor contain cell state for t = seq_length\n",
        "# shape: num_layers * num_directions, batch, hidden_size \n",
        "c0 = torch.randn(2,3,20)\n",
        "\n",
        "# output: shape (seq_len, batch, num_direction * hidden size)\n",
        "output, (hn,cn) = rnn(input, (h0,c0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUGwapsC8BvL",
        "colab_type": "code",
        "outputId": "eaf4e88c-6468-4c7d-e6b0-043d0675227e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example embedding layer\n",
        "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
        "lookup.weight.size()\n",
        "# train_dict = vocab size + 3"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33178, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoKNOBmB8BvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the baseline, we will stop the epoch around 20 \n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], \n",
        "                            dropout=options['lstm_dropout'], batch_first=True, bias = options['bias'],\n",
        "                           bidirectional = options['bid'])\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_outputs = self.lstm(embeddings)\n",
        "        # project of outputs \n",
        "        # rnn_outputs: tupple with second element being last hidden state. \n",
        "        logits = self.projection(lstm_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cnF0Xq98HPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = LSTMModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 64\n",
        "    hidden_size = 128 # output of dimension \n",
        "    num_layers = 2\n",
        "    lstm_dropout = 0.1\n",
        "#     input_size = lookup.weight.size(1)\n",
        "    vocab_size = len(train_dict)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "\n",
        "    \n",
        "    model = LSTMModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4LKhWXs9MeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "0431b77a-4ae9-40c9-ada7-2735d82ec9eb"
      },
      "source": [
        "model"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 64, padding_idx=2)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp6kNjF_9OD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4017a58-b2ed-46f9-cd05-5309c8b39ffe"
      },
      "source": [
        "#model = LSTMModel(options).to(current_device)\n",
        "plot_cache = []\n",
        "min_val_loss = 20 \n",
        "for epoch_number in range(20):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "        \n",
        "print('Saving best model...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './baseline_LSTM.pt')\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 7.0863\n",
            "Step 100 avg train loss = 7.0200\n",
            "Step 200 avg train loss = 6.9266\n",
            "Step 300 avg train loss = 6.8133\n",
            "Step 400 avg train loss = 6.7200\n",
            "Step 500 avg train loss = 6.6318\n",
            "Step 600 avg train loss = 6.5733\n",
            "Step 700 avg train loss = 6.5168\n",
            "Step 800 avg train loss = 6.4810\n",
            "Step 900 avg train loss = 6.4369\n",
            "Step 1000 avg train loss = 6.3943\n",
            "Step 1100 avg train loss = 6.3615\n",
            "Step 1200 avg train loss = 6.3266\n",
            "Step 1300 avg train loss = 6.2870\n",
            "Step 1400 avg train loss = 6.2620\n",
            "Step 1500 avg train loss = 6.2168\n",
            "Step 1600 avg train loss = 6.2051\n",
            "Step 1700 avg train loss = 6.1710\n",
            "Step 1800 avg train loss = 6.1293\n",
            "Step 1900 avg train loss = 6.1156\n",
            "Step 2000 avg train loss = 6.0862\n",
            "Step 2100 avg train loss = 6.0944\n",
            "Step 2200 avg train loss = 6.0612\n",
            "Step 2300 avg train loss = 6.0266\n",
            "Step 2400 avg train loss = 6.0032\n",
            "Validation loss after 0 epoch = 5.8233\n",
            "Step 0 avg train loss = 5.8466\n",
            "Step 100 avg train loss = 5.9111\n",
            "Step 200 avg train loss = 5.8860\n",
            "Step 300 avg train loss = 5.8898\n",
            "Step 400 avg train loss = 5.8895\n",
            "Step 500 avg train loss = 5.8599\n",
            "Step 600 avg train loss = 5.8661\n",
            "Step 700 avg train loss = 5.8486\n",
            "Step 800 avg train loss = 5.8073\n",
            "Step 900 avg train loss = 5.8136\n",
            "Step 1000 avg train loss = 5.8237\n",
            "Step 1100 avg train loss = 5.7940\n",
            "Step 1200 avg train loss = 5.7931\n",
            "Step 1300 avg train loss = 5.7645\n",
            "Step 1400 avg train loss = 5.7615\n",
            "Step 1500 avg train loss = 5.7428\n",
            "Step 1600 avg train loss = 5.7537\n",
            "Step 1700 avg train loss = 5.7298\n",
            "Step 1800 avg train loss = 5.6969\n",
            "Step 1900 avg train loss = 5.7121\n",
            "Step 2000 avg train loss = 5.6991\n",
            "Step 2100 avg train loss = 5.6891\n",
            "Step 2200 avg train loss = 5.6726\n",
            "Step 2300 avg train loss = 5.6940\n",
            "Step 2400 avg train loss = 5.6580\n",
            "Validation loss after 1 epoch = 5.5607\n",
            "Step 0 avg train loss = 5.5434\n",
            "Step 100 avg train loss = 5.5452\n",
            "Step 200 avg train loss = 5.5621\n",
            "Step 300 avg train loss = 5.5332\n",
            "Step 400 avg train loss = 5.5507\n",
            "Step 500 avg train loss = 5.5510\n",
            "Step 600 avg train loss = 5.5591\n",
            "Step 700 avg train loss = 5.5285\n",
            "Step 800 avg train loss = 5.5261\n",
            "Step 900 avg train loss = 5.5465\n",
            "Step 1000 avg train loss = 5.5328\n",
            "Step 1100 avg train loss = 5.5330\n",
            "Step 1200 avg train loss = 5.5034\n",
            "Step 1300 avg train loss = 5.5002\n",
            "Step 1400 avg train loss = 5.5018\n",
            "Step 1500 avg train loss = 5.4922\n",
            "Step 1600 avg train loss = 5.4849\n",
            "Step 1700 avg train loss = 5.4831\n",
            "Step 1800 avg train loss = 5.4891\n",
            "Step 1900 avg train loss = 5.4797\n",
            "Step 2000 avg train loss = 5.4614\n",
            "Step 2100 avg train loss = 5.4622\n",
            "Step 2200 avg train loss = 5.4617\n",
            "Step 2300 avg train loss = 5.4520\n",
            "Step 2400 avg train loss = 5.4403\n",
            "Validation loss after 2 epoch = 5.4297\n",
            "Step 0 avg train loss = 5.1113\n",
            "Step 100 avg train loss = 5.3614\n",
            "Step 200 avg train loss = 5.3298\n",
            "Step 300 avg train loss = 5.3446\n",
            "Step 400 avg train loss = 5.3246\n",
            "Step 500 avg train loss = 5.3085\n",
            "Step 600 avg train loss = 5.3320\n",
            "Step 700 avg train loss = 5.3338\n",
            "Step 800 avg train loss = 5.3105\n",
            "Step 900 avg train loss = 5.3265\n",
            "Step 1000 avg train loss = 5.3130\n",
            "Step 1100 avg train loss = 5.3274\n",
            "Step 1200 avg train loss = 5.3137\n",
            "Step 1300 avg train loss = 5.3129\n",
            "Step 1400 avg train loss = 5.3313\n",
            "Step 1500 avg train loss = 5.3077\n",
            "Step 1600 avg train loss = 5.2862\n",
            "Step 1700 avg train loss = 5.3240\n",
            "Step 1800 avg train loss = 5.2860\n",
            "Step 1900 avg train loss = 5.2976\n",
            "Step 2000 avg train loss = 5.2729\n",
            "Step 2100 avg train loss = 5.2752\n",
            "Step 2200 avg train loss = 5.2920\n",
            "Step 2300 avg train loss = 5.2710\n",
            "Step 2400 avg train loss = 5.2845\n",
            "Validation loss after 3 epoch = 5.3562\n",
            "Step 0 avg train loss = 5.1434\n",
            "Step 100 avg train loss = 5.1718\n",
            "Step 200 avg train loss = 5.1707\n",
            "Step 300 avg train loss = 5.1684\n",
            "Step 400 avg train loss = 5.1573\n",
            "Step 500 avg train loss = 5.1514\n",
            "Step 600 avg train loss = 5.1843\n",
            "Step 700 avg train loss = 5.1482\n",
            "Step 800 avg train loss = 5.1617\n",
            "Step 900 avg train loss = 5.1539\n",
            "Step 1000 avg train loss = 5.1785\n",
            "Step 1100 avg train loss = 5.1625\n",
            "Step 1200 avg train loss = 5.1502\n",
            "Step 1300 avg train loss = 5.1451\n",
            "Step 1400 avg train loss = 5.1660\n",
            "Step 1500 avg train loss = 5.1323\n",
            "Step 1600 avg train loss = 5.1497\n",
            "Step 1700 avg train loss = 5.1697\n",
            "Step 1800 avg train loss = 5.1509\n",
            "Step 1900 avg train loss = 5.1492\n",
            "Step 2000 avg train loss = 5.1416\n",
            "Step 2100 avg train loss = 5.1433\n",
            "Step 2200 avg train loss = 5.1633\n",
            "Step 2300 avg train loss = 5.1416\n",
            "Step 2400 avg train loss = 5.1608\n",
            "Validation loss after 4 epoch = 5.3118\n",
            "Step 0 avg train loss = 4.9447\n",
            "Step 100 avg train loss = 5.0148\n",
            "Step 200 avg train loss = 5.0230\n",
            "Step 300 avg train loss = 5.0276\n",
            "Step 400 avg train loss = 5.0230\n",
            "Step 500 avg train loss = 5.0330\n",
            "Step 600 avg train loss = 5.0120\n",
            "Step 700 avg train loss = 5.0337\n",
            "Step 800 avg train loss = 5.0339\n",
            "Step 900 avg train loss = 5.0303\n",
            "Step 1000 avg train loss = 5.0622\n",
            "Step 1100 avg train loss = 5.0296\n",
            "Step 1200 avg train loss = 5.0335\n",
            "Step 1300 avg train loss = 5.0202\n",
            "Step 1400 avg train loss = 5.0432\n",
            "Step 1500 avg train loss = 5.0423\n",
            "Step 1600 avg train loss = 5.0470\n",
            "Step 1700 avg train loss = 5.0465\n",
            "Step 1800 avg train loss = 5.0368\n",
            "Step 1900 avg train loss = 5.0461\n",
            "Step 2000 avg train loss = 5.0525\n",
            "Step 2100 avg train loss = 5.0449\n",
            "Step 2200 avg train loss = 5.0178\n",
            "Step 2300 avg train loss = 5.0546\n",
            "Step 2400 avg train loss = 5.0173\n",
            "Validation loss after 5 epoch = 5.2922\n",
            "Step 0 avg train loss = 4.7017\n",
            "Step 100 avg train loss = 4.9204\n",
            "Step 200 avg train loss = 4.9169\n",
            "Step 300 avg train loss = 4.9215\n",
            "Step 400 avg train loss = 4.9211\n",
            "Step 500 avg train loss = 4.8891\n",
            "Step 600 avg train loss = 4.9412\n",
            "Step 700 avg train loss = 4.9295\n",
            "Step 800 avg train loss = 4.9425\n",
            "Step 900 avg train loss = 4.9328\n",
            "Step 1000 avg train loss = 4.9519\n",
            "Step 1100 avg train loss = 4.9247\n",
            "Step 1200 avg train loss = 4.9585\n",
            "Step 1300 avg train loss = 4.9448\n",
            "Step 1400 avg train loss = 4.9278\n",
            "Step 1500 avg train loss = 4.9288\n",
            "Step 1600 avg train loss = 4.9582\n",
            "Step 1700 avg train loss = 4.9372\n",
            "Step 1800 avg train loss = 4.9196\n",
            "Step 1900 avg train loss = 4.9321\n",
            "Step 2000 avg train loss = 4.9250\n",
            "Step 2100 avg train loss = 4.9101\n",
            "Step 2200 avg train loss = 4.9163\n",
            "Step 2300 avg train loss = 4.9302\n",
            "Step 2400 avg train loss = 4.9549\n",
            "Validation loss after 6 epoch = 5.2841\n",
            "Step 0 avg train loss = 4.8695\n",
            "Step 100 avg train loss = 4.8045\n",
            "Step 200 avg train loss = 4.8294\n",
            "Step 300 avg train loss = 4.8278\n",
            "Step 400 avg train loss = 4.8329\n",
            "Step 500 avg train loss = 4.8239\n",
            "Step 600 avg train loss = 4.8349\n",
            "Step 700 avg train loss = 4.8690\n",
            "Step 800 avg train loss = 4.8324\n",
            "Step 900 avg train loss = 4.8435\n",
            "Step 1000 avg train loss = 4.8450\n",
            "Step 1100 avg train loss = 4.8368\n",
            "Step 1200 avg train loss = 4.8505\n",
            "Step 1300 avg train loss = 4.8468\n",
            "Step 1400 avg train loss = 4.8510\n",
            "Step 1500 avg train loss = 4.8339\n",
            "Step 1600 avg train loss = 4.8627\n",
            "Step 1700 avg train loss = 4.8453\n",
            "Step 1800 avg train loss = 4.8560\n",
            "Step 1900 avg train loss = 4.8612\n",
            "Step 2000 avg train loss = 4.8578\n",
            "Step 2100 avg train loss = 4.8430\n",
            "Step 2200 avg train loss = 4.8702\n",
            "Step 2300 avg train loss = 4.8353\n",
            "Step 2400 avg train loss = 4.8638\n",
            "Validation loss after 7 epoch = 5.2809\n",
            "Step 0 avg train loss = 4.7470\n",
            "Step 100 avg train loss = 4.7459\n",
            "Step 200 avg train loss = 4.7354\n",
            "Step 300 avg train loss = 4.7444\n",
            "Step 400 avg train loss = 4.7343\n",
            "Step 500 avg train loss = 4.7408\n",
            "Step 600 avg train loss = 4.7603\n",
            "Step 700 avg train loss = 4.7504\n",
            "Step 800 avg train loss = 4.7704\n",
            "Step 900 avg train loss = 4.7664\n",
            "Step 1000 avg train loss = 4.7505\n",
            "Step 1100 avg train loss = 4.7623\n",
            "Step 1200 avg train loss = 4.7743\n",
            "Step 1300 avg train loss = 4.7701\n",
            "Step 1400 avg train loss = 4.7791\n",
            "Step 1500 avg train loss = 4.7600\n",
            "Step 1600 avg train loss = 4.7797\n",
            "Step 1700 avg train loss = 4.7896\n",
            "Step 1800 avg train loss = 4.7708\n",
            "Step 1900 avg train loss = 4.7825\n",
            "Step 2000 avg train loss = 4.7981\n",
            "Step 2100 avg train loss = 4.7845\n",
            "Step 2200 avg train loss = 4.8000\n",
            "Step 2300 avg train loss = 4.7924\n",
            "Step 2400 avg train loss = 4.7921\n",
            "Validation loss after 8 epoch = 5.2987\n",
            "Step 0 avg train loss = 4.6857\n",
            "Step 100 avg train loss = 4.6830\n",
            "Step 200 avg train loss = 4.6996\n",
            "Step 300 avg train loss = 4.6605\n",
            "Step 400 avg train loss = 4.6979\n",
            "Step 500 avg train loss = 4.6663\n",
            "Step 600 avg train loss = 4.6855\n",
            "Step 700 avg train loss = 4.6916\n",
            "Step 800 avg train loss = 4.6988\n",
            "Step 900 avg train loss = 4.7003\n",
            "Step 1000 avg train loss = 4.6932\n",
            "Step 1100 avg train loss = 4.6996\n",
            "Step 1200 avg train loss = 4.7004\n",
            "Step 1300 avg train loss = 4.7015\n",
            "Step 1400 avg train loss = 4.7098\n",
            "Step 1500 avg train loss = 4.7027\n",
            "Step 1600 avg train loss = 4.7106\n",
            "Step 1700 avg train loss = 4.7222\n",
            "Step 1800 avg train loss = 4.7225\n",
            "Step 1900 avg train loss = 4.7178\n",
            "Step 2000 avg train loss = 4.7214\n",
            "Step 2100 avg train loss = 4.7269\n",
            "Step 2200 avg train loss = 4.7266\n",
            "Step 2300 avg train loss = 4.7068\n",
            "Step 2400 avg train loss = 4.7275\n",
            "Validation loss after 9 epoch = 5.3080\n",
            "Step 0 avg train loss = 4.5204\n",
            "Step 100 avg train loss = 4.5975\n",
            "Step 200 avg train loss = 4.5952\n",
            "Step 300 avg train loss = 4.6278\n",
            "Step 400 avg train loss = 4.6466\n",
            "Step 500 avg train loss = 4.6292\n",
            "Step 600 avg train loss = 4.6288\n",
            "Step 700 avg train loss = 4.6286\n",
            "Step 800 avg train loss = 4.6309\n",
            "Step 900 avg train loss = 4.6270\n",
            "Step 1000 avg train loss = 4.6611\n",
            "Step 1100 avg train loss = 4.6526\n",
            "Step 1200 avg train loss = 4.6538\n",
            "Step 1300 avg train loss = 4.6346\n",
            "Step 1400 avg train loss = 4.6592\n",
            "Step 1500 avg train loss = 4.6529\n",
            "Step 1600 avg train loss = 4.6495\n",
            "Step 1700 avg train loss = 4.6491\n",
            "Step 1800 avg train loss = 4.6481\n",
            "Step 1900 avg train loss = 4.6594\n",
            "Step 2000 avg train loss = 4.6472\n",
            "Step 2100 avg train loss = 4.6552\n",
            "Step 2200 avg train loss = 4.6634\n",
            "Step 2300 avg train loss = 4.6580\n",
            "Step 2400 avg train loss = 4.6653\n",
            "Validation loss after 10 epoch = 5.3153\n",
            "Step 0 avg train loss = 4.6411\n",
            "Step 100 avg train loss = 4.5584\n",
            "Step 200 avg train loss = 4.5436\n",
            "Step 300 avg train loss = 4.5719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-8114e8b6de41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfunetAO9QlT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f9899005-ace4-4685-dc05-71233a219567"
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of LSTM model')\n",
        "plt.show()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0CullJf7f9MERIOsxlWJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H658b0MW9UJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ce53cfc-cded-49fa-a571-9e2972ed3b3f"
      },
      "source": [
        "perp = [2**(i[1]/np.log(2)) for i in plot_cache] \n",
        "print('The minimum validation loss occurred at {} and it is {}'.format(perp.index(min(perp)),min(perp)))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum validation loss occurred at 7 and it is 196.54008318910067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAM0w4gp8BvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model with hyperparamters chosen: embed_dim: 300, hidden_zise = 300\n",
        "\n",
        "\n",
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = LSTMModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 300\n",
        "    hidden_size = 300 # output of dimension \n",
        "    num_layers = 2\n",
        "    lstm_dropout = 0.1\n",
        "#     input_size = lookup.weight.size(1)\n",
        "    vocab_size = len(train_dict)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "\n",
        "    \n",
        "    model = LSTMModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mztEAFSg8BvU",
        "colab_type": "code",
        "outputId": "a64e15ed-0573-4518-90b4-ee10699dddff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmVR8WOW8BvY",
        "colab_type": "code",
        "outputId": "4cb4889e-c6ed-408f-b581-1466bf9dca08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model = LSTMModel(options).to(current_device)\n",
        "plot_cache = []\n",
        "min_val_loss = 20 \n",
        "epoch_num = 20\n",
        "for epoch_number in range(epoch_num):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "            print(\"update the best to:\")\n",
        "            print(best_model)\n",
        "            print(\"current validation loss is:\")\n",
        "            print(min_val_loss)\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "        \n",
        "print('Saving best model...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './best_300_300_LSTM.pt')\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 7.0963\n",
            "Step 100 avg train loss = 7.1643\n",
            "Step 200 avg train loss = 6.9912\n",
            "Step 300 avg train loss = 6.8097\n",
            "Step 400 avg train loss = 6.6316\n",
            "Step 500 avg train loss = 6.5043\n",
            "Step 600 avg train loss = 6.4055\n",
            "Step 700 avg train loss = 6.3217\n",
            "Step 800 avg train loss = 6.2781\n",
            "Step 900 avg train loss = 6.1911\n",
            "Step 1000 avg train loss = 6.1272\n",
            "Step 1100 avg train loss = 6.0845\n",
            "Step 1200 avg train loss = 6.0255\n",
            "Step 1300 avg train loss = 6.0025\n",
            "Step 1400 avg train loss = 5.9498\n",
            "Step 1500 avg train loss = 5.9080\n",
            "Step 1600 avg train loss = 5.8947\n",
            "Step 1700 avg train loss = 5.8399\n",
            "Step 1800 avg train loss = 5.7976\n",
            "Step 1900 avg train loss = 5.8105\n",
            "Step 2000 avg train loss = 5.7670\n",
            "Step 2100 avg train loss = 5.7527\n",
            "Step 2200 avg train loss = 5.7363\n",
            "Step 2300 avg train loss = 5.6854\n",
            "Step 2400 avg train loss = 5.6774\n",
            "Validation loss after 0 epoch = 5.5204\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.520387977024295\n",
            "Step 0 avg train loss = 5.6179\n",
            "Step 100 avg train loss = 5.5519\n",
            "Step 200 avg train loss = 5.4950\n",
            "Step 300 avg train loss = 5.5017\n",
            "Step 400 avg train loss = 5.4774\n",
            "Step 500 avg train loss = 5.4747\n",
            "Step 600 avg train loss = 5.4848\n",
            "Step 700 avg train loss = 5.4545\n",
            "Step 800 avg train loss = 5.4363\n",
            "Step 900 avg train loss = 5.4233\n",
            "Step 1000 avg train loss = 5.4134\n",
            "Step 1100 avg train loss = 5.3924\n",
            "Step 1200 avg train loss = 5.3886\n",
            "Step 1300 avg train loss = 5.3621\n",
            "Step 1400 avg train loss = 5.3610\n",
            "Step 1500 avg train loss = 5.3277\n",
            "Step 1600 avg train loss = 5.3297\n",
            "Step 1700 avg train loss = 5.3323\n",
            "Step 1800 avg train loss = 5.3010\n",
            "Step 1900 avg train loss = 5.2779\n",
            "Step 2000 avg train loss = 5.3112\n",
            "Step 2100 avg train loss = 5.2738\n",
            "Step 2200 avg train loss = 5.2660\n",
            "Step 2300 avg train loss = 5.2601\n",
            "Step 2400 avg train loss = 5.2794\n",
            "Validation loss after 1 epoch = 5.2590\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.258957803474282\n",
            "Step 0 avg train loss = 5.0729\n",
            "Step 100 avg train loss = 5.0421\n",
            "Step 200 avg train loss = 5.0402\n",
            "Step 300 avg train loss = 5.0608\n",
            "Step 400 avg train loss = 5.0638\n",
            "Step 500 avg train loss = 5.0504\n",
            "Step 600 avg train loss = 5.0652\n",
            "Step 700 avg train loss = 5.0654\n",
            "Step 800 avg train loss = 5.0178\n",
            "Step 900 avg train loss = 5.0349\n",
            "Step 1000 avg train loss = 5.0329\n",
            "Step 1100 avg train loss = 5.0048\n",
            "Step 1200 avg train loss = 5.0134\n",
            "Step 1300 avg train loss = 4.9993\n",
            "Step 1400 avg train loss = 4.9877\n",
            "Step 1500 avg train loss = 4.9917\n",
            "Step 1600 avg train loss = 4.9959\n",
            "Step 1700 avg train loss = 4.9841\n",
            "Step 1800 avg train loss = 4.9921\n",
            "Step 1900 avg train loss = 4.9914\n",
            "Step 2000 avg train loss = 4.9707\n",
            "Step 2100 avg train loss = 4.9773\n",
            "Step 2200 avg train loss = 4.9577\n",
            "Step 2300 avg train loss = 4.9680\n",
            "Step 2400 avg train loss = 4.9633\n",
            "Validation loss after 2 epoch = 5.1532\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.153161520328162\n",
            "Step 0 avg train loss = 4.8499\n",
            "Step 100 avg train loss = 4.7384\n",
            "Step 200 avg train loss = 4.7116\n",
            "Step 300 avg train loss = 4.7450\n",
            "Step 400 avg train loss = 4.7599\n",
            "Step 500 avg train loss = 4.7415\n",
            "Step 600 avg train loss = 4.7406\n",
            "Step 700 avg train loss = 4.7455\n",
            "Step 800 avg train loss = 4.7482\n",
            "Step 900 avg train loss = 4.7498\n",
            "Step 1000 avg train loss = 4.7576\n",
            "Step 1100 avg train loss = 4.7450\n",
            "Step 1200 avg train loss = 4.7322\n",
            "Step 1300 avg train loss = 4.7445\n",
            "Step 1400 avg train loss = 4.7419\n",
            "Step 1500 avg train loss = 4.7139\n",
            "Step 1600 avg train loss = 4.7399\n",
            "Step 1700 avg train loss = 4.7451\n",
            "Step 1800 avg train loss = 4.7524\n",
            "Step 1900 avg train loss = 4.7283\n",
            "Step 2000 avg train loss = 4.7272\n",
            "Step 2100 avg train loss = 4.7242\n",
            "Step 2200 avg train loss = 4.7086\n",
            "Step 2300 avg train loss = 4.7202\n",
            "Step 2400 avg train loss = 4.7051\n",
            "Validation loss after 3 epoch = 5.1240\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.1240091377834105\n",
            "Step 0 avg train loss = 4.2403\n",
            "Step 100 avg train loss = 4.4755\n",
            "Step 200 avg train loss = 4.5189\n",
            "Step 300 avg train loss = 4.4997\n",
            "Step 400 avg train loss = 4.5024\n",
            "Step 500 avg train loss = 4.5085\n",
            "Step 600 avg train loss = 4.4997\n",
            "Step 700 avg train loss = 4.5424\n",
            "Step 800 avg train loss = 4.5209\n",
            "Step 900 avg train loss = 4.5155\n",
            "Step 1000 avg train loss = 4.5030\n",
            "Step 1100 avg train loss = 4.5263\n",
            "Step 1200 avg train loss = 4.5174\n",
            "Step 1300 avg train loss = 4.5216\n",
            "Step 1400 avg train loss = 4.5076\n",
            "Step 1500 avg train loss = 4.5216\n",
            "Step 1600 avg train loss = 4.5220\n",
            "Step 1700 avg train loss = 4.5271\n",
            "Step 1800 avg train loss = 4.5288\n",
            "Step 1900 avg train loss = 4.5162\n",
            "Step 2000 avg train loss = 4.5171\n",
            "Step 2100 avg train loss = 4.5284\n",
            "Step 2200 avg train loss = 4.5220\n",
            "Step 2300 avg train loss = 4.5491\n",
            "Step 2400 avg train loss = 4.5348\n",
            "Validation loss after 4 epoch = 5.1514\n",
            "Step 0 avg train loss = 4.2330\n",
            "Step 100 avg train loss = 4.2637\n",
            "Step 200 avg train loss = 4.2819\n",
            "Step 300 avg train loss = 4.3145\n",
            "Step 400 avg train loss = 4.2894\n",
            "Step 500 avg train loss = 4.3025\n",
            "Step 600 avg train loss = 4.3162\n",
            "Step 700 avg train loss = 4.3113\n",
            "Step 800 avg train loss = 4.3035\n",
            "Step 900 avg train loss = 4.3288\n",
            "Step 1000 avg train loss = 4.3191\n",
            "Step 1100 avg train loss = 4.3400\n",
            "Step 1200 avg train loss = 4.3778\n",
            "Step 1300 avg train loss = 4.3485\n",
            "Step 1400 avg train loss = 4.3576\n",
            "Step 1500 avg train loss = 4.3466\n",
            "Step 1600 avg train loss = 4.3487\n",
            "Step 1700 avg train loss = 4.3467\n",
            "Step 1800 avg train loss = 4.3449\n",
            "Step 1900 avg train loss = 4.3766\n",
            "Step 2000 avg train loss = 4.3859\n",
            "Step 2100 avg train loss = 4.3532\n",
            "Step 2200 avg train loss = 4.3767\n",
            "Step 2300 avg train loss = 4.3666\n",
            "Step 2400 avg train loss = 4.3729\n",
            "Validation loss after 5 epoch = 5.1989\n",
            "Step 0 avg train loss = 4.0421\n",
            "Step 100 avg train loss = 4.1299\n",
            "Step 200 avg train loss = 4.1321\n",
            "Step 300 avg train loss = 4.1553\n",
            "Step 400 avg train loss = 4.1473\n",
            "Step 500 avg train loss = 4.1434\n",
            "Step 600 avg train loss = 4.1833\n",
            "Step 700 avg train loss = 4.1568\n",
            "Step 800 avg train loss = 4.1612\n",
            "Step 900 avg train loss = 4.1660\n",
            "Step 1000 avg train loss = 4.1943\n",
            "Step 1100 avg train loss = 4.1805\n",
            "Step 1200 avg train loss = 4.1957\n",
            "Step 1300 avg train loss = 4.2071\n",
            "Step 1400 avg train loss = 4.1799\n",
            "Step 1500 avg train loss = 4.2031\n",
            "Step 1600 avg train loss = 4.1892\n",
            "Step 1700 avg train loss = 4.2055\n",
            "Step 1800 avg train loss = 4.1970\n",
            "Step 1900 avg train loss = 4.1906\n",
            "Step 2000 avg train loss = 4.2205\n",
            "Step 2100 avg train loss = 4.2090\n",
            "Step 2200 avg train loss = 4.2018\n",
            "Step 2300 avg train loss = 4.2241\n",
            "Step 2400 avg train loss = 4.2278\n",
            "Validation loss after 6 epoch = 5.2634\n",
            "Step 0 avg train loss = 3.9514\n",
            "Step 100 avg train loss = 3.9639\n",
            "Step 200 avg train loss = 3.9919\n",
            "Step 300 avg train loss = 3.9997\n",
            "Step 400 avg train loss = 3.9899\n",
            "Step 500 avg train loss = 4.0071\n",
            "Step 600 avg train loss = 4.0111\n",
            "Step 700 avg train loss = 4.0287\n",
            "Step 800 avg train loss = 4.0385\n",
            "Step 900 avg train loss = 4.0413\n",
            "Step 1000 avg train loss = 4.0285\n",
            "Step 1100 avg train loss = 4.0451\n",
            "Step 1200 avg train loss = 4.0490\n",
            "Step 1300 avg train loss = 4.0460\n",
            "Step 1400 avg train loss = 4.0713\n",
            "Step 1500 avg train loss = 4.0879\n",
            "Step 1600 avg train loss = 4.0937\n",
            "Step 1700 avg train loss = 4.0824\n",
            "Step 1800 avg train loss = 4.0584\n",
            "Step 1900 avg train loss = 4.0970\n",
            "Step 2000 avg train loss = 4.0975\n",
            "Step 2100 avg train loss = 4.1142\n",
            "Step 2200 avg train loss = 4.1017\n",
            "Step 2300 avg train loss = 4.1026\n",
            "Step 2400 avg train loss = 4.0853\n",
            "Validation loss after 7 epoch = 5.3301\n",
            "Step 0 avg train loss = 3.9261\n",
            "Step 100 avg train loss = 3.8440\n",
            "Step 200 avg train loss = 3.8684\n",
            "Step 300 avg train loss = 3.8729\n",
            "Step 400 avg train loss = 3.8734\n",
            "Step 500 avg train loss = 3.9069\n",
            "Step 600 avg train loss = 3.9219\n",
            "Step 700 avg train loss = 3.8951\n",
            "Step 800 avg train loss = 3.8965\n",
            "Step 900 avg train loss = 3.9264\n",
            "Step 1000 avg train loss = 3.9477\n",
            "Step 1100 avg train loss = 3.9400\n",
            "Step 1200 avg train loss = 3.9614\n",
            "Step 1300 avg train loss = 3.9445\n",
            "Step 1400 avg train loss = 3.9456\n",
            "Step 1500 avg train loss = 3.9390\n",
            "Step 1600 avg train loss = 3.9407\n",
            "Step 1700 avg train loss = 3.9592\n",
            "Step 1800 avg train loss = 3.9591\n",
            "Step 1900 avg train loss = 3.9809\n",
            "Step 2000 avg train loss = 3.9808\n",
            "Step 2100 avg train loss = 3.9648\n",
            "Step 2200 avg train loss = 3.9733\n",
            "Step 2300 avg train loss = 3.9929\n",
            "Step 2400 avg train loss = 4.0095\n",
            "Validation loss after 8 epoch = 5.3986\n",
            "Step 0 avg train loss = 3.8569\n",
            "Step 100 avg train loss = 3.7448\n",
            "Step 200 avg train loss = 3.7514\n",
            "Step 300 avg train loss = 3.7689\n",
            "Step 400 avg train loss = 3.7628\n",
            "Step 500 avg train loss = 3.7833\n",
            "Step 600 avg train loss = 3.7902\n",
            "Step 700 avg train loss = 3.7783\n",
            "Step 800 avg train loss = 3.8215\n",
            "Step 900 avg train loss = 3.8231\n",
            "Step 1000 avg train loss = 3.8275\n",
            "Step 1100 avg train loss = 3.8398\n",
            "Step 1200 avg train loss = 3.8314\n",
            "Step 1300 avg train loss = 3.8419\n",
            "Step 1400 avg train loss = 3.8455\n",
            "Step 1500 avg train loss = 3.8639\n",
            "Step 1600 avg train loss = 3.8601\n",
            "Step 1700 avg train loss = 3.8649\n",
            "Step 1800 avg train loss = 3.8622\n",
            "Step 1900 avg train loss = 3.8639\n",
            "Step 2000 avg train loss = 3.8649\n",
            "Step 2100 avg train loss = 3.8894\n",
            "Step 2200 avg train loss = 3.9049\n",
            "Step 2300 avg train loss = 3.8968\n",
            "Step 2400 avg train loss = 3.8814\n",
            "Validation loss after 9 epoch = 5.4724\n",
            "Step 0 avg train loss = 3.6322\n",
            "Step 100 avg train loss = 3.6339\n",
            "Step 200 avg train loss = 3.6528\n",
            "Step 300 avg train loss = 3.6707\n",
            "Step 400 avg train loss = 3.6762\n",
            "Step 500 avg train loss = 3.6733\n",
            "Step 600 avg train loss = 3.6885\n",
            "Step 700 avg train loss = 3.7217\n",
            "Step 800 avg train loss = 3.7021\n",
            "Step 900 avg train loss = 3.7318\n",
            "Step 1000 avg train loss = 3.7315\n",
            "Step 1100 avg train loss = 3.7547\n",
            "Step 1200 avg train loss = 3.7369\n",
            "Step 1300 avg train loss = 3.7508\n",
            "Step 1400 avg train loss = 3.7451\n",
            "Step 1500 avg train loss = 3.7455\n",
            "Step 1600 avg train loss = 3.7796\n",
            "Step 1700 avg train loss = 3.7875\n",
            "Step 1800 avg train loss = 3.7810\n",
            "Step 1900 avg train loss = 3.7766\n",
            "Step 2000 avg train loss = 3.7877\n",
            "Step 2100 avg train loss = 3.7848\n",
            "Step 2200 avg train loss = 3.7925\n",
            "Step 2300 avg train loss = 3.8148\n",
            "Step 2400 avg train loss = 3.8224\n",
            "Validation loss after 10 epoch = 5.5384\n",
            "Step 0 avg train loss = 3.6623\n",
            "Step 100 avg train loss = 3.5528\n",
            "Step 200 avg train loss = 3.5669\n",
            "Step 300 avg train loss = 3.5739\n",
            "Step 400 avg train loss = 3.5682\n",
            "Step 500 avg train loss = 3.5993\n",
            "Step 600 avg train loss = 3.6179\n",
            "Step 700 avg train loss = 3.6319\n",
            "Step 800 avg train loss = 3.6124\n",
            "Step 900 avg train loss = 3.6349\n",
            "Step 1000 avg train loss = 3.6409\n",
            "Step 1100 avg train loss = 3.6784\n",
            "Step 1200 avg train loss = 3.6471\n",
            "Step 1300 avg train loss = 3.6662\n",
            "Step 1400 avg train loss = 3.6523\n",
            "Step 1500 avg train loss = 3.6712\n",
            "Step 1600 avg train loss = 3.7040\n",
            "Step 1700 avg train loss = 3.7016\n",
            "Step 1800 avg train loss = 3.6965\n",
            "Step 1900 avg train loss = 3.7071\n",
            "Step 2000 avg train loss = 3.7121\n",
            "Step 2100 avg train loss = 3.6970\n",
            "Step 2200 avg train loss = 3.7061\n",
            "Step 2300 avg train loss = 3.7240\n",
            "Step 2400 avg train loss = 3.7387\n",
            "Validation loss after 11 epoch = 5.6109\n",
            "Step 0 avg train loss = 3.5031\n",
            "Step 100 avg train loss = 3.4773\n",
            "Step 200 avg train loss = 3.4853\n",
            "Step 300 avg train loss = 3.5031\n",
            "Step 400 avg train loss = 3.5186\n",
            "Step 500 avg train loss = 3.5199\n",
            "Step 600 avg train loss = 3.5490\n",
            "Step 700 avg train loss = 3.5425\n",
            "Step 800 avg train loss = 3.5337\n",
            "Step 900 avg train loss = 3.5510\n",
            "Step 1000 avg train loss = 3.5587\n",
            "Step 1100 avg train loss = 3.5905\n",
            "Step 1200 avg train loss = 3.5673\n",
            "Step 1300 avg train loss = 3.5937\n",
            "Step 1400 avg train loss = 3.5926\n",
            "Step 1500 avg train loss = 3.6081\n",
            "Step 1600 avg train loss = 3.5871\n",
            "Step 1700 avg train loss = 3.6191\n",
            "Step 1800 avg train loss = 3.6468\n",
            "Step 1900 avg train loss = 3.6066\n",
            "Step 2000 avg train loss = 3.6250\n",
            "Step 2100 avg train loss = 3.6412\n",
            "Step 2200 avg train loss = 3.6447\n",
            "Step 2300 avg train loss = 3.6349\n",
            "Step 2400 avg train loss = 3.6611\n",
            "Validation loss after 12 epoch = 5.6649\n",
            "Step 0 avg train loss = 3.2465\n",
            "Step 100 avg train loss = 3.3980\n",
            "Step 200 avg train loss = 3.3929\n",
            "Step 300 avg train loss = 3.4516\n",
            "Step 400 avg train loss = 3.4609\n",
            "Step 500 avg train loss = 3.4520\n",
            "Step 600 avg train loss = 3.4679\n",
            "Step 700 avg train loss = 3.4703\n",
            "Step 800 avg train loss = 3.4921\n",
            "Step 900 avg train loss = 3.5049\n",
            "Step 1000 avg train loss = 3.5061\n",
            "Step 1100 avg train loss = 3.5030\n",
            "Step 1200 avg train loss = 3.5061\n",
            "Step 1300 avg train loss = 3.5051\n",
            "Step 1400 avg train loss = 3.5276\n",
            "Step 1500 avg train loss = 3.5379\n",
            "Step 1600 avg train loss = 3.5539\n",
            "Step 1700 avg train loss = 3.5217\n",
            "Step 1800 avg train loss = 3.5708\n",
            "Step 1900 avg train loss = 3.5493\n",
            "Step 2000 avg train loss = 3.5485\n",
            "Step 2100 avg train loss = 3.5556\n",
            "Step 2200 avg train loss = 3.5588\n",
            "Step 2300 avg train loss = 3.5681\n",
            "Step 2400 avg train loss = 3.5727\n",
            "Validation loss after 13 epoch = 5.7428\n",
            "Step 0 avg train loss = 3.4843\n",
            "Step 100 avg train loss = 3.3408\n",
            "Step 200 avg train loss = 3.3699\n",
            "Step 300 avg train loss = 3.3662\n",
            "Step 400 avg train loss = 3.3690\n",
            "Step 500 avg train loss = 3.3666\n",
            "Step 600 avg train loss = 3.3983\n",
            "Step 700 avg train loss = 3.4059\n",
            "Step 800 avg train loss = 3.4280\n",
            "Step 900 avg train loss = 3.4331\n",
            "Step 1000 avg train loss = 3.4389\n",
            "Step 1100 avg train loss = 3.4261\n",
            "Step 1200 avg train loss = 3.4480\n",
            "Step 1300 avg train loss = 3.4433\n",
            "Step 1400 avg train loss = 3.4438\n",
            "Step 1500 avg train loss = 3.4724\n",
            "Step 1600 avg train loss = 3.4493\n",
            "Step 1700 avg train loss = 3.4870\n",
            "Step 1800 avg train loss = 3.4867\n",
            "Step 1900 avg train loss = 3.4986\n",
            "Step 2000 avg train loss = 3.4951\n",
            "Step 2100 avg train loss = 3.5089\n",
            "Step 2200 avg train loss = 3.5053\n",
            "Step 2300 avg train loss = 3.4890\n",
            "Step 2400 avg train loss = 3.5005\n",
            "Validation loss after 14 epoch = 5.8012\n",
            "Step 0 avg train loss = 3.2950\n",
            "Step 100 avg train loss = 3.2704\n",
            "Step 200 avg train loss = 3.3078\n",
            "Step 300 avg train loss = 3.2987\n",
            "Step 400 avg train loss = 3.3082\n",
            "Step 500 avg train loss = 3.3257\n",
            "Step 600 avg train loss = 3.3474\n",
            "Step 700 avg train loss = 3.3411\n",
            "Step 800 avg train loss = 3.3460\n",
            "Step 900 avg train loss = 3.3694\n",
            "Step 1000 avg train loss = 3.3675\n",
            "Step 1100 avg train loss = 3.3700\n",
            "Step 1200 avg train loss = 3.3982\n",
            "Step 1300 avg train loss = 3.3869\n",
            "Step 1400 avg train loss = 3.4140\n",
            "Step 1500 avg train loss = 3.3982\n",
            "Step 1600 avg train loss = 3.3937\n",
            "Step 1700 avg train loss = 3.3875\n",
            "Step 1800 avg train loss = 3.4254\n",
            "Step 1900 avg train loss = 3.4274\n",
            "Step 2000 avg train loss = 3.4398\n",
            "Step 2100 avg train loss = 3.4476\n",
            "Step 2200 avg train loss = 3.4547\n",
            "Step 2300 avg train loss = 3.4491\n",
            "Step 2400 avg train loss = 3.4558\n",
            "Validation loss after 15 epoch = 5.8667\n",
            "Step 0 avg train loss = 3.0764\n",
            "Step 100 avg train loss = 3.2389\n",
            "Step 200 avg train loss = 3.2277\n",
            "Step 300 avg train loss = 3.2444\n",
            "Step 400 avg train loss = 3.2594\n",
            "Step 500 avg train loss = 3.2803\n",
            "Step 600 avg train loss = 3.2746\n",
            "Step 700 avg train loss = 3.2957\n",
            "Step 800 avg train loss = 3.2991\n",
            "Step 900 avg train loss = 3.3178\n",
            "Step 1000 avg train loss = 3.3319\n",
            "Step 1100 avg train loss = 3.3365\n",
            "Step 1200 avg train loss = 3.3164\n",
            "Step 1300 avg train loss = 3.3491\n",
            "Step 1400 avg train loss = 3.3146\n",
            "Step 1500 avg train loss = 3.3328\n",
            "Step 1600 avg train loss = 3.3364\n",
            "Step 1700 avg train loss = 3.3402\n",
            "Step 1800 avg train loss = 3.3682\n",
            "Step 1900 avg train loss = 3.3743\n",
            "Step 2000 avg train loss = 3.3754\n",
            "Step 2100 avg train loss = 3.3694\n",
            "Step 2200 avg train loss = 3.3999\n",
            "Step 2300 avg train loss = 3.3946\n",
            "Step 2400 avg train loss = 3.4192\n",
            "Validation loss after 16 epoch = 5.9372\n",
            "Step 0 avg train loss = 3.3933\n",
            "Step 100 avg train loss = 3.1784\n",
            "Step 200 avg train loss = 3.1701\n",
            "Step 300 avg train loss = 3.1890\n",
            "Step 400 avg train loss = 3.1968\n",
            "Step 500 avg train loss = 3.2134\n",
            "Step 600 avg train loss = 3.2178\n",
            "Step 700 avg train loss = 3.2144\n",
            "Step 800 avg train loss = 3.2375\n",
            "Step 900 avg train loss = 3.2455\n",
            "Step 1000 avg train loss = 3.2681\n",
            "Step 1100 avg train loss = 3.2639\n",
            "Step 1200 avg train loss = 3.2868\n",
            "Step 1300 avg train loss = 3.2853\n",
            "Step 1400 avg train loss = 3.2861\n",
            "Step 1500 avg train loss = 3.2977\n",
            "Step 1600 avg train loss = 3.3114\n",
            "Step 1700 avg train loss = 3.3193\n",
            "Step 1800 avg train loss = 3.3016\n",
            "Step 1900 avg train loss = 3.3397\n",
            "Step 2000 avg train loss = 3.3420\n",
            "Step 2100 avg train loss = 3.3277\n",
            "Step 2200 avg train loss = 3.3378\n",
            "Step 2300 avg train loss = 3.3350\n",
            "Step 2400 avg train loss = 3.3512\n",
            "Validation loss after 17 epoch = 5.9951\n",
            "Step 0 avg train loss = 3.0336\n",
            "Step 100 avg train loss = 3.1353\n",
            "Step 200 avg train loss = 3.1264\n",
            "Step 300 avg train loss = 3.1186\n",
            "Step 400 avg train loss = 3.1541\n",
            "Step 500 avg train loss = 3.1671\n",
            "Step 600 avg train loss = 3.1556\n",
            "Step 700 avg train loss = 3.2066\n",
            "Step 800 avg train loss = 3.1836\n",
            "Step 900 avg train loss = 3.2164\n",
            "Step 1000 avg train loss = 3.2107\n",
            "Step 1100 avg train loss = 3.2105\n",
            "Step 1200 avg train loss = 3.2309\n",
            "Step 1300 avg train loss = 3.2257\n",
            "Step 1400 avg train loss = 3.2504\n",
            "Step 1500 avg train loss = 3.2449\n",
            "Step 1600 avg train loss = 3.2530\n",
            "Step 1700 avg train loss = 3.2546\n",
            "Step 1800 avg train loss = 3.2687\n",
            "Step 1900 avg train loss = 3.2740\n",
            "Step 2000 avg train loss = 3.2803\n",
            "Step 2100 avg train loss = 3.2730\n",
            "Step 2200 avg train loss = 3.3055\n",
            "Step 2300 avg train loss = 3.3069\n",
            "Step 2400 avg train loss = 3.2893\n",
            "Validation loss after 18 epoch = 6.0573\n",
            "Step 0 avg train loss = 2.9826\n",
            "Step 100 avg train loss = 3.1011\n",
            "Step 200 avg train loss = 3.0790\n",
            "Step 300 avg train loss = 3.1038\n",
            "Step 400 avg train loss = 3.0980\n",
            "Step 500 avg train loss = 3.1306\n",
            "Step 600 avg train loss = 3.1302\n",
            "Step 700 avg train loss = 3.1368\n",
            "Step 800 avg train loss = 3.1633\n",
            "Step 900 avg train loss = 3.1501\n",
            "Step 1000 avg train loss = 3.1537\n",
            "Step 1100 avg train loss = 3.1750\n",
            "Step 1200 avg train loss = 3.1863\n",
            "Step 1300 avg train loss = 3.1878\n",
            "Step 1400 avg train loss = 3.1940\n",
            "Step 1500 avg train loss = 3.2031\n",
            "Step 1600 avg train loss = 3.2004\n",
            "Step 1700 avg train loss = 3.1932\n",
            "Step 1800 avg train loss = 3.2144\n",
            "Step 1900 avg train loss = 3.1903\n",
            "Step 2000 avg train loss = 3.2335\n",
            "Step 2100 avg train loss = 3.2522\n",
            "Step 2200 avg train loss = 3.2534\n",
            "Step 2300 avg train loss = 3.2527\n",
            "Step 2400 avg train loss = 3.2656\n",
            "Validation loss after 19 epoch = 6.1049\n",
            "Saving best model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyqYWy08Bve",
        "colab_type": "code",
        "outputId": "002ffbb5-89b5-4713-b58b-c27229979efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of tuned LSTM model')\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK8jRW5c8Bvj",
        "colab_type": "code",
        "outputId": "1a803577-0fa6-4184-b8d3-3110bc75b830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "perp = [2**(i[1]/np.log(2)) for i in plot_cache] \n",
        "print('The minimum validation loss occurred at {} and it is {}'.format(perp.index(min(perp)),min(perp)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum validation loss occurred at 3 and it is 168.0075867791675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps3DtmuR8Bvo",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbgtu3AL3wSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58f751c5-5ab7-4a4e-b548-5c4b4ce030cc"
      },
      "source": [
        "# first hyperparameter tuning\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "# single grid search for the hyperparameter tuning\n",
        "param_grid_emb = {\"embedding_size\": [100,200,300,400,500]}      \n",
        "\n",
        "# list of parameters=========== \n",
        "epoch_num = 20\n",
        "hidden_size = 128 # the second hyperparameter to be tuned. \n",
        "learning_rate=0.001\n",
        "# Loop over embedding size:\n",
        "for embedding_size in param_grid_emb[\"embedding_size\"]:\n",
        "  \n",
        "  embedding_size = embedding_size\n",
        "  hidden_size = hidden_size # output of dimension \n",
        "  num_layers = 2\n",
        "  lstm_dropout = 0.1\n",
        "# input_size = lookup.weight.size(1)\n",
        "  vocab_size = len(train_dict)\n",
        "  \n",
        "  options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "  \n",
        "  model = LSTMModel(options).to(current_device)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "  model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = optim.Adam(model_parameters, lr=learning_rate)\n",
        "  \n",
        "  print(model)\n",
        "  \n",
        "  plot_cache = []\n",
        "  min_val_loss = 20   # why is it 20??\n",
        "\n",
        "  for epoch_number in range(epoch_num):\n",
        "    \n",
        "    # do train \n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "\n",
        "# save the best model in this single grid search:         \n",
        "print('Saving best model with best embedding dimension...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './emb_tune_best_LSTM.pt')\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 100, padding_idx=2)\n",
            "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4213\n",
            "Step 100 avg train loss = 7.8746\n",
            "Step 200 avg train loss = 7.1487\n",
            "Step 300 avg train loss = 6.9989\n",
            "Step 400 avg train loss = 6.8819\n",
            "Step 500 avg train loss = 6.7695\n",
            "Step 600 avg train loss = 6.6904\n",
            "Step 700 avg train loss = 6.6061\n",
            "Step 800 avg train loss = 6.5169\n",
            "Step 900 avg train loss = 6.4813\n",
            "Step 1000 avg train loss = 6.4483\n",
            "Step 1100 avg train loss = 6.3791\n",
            "Step 1200 avg train loss = 6.3451\n",
            "Step 1300 avg train loss = 6.3354\n",
            "Step 1400 avg train loss = 6.2372\n",
            "Step 1500 avg train loss = 6.2236\n",
            "Step 1600 avg train loss = 6.1985\n",
            "Step 1700 avg train loss = 6.1570\n",
            "Step 1800 avg train loss = 6.1361\n",
            "Step 1900 avg train loss = 6.1115\n",
            "Step 2000 avg train loss = 6.0735\n",
            "Step 2100 avg train loss = 6.0716\n",
            "Step 2200 avg train loss = 6.0352\n",
            "Step 2300 avg train loss = 6.0043\n",
            "Step 2400 avg train loss = 5.9858\n",
            "Validation loss after 0 epoch = 5.8060\n",
            "Step 0 avg train loss = 5.9210\n",
            "Step 100 avg train loss = 5.8924\n",
            "Step 200 avg train loss = 5.8761\n",
            "Step 300 avg train loss = 5.8770\n",
            "Step 400 avg train loss = 5.8671\n",
            "Step 500 avg train loss = 5.8507\n",
            "Step 600 avg train loss = 5.8032\n",
            "Step 700 avg train loss = 5.8165\n",
            "Step 800 avg train loss = 5.7875\n",
            "Step 900 avg train loss = 5.7804\n",
            "Step 1000 avg train loss = 5.7752\n",
            "Step 1100 avg train loss = 5.7494\n",
            "Step 1200 avg train loss = 5.7438\n",
            "Step 1300 avg train loss = 5.7528\n",
            "Step 1400 avg train loss = 5.7070\n",
            "Step 1500 avg train loss = 5.7132\n",
            "Step 1600 avg train loss = 5.6873\n",
            "Step 1700 avg train loss = 5.6781\n",
            "Step 1800 avg train loss = 5.6919\n",
            "Step 1900 avg train loss = 5.6888\n",
            "Step 2000 avg train loss = 5.6731\n",
            "Step 2100 avg train loss = 5.6440\n",
            "Step 2200 avg train loss = 5.6168\n",
            "Step 2300 avg train loss = 5.6168\n",
            "Step 2400 avg train loss = 5.6350\n",
            "Validation loss after 1 epoch = 5.5183\n",
            "Step 0 avg train loss = 5.2898\n",
            "Step 100 avg train loss = 5.5252\n",
            "Step 200 avg train loss = 5.5130\n",
            "Step 300 avg train loss = 5.4821\n",
            "Step 400 avg train loss = 5.5221\n",
            "Step 500 avg train loss = 5.5088\n",
            "Step 600 avg train loss = 5.4730\n",
            "Step 700 avg train loss = 5.4989\n",
            "Step 800 avg train loss = 5.4681\n",
            "Step 900 avg train loss = 5.4549\n",
            "Step 1000 avg train loss = 5.4655\n",
            "Step 1100 avg train loss = 5.4779\n",
            "Step 1200 avg train loss = 5.4838\n",
            "Step 1300 avg train loss = 5.4428\n",
            "Step 1400 avg train loss = 5.4224\n",
            "Step 1500 avg train loss = 5.4363\n",
            "Step 1600 avg train loss = 5.4266\n",
            "Step 1700 avg train loss = 5.4350\n",
            "Step 1800 avg train loss = 5.4167\n",
            "Step 1900 avg train loss = 5.4456\n",
            "Step 2000 avg train loss = 5.4338\n",
            "Step 2100 avg train loss = 5.3956\n",
            "Step 2200 avg train loss = 5.4055\n",
            "Step 2300 avg train loss = 5.3788\n",
            "Step 2400 avg train loss = 5.4058\n",
            "Validation loss after 2 epoch = 5.3863\n",
            "Step 0 avg train loss = 5.3325\n",
            "Step 100 avg train loss = 5.2772\n",
            "Step 200 avg train loss = 5.2860\n",
            "Step 300 avg train loss = 5.2600\n",
            "Step 400 avg train loss = 5.2802\n",
            "Step 500 avg train loss = 5.2665\n",
            "Step 600 avg train loss = 5.2443\n",
            "Step 700 avg train loss = 5.2463\n",
            "Step 800 avg train loss = 5.2773\n",
            "Step 900 avg train loss = 5.2569\n",
            "Step 1000 avg train loss = 5.2485\n",
            "Step 1100 avg train loss = 5.2619\n",
            "Step 1200 avg train loss = 5.2589\n",
            "Step 1300 avg train loss = 5.2629\n",
            "Step 1400 avg train loss = 5.2452\n",
            "Step 1500 avg train loss = 5.2442\n",
            "Step 1600 avg train loss = 5.2689\n",
            "Step 1700 avg train loss = 5.2641\n",
            "Step 1800 avg train loss = 5.2548\n",
            "Step 1900 avg train loss = 5.2363\n",
            "Step 2000 avg train loss = 5.2428\n",
            "Step 2100 avg train loss = 5.2338\n",
            "Step 2200 avg train loss = 5.2127\n",
            "Step 2300 avg train loss = 5.2210\n",
            "Step 2400 avg train loss = 5.2042\n",
            "Validation loss after 3 epoch = 5.3096\n",
            "Step 0 avg train loss = 4.8802\n",
            "Step 100 avg train loss = 5.1177\n",
            "Step 200 avg train loss = 5.1024\n",
            "Step 300 avg train loss = 5.1025\n",
            "Step 400 avg train loss = 5.1164\n",
            "Step 500 avg train loss = 5.1124\n",
            "Step 600 avg train loss = 5.1118\n",
            "Step 700 avg train loss = 5.1105\n",
            "Step 800 avg train loss = 5.1044\n",
            "Step 900 avg train loss = 5.0983\n",
            "Step 1000 avg train loss = 5.1144\n",
            "Step 1100 avg train loss = 5.1187\n",
            "Step 1200 avg train loss = 5.1188\n",
            "Step 1300 avg train loss = 5.0738\n",
            "Step 1400 avg train loss = 5.0609\n",
            "Step 1500 avg train loss = 5.0963\n",
            "Step 1600 avg train loss = 5.0890\n",
            "Step 1700 avg train loss = 5.1065\n",
            "Step 1800 avg train loss = 5.0918\n",
            "Step 1900 avg train loss = 5.0748\n",
            "Step 2000 avg train loss = 5.0908\n",
            "Step 2100 avg train loss = 5.0621\n",
            "Step 2200 avg train loss = 5.1015\n",
            "Step 2300 avg train loss = 5.0835\n",
            "Step 2400 avg train loss = 5.1103\n",
            "Validation loss after 4 epoch = 5.2752\n",
            "Step 0 avg train loss = 4.7336\n",
            "Step 100 avg train loss = 4.9430\n",
            "Step 200 avg train loss = 4.9663\n",
            "Step 300 avg train loss = 4.9658\n",
            "Step 400 avg train loss = 4.9664\n",
            "Step 500 avg train loss = 4.9587\n",
            "Step 600 avg train loss = 4.9794\n",
            "Step 700 avg train loss = 4.9756\n",
            "Step 800 avg train loss = 4.9788\n",
            "Step 900 avg train loss = 4.9832\n",
            "Step 1000 avg train loss = 4.9915\n",
            "Step 1100 avg train loss = 4.9797\n",
            "Step 1200 avg train loss = 4.9717\n",
            "Step 1300 avg train loss = 4.9927\n",
            "Step 1400 avg train loss = 4.9951\n",
            "Step 1500 avg train loss = 4.9762\n",
            "Step 1600 avg train loss = 4.9692\n",
            "Step 1700 avg train loss = 4.9871\n",
            "Step 1800 avg train loss = 4.9658\n",
            "Step 1900 avg train loss = 4.9636\n",
            "Step 2000 avg train loss = 4.9914\n",
            "Step 2100 avg train loss = 4.9707\n",
            "Step 2200 avg train loss = 4.9736\n",
            "Step 2300 avg train loss = 4.9931\n",
            "Step 2400 avg train loss = 4.9727\n",
            "Validation loss after 5 epoch = 5.2612\n",
            "Step 0 avg train loss = 4.9070\n",
            "Step 100 avg train loss = 4.8496\n",
            "Step 200 avg train loss = 4.8462\n",
            "Step 300 avg train loss = 4.8680\n",
            "Step 400 avg train loss = 4.8684\n",
            "Step 500 avg train loss = 4.8638\n",
            "Step 600 avg train loss = 4.8692\n",
            "Step 700 avg train loss = 4.8554\n",
            "Step 800 avg train loss = 4.8742\n",
            "Step 900 avg train loss = 4.8756\n",
            "Step 1000 avg train loss = 4.8830\n",
            "Step 1100 avg train loss = 4.8991\n",
            "Step 1200 avg train loss = 4.8882\n",
            "Step 1300 avg train loss = 4.8722\n",
            "Step 1400 avg train loss = 4.8845\n",
            "Step 1500 avg train loss = 4.8755\n",
            "Step 1600 avg train loss = 4.8785\n",
            "Step 1700 avg train loss = 4.8972\n",
            "Step 1800 avg train loss = 4.8711\n",
            "Step 1900 avg train loss = 4.8826\n",
            "Step 2000 avg train loss = 4.8773\n",
            "Step 2100 avg train loss = 4.8837\n",
            "Step 2200 avg train loss = 4.8666\n",
            "Step 2300 avg train loss = 4.8724\n",
            "Step 2400 avg train loss = 4.8667\n",
            "Validation loss after 6 epoch = 5.2549\n",
            "Step 0 avg train loss = 4.9683\n",
            "Step 100 avg train loss = 4.7569\n",
            "Step 200 avg train loss = 4.7748\n",
            "Step 300 avg train loss = 4.7621\n",
            "Step 400 avg train loss = 4.7830\n",
            "Step 500 avg train loss = 4.7746\n",
            "Step 600 avg train loss = 4.7811\n",
            "Step 700 avg train loss = 4.7867\n",
            "Step 800 avg train loss = 4.7782\n",
            "Step 900 avg train loss = 4.7891\n",
            "Step 1000 avg train loss = 4.7958\n",
            "Step 1100 avg train loss = 4.7838\n",
            "Step 1200 avg train loss = 4.7977\n",
            "Step 1300 avg train loss = 4.7782\n",
            "Step 1400 avg train loss = 4.8013\n",
            "Step 1500 avg train loss = 4.8037\n",
            "Step 1600 avg train loss = 4.8004\n",
            "Step 1700 avg train loss = 4.7862\n",
            "Step 1800 avg train loss = 4.7852\n",
            "Step 1900 avg train loss = 4.7985\n",
            "Step 2000 avg train loss = 4.7969\n",
            "Step 2100 avg train loss = 4.7974\n",
            "Step 2200 avg train loss = 4.7890\n",
            "Step 2300 avg train loss = 4.7932\n",
            "Step 2400 avg train loss = 4.8149\n",
            "Validation loss after 7 epoch = 5.2613\n",
            "Step 0 avg train loss = 4.7626\n",
            "Step 100 avg train loss = 4.6603\n",
            "Step 200 avg train loss = 4.6610\n",
            "Step 300 avg train loss = 4.6759\n",
            "Step 400 avg train loss = 4.6923\n",
            "Step 500 avg train loss = 4.7105\n",
            "Step 600 avg train loss = 4.7110\n",
            "Step 700 avg train loss = 4.6866\n",
            "Step 800 avg train loss = 4.7092\n",
            "Step 900 avg train loss = 4.6980\n",
            "Step 1000 avg train loss = 4.6930\n",
            "Step 1100 avg train loss = 4.7068\n",
            "Step 1200 avg train loss = 4.7120\n",
            "Step 1300 avg train loss = 4.7182\n",
            "Step 1400 avg train loss = 4.7141\n",
            "Step 1500 avg train loss = 4.7250\n",
            "Step 1600 avg train loss = 4.7170\n",
            "Step 1700 avg train loss = 4.7178\n",
            "Step 1800 avg train loss = 4.7172\n",
            "Step 1900 avg train loss = 4.7579\n",
            "Step 2000 avg train loss = 4.7407\n",
            "Step 2100 avg train loss = 4.7469\n",
            "Step 2200 avg train loss = 4.7218\n",
            "Step 2300 avg train loss = 4.7487\n",
            "Step 2400 avg train loss = 4.7335\n",
            "Validation loss after 8 epoch = 5.2732\n",
            "Step 0 avg train loss = 4.2264\n",
            "Step 100 avg train loss = 4.5932\n",
            "Step 200 avg train loss = 4.6094\n",
            "Step 300 avg train loss = 4.6086\n",
            "Step 400 avg train loss = 4.6216\n",
            "Step 500 avg train loss = 4.6283\n",
            "Step 600 avg train loss = 4.6185\n",
            "Step 700 avg train loss = 4.6355\n",
            "Step 800 avg train loss = 4.6510\n",
            "Step 900 avg train loss = 4.6355\n",
            "Step 1000 avg train loss = 4.6416\n",
            "Step 1100 avg train loss = 4.6430\n",
            "Step 1200 avg train loss = 4.6357\n",
            "Step 1300 avg train loss = 4.6544\n",
            "Step 1400 avg train loss = 4.6548\n",
            "Step 1500 avg train loss = 4.6553\n",
            "Step 1600 avg train loss = 4.6641\n",
            "Step 1700 avg train loss = 4.6564\n",
            "Step 1800 avg train loss = 4.6535\n",
            "Step 1900 avg train loss = 4.6741\n",
            "Step 2000 avg train loss = 4.6606\n",
            "Step 2100 avg train loss = 4.6679\n",
            "Step 2200 avg train loss = 4.6566\n",
            "Step 2300 avg train loss = 4.6698\n",
            "Step 2400 avg train loss = 4.6707\n",
            "Validation loss after 9 epoch = 5.2908\n",
            "Step 0 avg train loss = 4.3609\n",
            "Step 100 avg train loss = 4.5336\n",
            "Step 200 avg train loss = 4.5316\n",
            "Step 300 avg train loss = 4.5584\n",
            "Step 400 avg train loss = 4.5622\n",
            "Step 500 avg train loss = 4.5577\n",
            "Step 600 avg train loss = 4.5820\n",
            "Step 700 avg train loss = 4.5767\n",
            "Step 800 avg train loss = 4.5726\n",
            "Step 900 avg train loss = 4.5919\n",
            "Step 1000 avg train loss = 4.5902\n",
            "Step 1100 avg train loss = 4.5911\n",
            "Step 1200 avg train loss = 4.5693\n",
            "Step 1300 avg train loss = 4.6063\n",
            "Step 1400 avg train loss = 4.5837\n",
            "Step 1500 avg train loss = 4.5770\n",
            "Step 1600 avg train loss = 4.6167\n",
            "Step 1700 avg train loss = 4.6097\n",
            "Step 1800 avg train loss = 4.5910\n",
            "Step 1900 avg train loss = 4.5874\n",
            "Step 2000 avg train loss = 4.5941\n",
            "Step 2100 avg train loss = 4.6049\n",
            "Step 2200 avg train loss = 4.6131\n",
            "Step 2300 avg train loss = 4.6140\n",
            "Step 2400 avg train loss = 4.6092\n",
            "Validation loss after 10 epoch = 5.3071\n",
            "Step 0 avg train loss = 4.5234\n",
            "Step 100 avg train loss = 4.4774\n",
            "Step 200 avg train loss = 4.4867\n",
            "Step 300 avg train loss = 4.5109\n",
            "Step 400 avg train loss = 4.5060\n",
            "Step 500 avg train loss = 4.4964\n",
            "Step 600 avg train loss = 4.4925\n",
            "Step 700 avg train loss = 4.5249\n",
            "Step 800 avg train loss = 4.5542\n",
            "Step 900 avg train loss = 4.5296\n",
            "Step 1000 avg train loss = 4.5133\n",
            "Step 1100 avg train loss = 4.5333\n",
            "Step 1200 avg train loss = 4.5208\n",
            "Step 1300 avg train loss = 4.5406\n",
            "Step 1400 avg train loss = 4.5366\n",
            "Step 1500 avg train loss = 4.5508\n",
            "Step 1600 avg train loss = 4.5247\n",
            "Step 1700 avg train loss = 4.5251\n",
            "Step 1800 avg train loss = 4.5395\n",
            "Step 1900 avg train loss = 4.5391\n",
            "Step 2000 avg train loss = 4.5513\n",
            "Step 2100 avg train loss = 4.5668\n",
            "Step 2200 avg train loss = 4.5612\n",
            "Step 2300 avg train loss = 4.5668\n",
            "Step 2400 avg train loss = 4.5683\n",
            "Validation loss after 11 epoch = 5.3244\n",
            "Step 0 avg train loss = 4.6189\n",
            "Step 100 avg train loss = 4.4407\n",
            "Step 200 avg train loss = 4.4447\n",
            "Step 300 avg train loss = 4.4481\n",
            "Step 400 avg train loss = 4.4525\n",
            "Step 500 avg train loss = 4.4532\n",
            "Step 600 avg train loss = 4.4556\n",
            "Step 700 avg train loss = 4.4450\n",
            "Step 800 avg train loss = 4.4729\n",
            "Step 900 avg train loss = 4.4709\n",
            "Step 1000 avg train loss = 4.4684\n",
            "Step 1100 avg train loss = 4.4602\n",
            "Step 1200 avg train loss = 4.4782\n",
            "Step 1300 avg train loss = 4.4995\n",
            "Step 1400 avg train loss = 4.4816\n",
            "Step 1500 avg train loss = 4.4980\n",
            "Step 1600 avg train loss = 4.4720\n",
            "Step 1700 avg train loss = 4.4893\n",
            "Step 1800 avg train loss = 4.4811\n",
            "Step 1900 avg train loss = 4.5094\n",
            "Step 2000 avg train loss = 4.5121\n",
            "Step 2100 avg train loss = 4.5122\n",
            "Step 2200 avg train loss = 4.5054\n",
            "Step 2300 avg train loss = 4.5327\n",
            "Step 2400 avg train loss = 4.5322\n",
            "Validation loss after 12 epoch = 5.3482\n",
            "Step 0 avg train loss = 4.5102\n",
            "Step 100 avg train loss = 4.3774\n",
            "Step 200 avg train loss = 4.4062\n",
            "Step 300 avg train loss = 4.3978\n",
            "Step 400 avg train loss = 4.4094\n",
            "Step 500 avg train loss = 4.3967\n",
            "Step 600 avg train loss = 4.4133\n",
            "Step 700 avg train loss = 4.4134\n",
            "Step 800 avg train loss = 4.3941\n",
            "Step 900 avg train loss = 4.4223\n",
            "Step 1000 avg train loss = 4.4277\n",
            "Step 1100 avg train loss = 4.4349\n",
            "Step 1200 avg train loss = 4.4507\n",
            "Step 1300 avg train loss = 4.4455\n",
            "Step 1400 avg train loss = 4.4516\n",
            "Step 1500 avg train loss = 4.4522\n",
            "Step 1600 avg train loss = 4.4395\n",
            "Step 1700 avg train loss = 4.4390\n",
            "Step 1800 avg train loss = 4.4805\n",
            "Step 1900 avg train loss = 4.4753\n",
            "Step 2000 avg train loss = 4.4691\n",
            "Step 2100 avg train loss = 4.4474\n",
            "Step 2200 avg train loss = 4.4603\n",
            "Step 2300 avg train loss = 4.4719\n",
            "Step 2400 avg train loss = 4.4617\n",
            "Validation loss after 13 epoch = 5.3702\n",
            "Step 0 avg train loss = 4.5009\n",
            "Step 100 avg train loss = 4.3261\n",
            "Step 200 avg train loss = 4.3439\n",
            "Step 300 avg train loss = 4.3785\n",
            "Step 400 avg train loss = 4.3482\n",
            "Step 500 avg train loss = 4.3721\n",
            "Step 600 avg train loss = 4.3677\n",
            "Step 700 avg train loss = 4.3822\n",
            "Step 800 avg train loss = 4.3751\n",
            "Step 900 avg train loss = 4.3660\n",
            "Step 1000 avg train loss = 4.3929\n",
            "Step 1100 avg train loss = 4.3891\n",
            "Step 1200 avg train loss = 4.4001\n",
            "Step 1300 avg train loss = 4.4061\n",
            "Step 1400 avg train loss = 4.3954\n",
            "Step 1500 avg train loss = 4.3983\n",
            "Step 1600 avg train loss = 4.4164\n",
            "Step 1700 avg train loss = 4.4249\n",
            "Step 1800 avg train loss = 4.4160\n",
            "Step 1900 avg train loss = 4.4266\n",
            "Step 2000 avg train loss = 4.4438\n",
            "Step 2100 avg train loss = 4.4120\n",
            "Step 2200 avg train loss = 4.3997\n",
            "Step 2300 avg train loss = 4.4218\n",
            "Step 2400 avg train loss = 4.4174\n",
            "Validation loss after 14 epoch = 5.3874\n",
            "Step 0 avg train loss = 4.2326\n",
            "Step 100 avg train loss = 4.3039\n",
            "Step 200 avg train loss = 4.2992\n",
            "Step 300 avg train loss = 4.3161\n",
            "Step 400 avg train loss = 4.3237\n",
            "Step 500 avg train loss = 4.3170\n",
            "Step 600 avg train loss = 4.3386\n",
            "Step 700 avg train loss = 4.3483\n",
            "Step 800 avg train loss = 4.3323\n",
            "Step 900 avg train loss = 4.3385\n",
            "Step 1000 avg train loss = 4.3675\n",
            "Step 1100 avg train loss = 4.3489\n",
            "Step 1200 avg train loss = 4.3481\n",
            "Step 1300 avg train loss = 4.3677\n",
            "Step 1400 avg train loss = 4.3548\n",
            "Step 1500 avg train loss = 4.3598\n",
            "Step 1600 avg train loss = 4.3679\n",
            "Step 1700 avg train loss = 4.3411\n",
            "Step 1800 avg train loss = 4.3936\n",
            "Step 1900 avg train loss = 4.3764\n",
            "Step 2000 avg train loss = 4.3731\n",
            "Step 2100 avg train loss = 4.3836\n",
            "Step 2200 avg train loss = 4.3778\n",
            "Step 2300 avg train loss = 4.4034\n",
            "Step 2400 avg train loss = 4.3864\n",
            "Validation loss after 15 epoch = 5.4136\n",
            "Step 0 avg train loss = 4.2984\n",
            "Step 100 avg train loss = 4.2669\n",
            "Step 200 avg train loss = 4.2530\n",
            "Step 300 avg train loss = 4.2793\n",
            "Step 400 avg train loss = 4.2854\n",
            "Step 500 avg train loss = 4.3026\n",
            "Step 600 avg train loss = 4.2912\n",
            "Step 700 avg train loss = 4.2836\n",
            "Step 800 avg train loss = 4.2947\n",
            "Step 900 avg train loss = 4.3052\n",
            "Step 1000 avg train loss = 4.2963\n",
            "Step 1100 avg train loss = 4.3239\n",
            "Step 1200 avg train loss = 4.3211\n",
            "Step 1300 avg train loss = 4.2995\n",
            "Step 1400 avg train loss = 4.3260\n",
            "Step 1500 avg train loss = 4.3411\n",
            "Step 1600 avg train loss = 4.3345\n",
            "Step 1700 avg train loss = 4.3502\n",
            "Step 1800 avg train loss = 4.3376\n",
            "Step 1900 avg train loss = 4.3567\n",
            "Step 2000 avg train loss = 4.3515\n",
            "Step 2100 avg train loss = 4.3368\n",
            "Step 2200 avg train loss = 4.3526\n",
            "Step 2300 avg train loss = 4.3681\n",
            "Step 2400 avg train loss = 4.3282\n",
            "Validation loss after 16 epoch = 5.4343\n",
            "Step 0 avg train loss = 4.1605\n",
            "Step 100 avg train loss = 4.2267\n",
            "Step 200 avg train loss = 4.2250\n",
            "Step 300 avg train loss = 4.2411\n",
            "Step 400 avg train loss = 4.2660\n",
            "Step 500 avg train loss = 4.2262\n",
            "Step 600 avg train loss = 4.2590\n",
            "Step 700 avg train loss = 4.2621\n",
            "Step 800 avg train loss = 4.2726\n",
            "Step 900 avg train loss = 4.2636\n",
            "Step 1000 avg train loss = 4.2787\n",
            "Step 1100 avg train loss = 4.2690\n",
            "Step 1200 avg train loss = 4.2664\n",
            "Step 1300 avg train loss = 4.2884\n",
            "Step 1400 avg train loss = 4.3142\n",
            "Step 1500 avg train loss = 4.2918\n",
            "Step 1600 avg train loss = 4.2721\n",
            "Step 1700 avg train loss = 4.2999\n",
            "Step 1800 avg train loss = 4.2989\n",
            "Step 1900 avg train loss = 4.3143\n",
            "Step 2000 avg train loss = 4.2947\n",
            "Step 2100 avg train loss = 4.3045\n",
            "Step 2200 avg train loss = 4.3129\n",
            "Step 2300 avg train loss = 4.3625\n",
            "Step 2400 avg train loss = 4.3251\n",
            "Validation loss after 17 epoch = 5.4610\n",
            "Step 0 avg train loss = 4.2360\n",
            "Step 100 avg train loss = 4.1870\n",
            "Step 200 avg train loss = 4.1929\n",
            "Step 300 avg train loss = 4.2190\n",
            "Step 400 avg train loss = 4.1917\n",
            "Step 500 avg train loss = 4.2137\n",
            "Step 600 avg train loss = 4.2025\n",
            "Step 700 avg train loss = 4.2336\n",
            "Step 800 avg train loss = 4.2289\n",
            "Step 900 avg train loss = 4.2205\n",
            "Step 1000 avg train loss = 4.2273\n",
            "Step 1100 avg train loss = 4.2746\n",
            "Step 1200 avg train loss = 4.2316\n",
            "Step 1300 avg train loss = 4.2392\n",
            "Step 1400 avg train loss = 4.2620\n",
            "Step 1500 avg train loss = 4.2703\n",
            "Step 1600 avg train loss = 4.2682\n",
            "Step 1700 avg train loss = 4.2784\n",
            "Step 1800 avg train loss = 4.2766\n",
            "Step 1900 avg train loss = 4.2842\n",
            "Step 2000 avg train loss = 4.2974\n",
            "Step 2100 avg train loss = 4.2884\n",
            "Step 2200 avg train loss = 4.2990\n",
            "Step 2300 avg train loss = 4.2737\n",
            "Step 2400 avg train loss = 4.3108\n",
            "Validation loss after 18 epoch = 5.4789\n",
            "Step 0 avg train loss = 4.2661\n",
            "Step 100 avg train loss = 4.1634\n",
            "Step 200 avg train loss = 4.1750\n",
            "Step 300 avg train loss = 4.1774\n",
            "Step 400 avg train loss = 4.1695\n",
            "Step 500 avg train loss = 4.1819\n",
            "Step 600 avg train loss = 4.1751\n",
            "Step 700 avg train loss = 4.1987\n",
            "Step 800 avg train loss = 4.2052\n",
            "Step 900 avg train loss = 4.2081\n",
            "Step 1000 avg train loss = 4.1995\n",
            "Step 1100 avg train loss = 4.2407\n",
            "Step 1200 avg train loss = 4.2215\n",
            "Step 1300 avg train loss = 4.2203\n",
            "Step 1400 avg train loss = 4.2317\n",
            "Step 1500 avg train loss = 4.2465\n",
            "Step 1600 avg train loss = 4.2369\n",
            "Step 1700 avg train loss = 4.2419\n",
            "Step 1800 avg train loss = 4.2411\n",
            "Step 1900 avg train loss = 4.2564\n",
            "Step 2000 avg train loss = 4.2516\n",
            "Step 2100 avg train loss = 4.2519\n",
            "Step 2200 avg train loss = 4.2472\n",
            "Step 2300 avg train loss = 4.2614\n",
            "Step 2400 avg train loss = 4.2573\n",
            "Validation loss after 19 epoch = 5.5068\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 200, padding_idx=2)\n",
            "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4072\n",
            "Step 100 avg train loss = 7.8734\n",
            "Step 200 avg train loss = 7.1471\n",
            "Step 300 avg train loss = 6.9900\n",
            "Step 400 avg train loss = 6.8690\n",
            "Step 500 avg train loss = 6.7549\n",
            "Step 600 avg train loss = 6.6119\n",
            "Step 700 avg train loss = 6.5381\n",
            "Step 800 avg train loss = 6.4657\n",
            "Step 900 avg train loss = 6.4130\n",
            "Step 1000 avg train loss = 6.3581\n",
            "Step 1100 avg train loss = 6.3145\n",
            "Step 1200 avg train loss = 6.2397\n",
            "Step 1300 avg train loss = 6.2099\n",
            "Step 1400 avg train loss = 6.1619\n",
            "Step 1500 avg train loss = 6.1458\n",
            "Step 1600 avg train loss = 6.1025\n",
            "Step 1700 avg train loss = 6.0803\n",
            "Step 1800 avg train loss = 6.0359\n",
            "Step 1900 avg train loss = 6.0127\n",
            "Step 2000 avg train loss = 5.9815\n",
            "Step 2100 avg train loss = 5.9464\n",
            "Step 2200 avg train loss = 5.9454\n",
            "Step 2300 avg train loss = 5.9100\n",
            "Step 2400 avg train loss = 5.9083\n",
            "Validation loss after 0 epoch = 5.7213\n",
            "Step 0 avg train loss = 5.7127\n",
            "Step 100 avg train loss = 5.7983\n",
            "Step 200 avg train loss = 5.7793\n",
            "Step 300 avg train loss = 5.7620\n",
            "Step 400 avg train loss = 5.7586\n",
            "Step 500 avg train loss = 5.7486\n",
            "Step 600 avg train loss = 5.7258\n",
            "Step 700 avg train loss = 5.7270\n",
            "Step 800 avg train loss = 5.7068\n",
            "Step 900 avg train loss = 5.6857\n",
            "Step 1000 avg train loss = 5.6785\n",
            "Step 1100 avg train loss = 5.6672\n",
            "Step 1200 avg train loss = 5.6578\n",
            "Step 1300 avg train loss = 5.6541\n",
            "Step 1400 avg train loss = 5.6405\n",
            "Step 1500 avg train loss = 5.6194\n",
            "Step 1600 avg train loss = 5.6212\n",
            "Step 1700 avg train loss = 5.5919\n",
            "Step 1800 avg train loss = 5.5986\n",
            "Step 1900 avg train loss = 5.5820\n",
            "Step 2000 avg train loss = 5.5825\n",
            "Step 2100 avg train loss = 5.5540\n",
            "Step 2200 avg train loss = 5.5547\n",
            "Step 2300 avg train loss = 5.5617\n",
            "Step 2400 avg train loss = 5.5197\n",
            "Validation loss after 1 epoch = 5.4519\n",
            "Step 0 avg train loss = 5.3700\n",
            "Step 100 avg train loss = 5.4337\n",
            "Step 200 avg train loss = 5.3993\n",
            "Step 300 avg train loss = 5.4125\n",
            "Step 400 avg train loss = 5.4128\n",
            "Step 500 avg train loss = 5.4004\n",
            "Step 600 avg train loss = 5.4129\n",
            "Step 700 avg train loss = 5.3832\n",
            "Step 800 avg train loss = 5.3888\n",
            "Step 900 avg train loss = 5.3802\n",
            "Step 1000 avg train loss = 5.4008\n",
            "Step 1100 avg train loss = 5.3652\n",
            "Step 1200 avg train loss = 5.3819\n",
            "Step 1300 avg train loss = 5.3645\n",
            "Step 1400 avg train loss = 5.3651\n",
            "Step 1500 avg train loss = 5.3479\n",
            "Step 1600 avg train loss = 5.3501\n",
            "Step 1700 avg train loss = 5.3341\n",
            "Step 1800 avg train loss = 5.3439\n",
            "Step 1900 avg train loss = 5.3335\n",
            "Step 2000 avg train loss = 5.3543\n",
            "Step 2100 avg train loss = 5.3197\n",
            "Step 2200 avg train loss = 5.3092\n",
            "Step 2300 avg train loss = 5.3198\n",
            "Step 2400 avg train loss = 5.3211\n",
            "Validation loss after 2 epoch = 5.3333\n",
            "Step 0 avg train loss = 5.1641\n",
            "Step 100 avg train loss = 5.1876\n",
            "Step 200 avg train loss = 5.1706\n",
            "Step 300 avg train loss = 5.1738\n",
            "Step 400 avg train loss = 5.1707\n",
            "Step 500 avg train loss = 5.1782\n",
            "Step 600 avg train loss = 5.1721\n",
            "Step 700 avg train loss = 5.1858\n",
            "Step 800 avg train loss = 5.1804\n",
            "Step 900 avg train loss = 5.1827\n",
            "Step 1000 avg train loss = 5.1771\n",
            "Step 1100 avg train loss = 5.1885\n",
            "Step 1200 avg train loss = 5.1812\n",
            "Step 1300 avg train loss = 5.1638\n",
            "Step 1400 avg train loss = 5.1602\n",
            "Step 1500 avg train loss = 5.1755\n",
            "Step 1600 avg train loss = 5.1627\n",
            "Step 1700 avg train loss = 5.1379\n",
            "Step 1800 avg train loss = 5.1525\n",
            "Step 1900 avg train loss = 5.1664\n",
            "Step 2000 avg train loss = 5.1349\n",
            "Step 2100 avg train loss = 5.1540\n",
            "Step 2200 avg train loss = 5.1496\n",
            "Step 2300 avg train loss = 5.1238\n",
            "Step 2400 avg train loss = 5.1435\n",
            "Validation loss after 3 epoch = 5.2613\n",
            "Step 0 avg train loss = 5.0862\n",
            "Step 100 avg train loss = 5.0027\n",
            "Step 200 avg train loss = 5.0167\n",
            "Step 300 avg train loss = 5.0061\n",
            "Step 400 avg train loss = 5.0147\n",
            "Step 500 avg train loss = 5.0007\n",
            "Step 600 avg train loss = 4.9987\n",
            "Step 700 avg train loss = 5.0205\n",
            "Step 800 avg train loss = 4.9979\n",
            "Step 900 avg train loss = 5.0138\n",
            "Step 1000 avg train loss = 5.0195\n",
            "Step 1100 avg train loss = 5.0022\n",
            "Step 1200 avg train loss = 5.0169\n",
            "Step 1300 avg train loss = 5.0220\n",
            "Step 1400 avg train loss = 5.0102\n",
            "Step 1500 avg train loss = 5.0159\n",
            "Step 1600 avg train loss = 5.0053\n",
            "Step 1700 avg train loss = 5.0121\n",
            "Step 1800 avg train loss = 5.0187\n",
            "Step 1900 avg train loss = 5.0028\n",
            "Step 2000 avg train loss = 5.0128\n",
            "Step 2100 avg train loss = 4.9970\n",
            "Step 2200 avg train loss = 5.0086\n",
            "Step 2300 avg train loss = 5.0139\n",
            "Step 2400 avg train loss = 4.9951\n",
            "Validation loss after 4 epoch = 5.2234\n",
            "Step 0 avg train loss = 4.7910\n",
            "Step 100 avg train loss = 4.8562\n",
            "Step 200 avg train loss = 4.8613\n",
            "Step 300 avg train loss = 4.8863\n",
            "Step 400 avg train loss = 4.8549\n",
            "Step 500 avg train loss = 4.8889\n",
            "Step 600 avg train loss = 4.8718\n",
            "Step 700 avg train loss = 4.8905\n",
            "Step 800 avg train loss = 4.8810\n",
            "Step 900 avg train loss = 4.8913\n",
            "Step 1000 avg train loss = 4.8911\n",
            "Step 1100 avg train loss = 4.8873\n",
            "Step 1200 avg train loss = 4.8806\n",
            "Step 1300 avg train loss = 4.8800\n",
            "Step 1400 avg train loss = 4.8919\n",
            "Step 1500 avg train loss = 4.8815\n",
            "Step 1600 avg train loss = 4.8875\n",
            "Step 1700 avg train loss = 4.8996\n",
            "Step 1800 avg train loss = 4.8842\n",
            "Step 1900 avg train loss = 4.8843\n",
            "Step 2000 avg train loss = 4.8781\n",
            "Step 2100 avg train loss = 4.8721\n",
            "Step 2200 avg train loss = 4.9032\n",
            "Step 2300 avg train loss = 4.8716\n",
            "Step 2400 avg train loss = 4.9014\n",
            "Validation loss after 5 epoch = 5.2085\n",
            "Step 0 avg train loss = 4.5154\n",
            "Step 100 avg train loss = 4.7395\n",
            "Step 200 avg train loss = 4.7420\n",
            "Step 300 avg train loss = 4.7591\n",
            "Step 400 avg train loss = 4.7462\n",
            "Step 500 avg train loss = 4.7480\n",
            "Step 600 avg train loss = 4.7729\n",
            "Step 700 avg train loss = 4.7864\n",
            "Step 800 avg train loss = 4.7813\n",
            "Step 900 avg train loss = 4.7982\n",
            "Step 1000 avg train loss = 4.7523\n",
            "Step 1100 avg train loss = 4.7426\n",
            "Step 1200 avg train loss = 4.7852\n",
            "Step 1300 avg train loss = 4.7630\n",
            "Step 1400 avg train loss = 4.7863\n",
            "Step 1500 avg train loss = 4.7631\n",
            "Step 1600 avg train loss = 4.7773\n",
            "Step 1700 avg train loss = 4.8014\n",
            "Step 1800 avg train loss = 4.7995\n",
            "Step 1900 avg train loss = 4.7921\n",
            "Step 2000 avg train loss = 4.7768\n",
            "Step 2100 avg train loss = 4.7744\n",
            "Step 2200 avg train loss = 4.8140\n",
            "Step 2300 avg train loss = 4.8047\n",
            "Step 2400 avg train loss = 4.8054\n",
            "Validation loss after 6 epoch = 5.2093\n",
            "Step 0 avg train loss = 4.5736\n",
            "Step 100 avg train loss = 4.6334\n",
            "Step 200 avg train loss = 4.6583\n",
            "Step 300 avg train loss = 4.6586\n",
            "Step 400 avg train loss = 4.6652\n",
            "Step 500 avg train loss = 4.6765\n",
            "Step 600 avg train loss = 4.6583\n",
            "Step 700 avg train loss = 4.6679\n",
            "Step 800 avg train loss = 4.6738\n",
            "Step 900 avg train loss = 4.6697\n",
            "Step 1000 avg train loss = 4.6871\n",
            "Step 1100 avg train loss = 4.6736\n",
            "Step 1200 avg train loss = 4.6892\n",
            "Step 1300 avg train loss = 4.6855\n",
            "Step 1400 avg train loss = 4.6756\n",
            "Step 1500 avg train loss = 4.7132\n",
            "Step 1600 avg train loss = 4.6891\n",
            "Step 1700 avg train loss = 4.7001\n",
            "Step 1800 avg train loss = 4.6795\n",
            "Step 1900 avg train loss = 4.7140\n",
            "Step 2000 avg train loss = 4.6911\n",
            "Step 2100 avg train loss = 4.6988\n",
            "Step 2200 avg train loss = 4.7135\n",
            "Step 2300 avg train loss = 4.7172\n",
            "Step 2400 avg train loss = 4.7030\n",
            "Validation loss after 7 epoch = 5.2206\n",
            "Step 0 avg train loss = 4.7209\n",
            "Step 100 avg train loss = 4.5570\n",
            "Step 200 avg train loss = 4.5865\n",
            "Step 300 avg train loss = 4.5709\n",
            "Step 400 avg train loss = 4.5668\n",
            "Step 500 avg train loss = 4.5801\n",
            "Step 600 avg train loss = 4.5937\n",
            "Step 700 avg train loss = 4.5678\n",
            "Step 800 avg train loss = 4.5914\n",
            "Step 900 avg train loss = 4.6029\n",
            "Step 1000 avg train loss = 4.5958\n",
            "Step 1100 avg train loss = 4.6013\n",
            "Step 1200 avg train loss = 4.6071\n",
            "Step 1300 avg train loss = 4.6439\n",
            "Step 1400 avg train loss = 4.6006\n",
            "Step 1500 avg train loss = 4.5940\n",
            "Step 1600 avg train loss = 4.6271\n",
            "Step 1700 avg train loss = 4.6006\n",
            "Step 1800 avg train loss = 4.6212\n",
            "Step 1900 avg train loss = 4.6221\n",
            "Step 2000 avg train loss = 4.6258\n",
            "Step 2100 avg train loss = 4.6108\n",
            "Step 2200 avg train loss = 4.6259\n",
            "Step 2300 avg train loss = 4.6351\n",
            "Step 2400 avg train loss = 4.6376\n",
            "Validation loss after 8 epoch = 5.2339\n",
            "Step 0 avg train loss = 4.5395\n",
            "Step 100 avg train loss = 4.4844\n",
            "Step 200 avg train loss = 4.4784\n",
            "Step 300 avg train loss = 4.5089\n",
            "Step 400 avg train loss = 4.4969\n",
            "Step 500 avg train loss = 4.5090\n",
            "Step 600 avg train loss = 4.5387\n",
            "Step 700 avg train loss = 4.5052\n",
            "Step 800 avg train loss = 4.5192\n",
            "Step 900 avg train loss = 4.5294\n",
            "Step 1000 avg train loss = 4.5147\n",
            "Step 1100 avg train loss = 4.5183\n",
            "Step 1200 avg train loss = 4.5238\n",
            "Step 1300 avg train loss = 4.5396\n",
            "Step 1400 avg train loss = 4.5302\n",
            "Step 1500 avg train loss = 4.5614\n",
            "Step 1600 avg train loss = 4.5436\n",
            "Step 1700 avg train loss = 4.5283\n",
            "Step 1800 avg train loss = 4.5534\n",
            "Step 1900 avg train loss = 4.5506\n",
            "Step 2000 avg train loss = 4.5634\n",
            "Step 2100 avg train loss = 4.5676\n",
            "Step 2200 avg train loss = 4.5539\n",
            "Step 2300 avg train loss = 4.5558\n",
            "Step 2400 avg train loss = 4.5609\n",
            "Validation loss after 9 epoch = 5.2535\n",
            "Step 0 avg train loss = 4.4260\n",
            "Step 100 avg train loss = 4.4039\n",
            "Step 200 avg train loss = 4.4179\n",
            "Step 300 avg train loss = 4.4186\n",
            "Step 400 avg train loss = 4.4270\n",
            "Step 500 avg train loss = 4.4493\n",
            "Step 600 avg train loss = 4.4389\n",
            "Step 700 avg train loss = 4.4444\n",
            "Step 800 avg train loss = 4.4455\n",
            "Step 900 avg train loss = 4.4573\n",
            "Step 1000 avg train loss = 4.4721\n",
            "Step 1100 avg train loss = 4.4426\n",
            "Step 1200 avg train loss = 4.4768\n",
            "Step 1300 avg train loss = 4.4696\n",
            "Step 1400 avg train loss = 4.4682\n",
            "Step 1500 avg train loss = 4.4698\n",
            "Step 1600 avg train loss = 4.5027\n",
            "Step 1700 avg train loss = 4.4927\n",
            "Step 1800 avg train loss = 4.4908\n",
            "Step 1900 avg train loss = 4.4720\n",
            "Step 2000 avg train loss = 4.4912\n",
            "Step 2100 avg train loss = 4.4981\n",
            "Step 2200 avg train loss = 4.4861\n",
            "Step 2300 avg train loss = 4.5120\n",
            "Step 2400 avg train loss = 4.5051\n",
            "Validation loss after 10 epoch = 5.2752\n",
            "Step 0 avg train loss = 4.1379\n",
            "Step 100 avg train loss = 4.3538\n",
            "Step 200 avg train loss = 4.3465\n",
            "Step 300 avg train loss = 4.3662\n",
            "Step 400 avg train loss = 4.3811\n",
            "Step 500 avg train loss = 4.3694\n",
            "Step 600 avg train loss = 4.3720\n",
            "Step 700 avg train loss = 4.3686\n",
            "Step 800 avg train loss = 4.3761\n",
            "Step 900 avg train loss = 4.3973\n",
            "Step 1000 avg train loss = 4.4346\n",
            "Step 1100 avg train loss = 4.4071\n",
            "Step 1200 avg train loss = 4.4056\n",
            "Step 1300 avg train loss = 4.4084\n",
            "Step 1400 avg train loss = 4.4134\n",
            "Step 1500 avg train loss = 4.4248\n",
            "Step 1600 avg train loss = 4.4239\n",
            "Step 1700 avg train loss = 4.4405\n",
            "Step 1800 avg train loss = 4.4399\n",
            "Step 1900 avg train loss = 4.4291\n",
            "Step 2000 avg train loss = 4.4387\n",
            "Step 2100 avg train loss = 4.4464\n",
            "Step 2200 avg train loss = 4.4417\n",
            "Step 2300 avg train loss = 4.4337\n",
            "Step 2400 avg train loss = 4.4449\n",
            "Validation loss after 11 epoch = 5.2981\n",
            "Step 0 avg train loss = 4.4015\n",
            "Step 100 avg train loss = 4.3116\n",
            "Step 200 avg train loss = 4.2860\n",
            "Step 300 avg train loss = 4.3145\n",
            "Step 400 avg train loss = 4.3321\n",
            "Step 500 avg train loss = 4.3110\n",
            "Step 600 avg train loss = 4.3394\n",
            "Step 700 avg train loss = 4.3310\n",
            "Step 800 avg train loss = 4.3234\n",
            "Step 900 avg train loss = 4.3438\n",
            "Step 1000 avg train loss = 4.3542\n",
            "Step 1100 avg train loss = 4.3503\n",
            "Step 1200 avg train loss = 4.3205\n",
            "Step 1300 avg train loss = 4.3513\n",
            "Step 1400 avg train loss = 4.3764\n",
            "Step 1500 avg train loss = 4.3456\n",
            "Step 1600 avg train loss = 4.3771\n",
            "Step 1700 avg train loss = 4.3615\n",
            "Step 1800 avg train loss = 4.3607\n",
            "Step 1900 avg train loss = 4.3651\n",
            "Step 2000 avg train loss = 4.4010\n",
            "Step 2100 avg train loss = 4.3963\n",
            "Step 2200 avg train loss = 4.4192\n",
            "Step 2300 avg train loss = 4.3878\n",
            "Step 2400 avg train loss = 4.3866\n",
            "Validation loss after 12 epoch = 5.3268\n",
            "Step 0 avg train loss = 4.1954\n",
            "Step 100 avg train loss = 4.2482\n",
            "Step 200 avg train loss = 4.2359\n",
            "Step 300 avg train loss = 4.2367\n",
            "Step 400 avg train loss = 4.2612\n",
            "Step 500 avg train loss = 4.2920\n",
            "Step 600 avg train loss = 4.2888\n",
            "Step 700 avg train loss = 4.2701\n",
            "Step 800 avg train loss = 4.2946\n",
            "Step 900 avg train loss = 4.3153\n",
            "Step 1000 avg train loss = 4.2971\n",
            "Step 1100 avg train loss = 4.2984\n",
            "Step 1200 avg train loss = 4.2932\n",
            "Step 1300 avg train loss = 4.3182\n",
            "Step 1400 avg train loss = 4.3139\n",
            "Step 1500 avg train loss = 4.3306\n",
            "Step 1600 avg train loss = 4.3260\n",
            "Step 1700 avg train loss = 4.3133\n",
            "Step 1800 avg train loss = 4.3404\n",
            "Step 1900 avg train loss = 4.3176\n",
            "Step 2000 avg train loss = 4.3522\n",
            "Step 2100 avg train loss = 4.3210\n",
            "Step 2200 avg train loss = 4.3386\n",
            "Step 2300 avg train loss = 4.3423\n",
            "Step 2400 avg train loss = 4.3232\n",
            "Validation loss after 13 epoch = 5.3515\n",
            "Step 0 avg train loss = 4.1868\n",
            "Step 100 avg train loss = 4.2284\n",
            "Step 200 avg train loss = 4.1772\n",
            "Step 300 avg train loss = 4.2114\n",
            "Step 400 avg train loss = 4.2367\n",
            "Step 500 avg train loss = 4.2289\n",
            "Step 600 avg train loss = 4.2441\n",
            "Step 700 avg train loss = 4.2209\n",
            "Step 800 avg train loss = 4.2631\n",
            "Step 900 avg train loss = 4.2440\n",
            "Step 1000 avg train loss = 4.2469\n",
            "Step 1100 avg train loss = 4.2578\n",
            "Step 1200 avg train loss = 4.2676\n",
            "Step 1300 avg train loss = 4.2573\n",
            "Step 1400 avg train loss = 4.2699\n",
            "Step 1500 avg train loss = 4.2751\n",
            "Step 1600 avg train loss = 4.2915\n",
            "Step 1700 avg train loss = 4.2617\n",
            "Step 1800 avg train loss = 4.2539\n",
            "Step 1900 avg train loss = 4.2760\n",
            "Step 2000 avg train loss = 4.2728\n",
            "Step 2100 avg train loss = 4.2873\n",
            "Step 2200 avg train loss = 4.2856\n",
            "Step 2300 avg train loss = 4.2880\n",
            "Step 2400 avg train loss = 4.2990\n",
            "Validation loss after 14 epoch = 5.3798\n",
            "Step 0 avg train loss = 3.9694\n",
            "Step 100 avg train loss = 4.1541\n",
            "Step 200 avg train loss = 4.1601\n",
            "Step 300 avg train loss = 4.1889\n",
            "Step 400 avg train loss = 4.1788\n",
            "Step 500 avg train loss = 4.1664\n",
            "Step 600 avg train loss = 4.1742\n",
            "Step 700 avg train loss = 4.2005\n",
            "Step 800 avg train loss = 4.2060\n",
            "Step 900 avg train loss = 4.2018\n",
            "Step 1000 avg train loss = 4.2024\n",
            "Step 1100 avg train loss = 4.2122\n",
            "Step 1200 avg train loss = 4.2456\n",
            "Step 1300 avg train loss = 4.2165\n",
            "Step 1400 avg train loss = 4.2209\n",
            "Step 1500 avg train loss = 4.2472\n",
            "Step 1600 avg train loss = 4.2171\n",
            "Step 1700 avg train loss = 4.2426\n",
            "Step 1800 avg train loss = 4.2255\n",
            "Step 1900 avg train loss = 4.2298\n",
            "Step 2000 avg train loss = 4.2223\n",
            "Step 2100 avg train loss = 4.2484\n",
            "Step 2200 avg train loss = 4.2681\n",
            "Step 2300 avg train loss = 4.2514\n",
            "Step 2400 avg train loss = 4.2473\n",
            "Validation loss after 15 epoch = 5.4066\n",
            "Step 0 avg train loss = 4.2376\n",
            "Step 100 avg train loss = 4.1166\n",
            "Step 200 avg train loss = 4.1233\n",
            "Step 300 avg train loss = 4.1392\n",
            "Step 400 avg train loss = 4.1489\n",
            "Step 500 avg train loss = 4.1407\n",
            "Step 600 avg train loss = 4.1554\n",
            "Step 700 avg train loss = 4.1312\n",
            "Step 800 avg train loss = 4.1651\n",
            "Step 900 avg train loss = 4.1784\n",
            "Step 1000 avg train loss = 4.1738\n",
            "Step 1100 avg train loss = 4.1455\n",
            "Step 1200 avg train loss = 4.1770\n",
            "Step 1300 avg train loss = 4.1742\n",
            "Step 1400 avg train loss = 4.1887\n",
            "Step 1500 avg train loss = 4.1778\n",
            "Step 1600 avg train loss = 4.1858\n",
            "Step 1700 avg train loss = 4.1826\n",
            "Step 1800 avg train loss = 4.1982\n",
            "Step 1900 avg train loss = 4.2017\n",
            "Step 2000 avg train loss = 4.2095\n",
            "Step 2100 avg train loss = 4.2138\n",
            "Step 2200 avg train loss = 4.2040\n",
            "Step 2300 avg train loss = 4.2178\n",
            "Step 2400 avg train loss = 4.2090\n",
            "Validation loss after 16 epoch = 5.4379\n",
            "Step 0 avg train loss = 4.0614\n",
            "Step 100 avg train loss = 4.0570\n",
            "Step 200 avg train loss = 4.0855\n",
            "Step 300 avg train loss = 4.0890\n",
            "Step 400 avg train loss = 4.1000\n",
            "Step 500 avg train loss = 4.1185\n",
            "Step 600 avg train loss = 4.0977\n",
            "Step 700 avg train loss = 4.1101\n",
            "Step 800 avg train loss = 4.1214\n",
            "Step 900 avg train loss = 4.1088\n",
            "Step 1000 avg train loss = 4.1242\n",
            "Step 1100 avg train loss = 4.1499\n",
            "Step 1200 avg train loss = 4.1463\n",
            "Step 1300 avg train loss = 4.1285\n",
            "Step 1400 avg train loss = 4.1380\n",
            "Step 1500 avg train loss = 4.1419\n",
            "Step 1600 avg train loss = 4.1527\n",
            "Step 1700 avg train loss = 4.1422\n",
            "Step 1800 avg train loss = 4.1489\n",
            "Step 1900 avg train loss = 4.1748\n",
            "Step 2000 avg train loss = 4.1730\n",
            "Step 2100 avg train loss = 4.1749\n",
            "Step 2200 avg train loss = 4.1967\n",
            "Step 2300 avg train loss = 4.1798\n",
            "Step 2400 avg train loss = 4.1927\n",
            "Validation loss after 17 epoch = 5.4691\n",
            "Step 0 avg train loss = 4.2660\n",
            "Step 100 avg train loss = 4.0254\n",
            "Step 200 avg train loss = 4.0464\n",
            "Step 300 avg train loss = 4.0588\n",
            "Step 400 avg train loss = 4.0721\n",
            "Step 500 avg train loss = 4.0575\n",
            "Step 600 avg train loss = 4.0670\n",
            "Step 700 avg train loss = 4.0978\n",
            "Step 800 avg train loss = 4.0892\n",
            "Step 900 avg train loss = 4.0724\n",
            "Step 1000 avg train loss = 4.1005\n",
            "Step 1100 avg train loss = 4.0947\n",
            "Step 1200 avg train loss = 4.1061\n",
            "Step 1300 avg train loss = 4.0877\n",
            "Step 1400 avg train loss = 4.1069\n",
            "Step 1500 avg train loss = 4.1205\n",
            "Step 1600 avg train loss = 4.1290\n",
            "Step 1700 avg train loss = 4.1129\n",
            "Step 1800 avg train loss = 4.1316\n",
            "Step 1900 avg train loss = 4.1349\n",
            "Step 2000 avg train loss = 4.1324\n",
            "Step 2100 avg train loss = 4.1205\n",
            "Step 2200 avg train loss = 4.1396\n",
            "Step 2300 avg train loss = 4.1578\n",
            "Step 2400 avg train loss = 4.1480\n",
            "Validation loss after 18 epoch = 5.4949\n",
            "Step 0 avg train loss = 4.0512\n",
            "Step 100 avg train loss = 3.9958\n",
            "Step 200 avg train loss = 4.0035\n",
            "Step 300 avg train loss = 4.0203\n",
            "Step 400 avg train loss = 4.0256\n",
            "Step 500 avg train loss = 4.0288\n",
            "Step 600 avg train loss = 4.0257\n",
            "Step 700 avg train loss = 4.0490\n",
            "Step 800 avg train loss = 4.0708\n",
            "Step 900 avg train loss = 4.0587\n",
            "Step 1000 avg train loss = 4.0584\n",
            "Step 1100 avg train loss = 4.0559\n",
            "Step 1200 avg train loss = 4.0658\n",
            "Step 1300 avg train loss = 4.0672\n",
            "Step 1400 avg train loss = 4.0846\n",
            "Step 1500 avg train loss = 4.0657\n",
            "Step 1600 avg train loss = 4.0857\n",
            "Step 1700 avg train loss = 4.1016\n",
            "Step 1800 avg train loss = 4.0952\n",
            "Step 1900 avg train loss = 4.1037\n",
            "Step 2000 avg train loss = 4.1114\n",
            "Step 2100 avg train loss = 4.0961\n",
            "Step 2200 avg train loss = 4.0982\n",
            "Step 2300 avg train loss = 4.0962\n",
            "Step 2400 avg train loss = 4.1322\n",
            "Validation loss after 19 epoch = 5.5245\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4132\n",
            "Step 100 avg train loss = 7.8714\n",
            "Step 200 avg train loss = 7.0718\n",
            "Step 300 avg train loss = 6.9174\n",
            "Step 400 avg train loss = 6.7383\n",
            "Step 500 avg train loss = 6.6138\n",
            "Step 600 avg train loss = 6.5480\n",
            "Step 700 avg train loss = 6.4684\n",
            "Step 800 avg train loss = 6.3962\n",
            "Step 900 avg train loss = 6.3300\n",
            "Step 1000 avg train loss = 6.2845\n",
            "Step 1100 avg train loss = 6.2470\n",
            "Step 1200 avg train loss = 6.1912\n",
            "Step 1300 avg train loss = 6.1466\n",
            "Step 1400 avg train loss = 6.0905\n",
            "Step 1500 avg train loss = 6.0833\n",
            "Step 1600 avg train loss = 6.0545\n",
            "Step 1700 avg train loss = 6.0238\n",
            "Step 1800 avg train loss = 6.0048\n",
            "Step 1900 avg train loss = 5.9594\n",
            "Step 2000 avg train loss = 5.9311\n",
            "Step 2100 avg train loss = 5.9381\n",
            "Step 2200 avg train loss = 5.8777\n",
            "Step 2300 avg train loss = 5.8600\n",
            "Step 2400 avg train loss = 5.8589\n",
            "Validation loss after 0 epoch = 5.6767\n",
            "Step 0 avg train loss = 5.7587\n",
            "Step 100 avg train loss = 5.7528\n",
            "Step 200 avg train loss = 5.7185\n",
            "Step 300 avg train loss = 5.7048\n",
            "Step 400 avg train loss = 5.7110\n",
            "Step 500 avg train loss = 5.7001\n",
            "Step 600 avg train loss = 5.7094\n",
            "Step 700 avg train loss = 5.6934\n",
            "Step 800 avg train loss = 5.6502\n",
            "Step 900 avg train loss = 5.6611\n",
            "Step 1000 avg train loss = 5.6310\n",
            "Step 1100 avg train loss = 5.6267\n",
            "Step 1200 avg train loss = 5.6277\n",
            "Step 1300 avg train loss = 5.5999\n",
            "Step 1400 avg train loss = 5.6039\n",
            "Step 1500 avg train loss = 5.5820\n",
            "Step 1600 avg train loss = 5.5850\n",
            "Step 1700 avg train loss = 5.5689\n",
            "Step 1800 avg train loss = 5.5302\n",
            "Step 1900 avg train loss = 5.5379\n",
            "Step 2000 avg train loss = 5.5269\n",
            "Step 2100 avg train loss = 5.5479\n",
            "Step 2200 avg train loss = 5.5299\n",
            "Step 2300 avg train loss = 5.5011\n",
            "Step 2400 avg train loss = 5.5004\n",
            "Validation loss after 1 epoch = 5.4238\n",
            "Step 0 avg train loss = 5.5506\n",
            "Step 100 avg train loss = 5.3963\n",
            "Step 200 avg train loss = 5.3657\n",
            "Step 300 avg train loss = 5.3674\n",
            "Step 400 avg train loss = 5.3633\n",
            "Step 500 avg train loss = 5.3702\n",
            "Step 600 avg train loss = 5.3783\n",
            "Step 700 avg train loss = 5.3514\n",
            "Step 800 avg train loss = 5.3522\n",
            "Step 900 avg train loss = 5.3666\n",
            "Step 1000 avg train loss = 5.3614\n",
            "Step 1100 avg train loss = 5.3267\n",
            "Step 1200 avg train loss = 5.3230\n",
            "Step 1300 avg train loss = 5.3262\n",
            "Step 1400 avg train loss = 5.3109\n",
            "Step 1500 avg train loss = 5.3222\n",
            "Step 1600 avg train loss = 5.2921\n",
            "Step 1700 avg train loss = 5.2936\n",
            "Step 1800 avg train loss = 5.2926\n",
            "Step 1900 avg train loss = 5.2965\n",
            "Step 2000 avg train loss = 5.2875\n",
            "Step 2100 avg train loss = 5.2914\n",
            "Step 2200 avg train loss = 5.2825\n",
            "Step 2300 avg train loss = 5.2815\n",
            "Step 2400 avg train loss = 5.2795\n",
            "Validation loss after 2 epoch = 5.2959\n",
            "Step 0 avg train loss = 5.0195\n",
            "Step 100 avg train loss = 5.1366\n",
            "Step 200 avg train loss = 5.1382\n",
            "Step 300 avg train loss = 5.1205\n",
            "Step 400 avg train loss = 5.1541\n",
            "Step 500 avg train loss = 5.1322\n",
            "Step 600 avg train loss = 5.1403\n",
            "Step 700 avg train loss = 5.1263\n",
            "Step 800 avg train loss = 5.1309\n",
            "Step 900 avg train loss = 5.1300\n",
            "Step 1000 avg train loss = 5.1390\n",
            "Step 1100 avg train loss = 5.1246\n",
            "Step 1200 avg train loss = 5.1436\n",
            "Step 1300 avg train loss = 5.1297\n",
            "Step 1400 avg train loss = 5.1331\n",
            "Step 1500 avg train loss = 5.1092\n",
            "Step 1600 avg train loss = 5.1036\n",
            "Step 1700 avg train loss = 5.1178\n",
            "Step 1800 avg train loss = 5.1242\n",
            "Step 1900 avg train loss = 5.1057\n",
            "Step 2000 avg train loss = 5.1104\n",
            "Step 2100 avg train loss = 5.0977\n",
            "Step 2200 avg train loss = 5.1044\n",
            "Step 2300 avg train loss = 5.1035\n",
            "Step 2400 avg train loss = 5.0999\n",
            "Validation loss after 3 epoch = 5.2366\n",
            "Step 0 avg train loss = 4.8921\n",
            "Step 100 avg train loss = 4.9432\n",
            "Step 200 avg train loss = 4.9525\n",
            "Step 300 avg train loss = 4.9740\n",
            "Step 400 avg train loss = 4.9508\n",
            "Step 500 avg train loss = 4.9661\n",
            "Step 600 avg train loss = 4.9629\n",
            "Step 700 avg train loss = 4.9784\n",
            "Step 800 avg train loss = 4.9607\n",
            "Step 900 avg train loss = 4.9889\n",
            "Step 1000 avg train loss = 4.9693\n",
            "Step 1100 avg train loss = 4.9704\n",
            "Step 1200 avg train loss = 4.9815\n",
            "Step 1300 avg train loss = 4.9598\n",
            "Step 1400 avg train loss = 4.9858\n",
            "Step 1500 avg train loss = 4.9625\n",
            "Step 1600 avg train loss = 4.9674\n",
            "Step 1700 avg train loss = 4.9663\n",
            "Step 1800 avg train loss = 4.9582\n",
            "Step 1900 avg train loss = 4.9630\n",
            "Step 2000 avg train loss = 4.9627\n",
            "Step 2100 avg train loss = 4.9805\n",
            "Step 2200 avg train loss = 4.9635\n",
            "Step 2300 avg train loss = 4.9540\n",
            "Step 2400 avg train loss = 4.9612\n",
            "Validation loss after 4 epoch = 5.2103\n",
            "Step 0 avg train loss = 4.6798\n",
            "Step 100 avg train loss = 4.8180\n",
            "Step 200 avg train loss = 4.8346\n",
            "Step 300 avg train loss = 4.8313\n",
            "Step 400 avg train loss = 4.8162\n",
            "Step 500 avg train loss = 4.8379\n",
            "Step 600 avg train loss = 4.8344\n",
            "Step 700 avg train loss = 4.8493\n",
            "Step 800 avg train loss = 4.8353\n",
            "Step 900 avg train loss = 4.8277\n",
            "Step 1000 avg train loss = 4.8371\n",
            "Step 1100 avg train loss = 4.8447\n",
            "Step 1200 avg train loss = 4.8419\n",
            "Step 1300 avg train loss = 4.8302\n",
            "Step 1400 avg train loss = 4.8531\n",
            "Step 1500 avg train loss = 4.8365\n",
            "Step 1600 avg train loss = 4.8542\n",
            "Step 1700 avg train loss = 4.8545\n",
            "Step 1800 avg train loss = 4.8356\n",
            "Step 1900 avg train loss = 4.8338\n",
            "Step 2000 avg train loss = 4.8203\n",
            "Step 2100 avg train loss = 4.8353\n",
            "Step 2200 avg train loss = 4.8453\n",
            "Step 2300 avg train loss = 4.8232\n",
            "Step 2400 avg train loss = 4.8604\n",
            "Validation loss after 5 epoch = 5.2010\n",
            "Step 0 avg train loss = 4.6998\n",
            "Step 100 avg train loss = 4.7235\n",
            "Step 200 avg train loss = 4.6999\n",
            "Step 300 avg train loss = 4.6949\n",
            "Step 400 avg train loss = 4.7107\n",
            "Step 500 avg train loss = 4.7160\n",
            "Step 600 avg train loss = 4.7304\n",
            "Step 700 avg train loss = 4.7101\n",
            "Step 800 avg train loss = 4.7077\n",
            "Step 900 avg train loss = 4.7209\n",
            "Step 1000 avg train loss = 4.7556\n",
            "Step 1100 avg train loss = 4.7351\n",
            "Step 1200 avg train loss = 4.7228\n",
            "Step 1300 avg train loss = 4.7430\n",
            "Step 1400 avg train loss = 4.7261\n",
            "Step 1500 avg train loss = 4.7591\n",
            "Step 1600 avg train loss = 4.7589\n",
            "Step 1700 avg train loss = 4.7402\n",
            "Step 1800 avg train loss = 4.7493\n",
            "Step 1900 avg train loss = 4.7287\n",
            "Step 2000 avg train loss = 4.7516\n",
            "Step 2100 avg train loss = 4.7284\n",
            "Step 2200 avg train loss = 4.7357\n",
            "Step 2300 avg train loss = 4.7516\n",
            "Step 2400 avg train loss = 4.7478\n",
            "Validation loss after 6 epoch = 5.2127\n",
            "Step 0 avg train loss = 4.4473\n",
            "Step 100 avg train loss = 4.6054\n",
            "Step 200 avg train loss = 4.6297\n",
            "Step 300 avg train loss = 4.6115\n",
            "Step 400 avg train loss = 4.6327\n",
            "Step 500 avg train loss = 4.6328\n",
            "Step 600 avg train loss = 4.6112\n",
            "Step 700 avg train loss = 4.6301\n",
            "Step 800 avg train loss = 4.6088\n",
            "Step 900 avg train loss = 4.6406\n",
            "Step 1000 avg train loss = 4.6494\n",
            "Step 1100 avg train loss = 4.6394\n",
            "Step 1200 avg train loss = 4.6366\n",
            "Step 1300 avg train loss = 4.6285\n",
            "Step 1400 avg train loss = 4.6434\n",
            "Step 1500 avg train loss = 4.6441\n",
            "Step 1600 avg train loss = 4.6587\n",
            "Step 1700 avg train loss = 4.6592\n",
            "Step 1800 avg train loss = 4.6515\n",
            "Step 1900 avg train loss = 4.6606\n",
            "Step 2000 avg train loss = 4.6479\n",
            "Step 2100 avg train loss = 4.6624\n",
            "Step 2200 avg train loss = 4.6377\n",
            "Step 2300 avg train loss = 4.6708\n",
            "Step 2400 avg train loss = 4.6581\n",
            "Validation loss after 7 epoch = 5.2190\n",
            "Step 0 avg train loss = 4.2927\n",
            "Step 100 avg train loss = 4.4935\n",
            "Step 200 avg train loss = 4.5162\n",
            "Step 300 avg train loss = 4.5264\n",
            "Step 400 avg train loss = 4.5315\n",
            "Step 500 avg train loss = 4.5391\n",
            "Step 600 avg train loss = 4.5384\n",
            "Step 700 avg train loss = 4.5460\n",
            "Step 800 avg train loss = 4.5488\n",
            "Step 900 avg train loss = 4.5770\n",
            "Step 1000 avg train loss = 4.5476\n",
            "Step 1100 avg train loss = 4.5600\n",
            "Step 1200 avg train loss = 4.5727\n",
            "Step 1300 avg train loss = 4.5778\n",
            "Step 1400 avg train loss = 4.5445\n",
            "Step 1500 avg train loss = 4.5699\n",
            "Step 1600 avg train loss = 4.5815\n",
            "Step 1700 avg train loss = 4.5835\n",
            "Step 1800 avg train loss = 4.5899\n",
            "Step 1900 avg train loss = 4.5956\n",
            "Step 2000 avg train loss = 4.5685\n",
            "Step 2100 avg train loss = 4.5766\n",
            "Step 2200 avg train loss = 4.6023\n",
            "Step 2300 avg train loss = 4.5796\n",
            "Step 2400 avg train loss = 4.5731\n",
            "Validation loss after 8 epoch = 5.2386\n",
            "Step 0 avg train loss = 4.4380\n",
            "Step 100 avg train loss = 4.4550\n",
            "Step 200 avg train loss = 4.4457\n",
            "Step 300 avg train loss = 4.4483\n",
            "Step 400 avg train loss = 4.4734\n",
            "Step 500 avg train loss = 4.4549\n",
            "Step 600 avg train loss = 4.4839\n",
            "Step 700 avg train loss = 4.4780\n",
            "Step 800 avg train loss = 4.4784\n",
            "Step 900 avg train loss = 4.5049\n",
            "Step 1000 avg train loss = 4.4631\n",
            "Step 1100 avg train loss = 4.4889\n",
            "Step 1200 avg train loss = 4.4929\n",
            "Step 1300 avg train loss = 4.4843\n",
            "Step 1400 avg train loss = 4.4992\n",
            "Step 1500 avg train loss = 4.4775\n",
            "Step 1600 avg train loss = 4.5039\n",
            "Step 1700 avg train loss = 4.5227\n",
            "Step 1800 avg train loss = 4.4995\n",
            "Step 1900 avg train loss = 4.5082\n",
            "Step 2000 avg train loss = 4.4815\n",
            "Step 2100 avg train loss = 4.5146\n",
            "Step 2200 avg train loss = 4.4961\n",
            "Step 2300 avg train loss = 4.5280\n",
            "Step 2400 avg train loss = 4.5080\n",
            "Validation loss after 9 epoch = 5.2582\n",
            "Step 0 avg train loss = 4.4055\n",
            "Step 100 avg train loss = 4.3765\n",
            "Step 200 avg train loss = 4.3724\n",
            "Step 300 avg train loss = 4.3788\n",
            "Step 400 avg train loss = 4.4054\n",
            "Step 500 avg train loss = 4.3940\n",
            "Step 600 avg train loss = 4.4031\n",
            "Step 700 avg train loss = 4.3958\n",
            "Step 800 avg train loss = 4.4139\n",
            "Step 900 avg train loss = 4.4374\n",
            "Step 1000 avg train loss = 4.4126\n",
            "Step 1100 avg train loss = 4.4152\n",
            "Step 1200 avg train loss = 4.4322\n",
            "Step 1300 avg train loss = 4.4198\n",
            "Step 1400 avg train loss = 4.4141\n",
            "Step 1500 avg train loss = 4.4266\n",
            "Step 1600 avg train loss = 4.4492\n",
            "Step 1700 avg train loss = 4.4342\n",
            "Step 1800 avg train loss = 4.4374\n",
            "Step 1900 avg train loss = 4.4539\n",
            "Step 2000 avg train loss = 4.4652\n",
            "Step 2100 avg train loss = 4.4614\n",
            "Step 2200 avg train loss = 4.4463\n",
            "Step 2300 avg train loss = 4.4552\n",
            "Step 2400 avg train loss = 4.4479\n",
            "Validation loss after 10 epoch = 5.2892\n",
            "Step 0 avg train loss = 4.3618\n",
            "Step 100 avg train loss = 4.3037\n",
            "Step 200 avg train loss = 4.3026\n",
            "Step 300 avg train loss = 4.3128\n",
            "Step 400 avg train loss = 4.3145\n",
            "Step 500 avg train loss = 4.3393\n",
            "Step 600 avg train loss = 4.3507\n",
            "Step 700 avg train loss = 4.3289\n",
            "Step 800 avg train loss = 4.3566\n",
            "Step 900 avg train loss = 4.3549\n",
            "Step 1000 avg train loss = 4.3586\n",
            "Step 1100 avg train loss = 4.3561\n",
            "Step 1200 avg train loss = 4.3660\n",
            "Step 1300 avg train loss = 4.3641\n",
            "Step 1400 avg train loss = 4.3784\n",
            "Step 1500 avg train loss = 4.3786\n",
            "Step 1600 avg train loss = 4.3619\n",
            "Step 1700 avg train loss = 4.3822\n",
            "Step 1800 avg train loss = 4.4005\n",
            "Step 1900 avg train loss = 4.4051\n",
            "Step 2000 avg train loss = 4.3930\n",
            "Step 2100 avg train loss = 4.3909\n",
            "Step 2200 avg train loss = 4.4177\n",
            "Step 2300 avg train loss = 4.4144\n",
            "Step 2400 avg train loss = 4.4052\n",
            "Validation loss after 11 epoch = 5.3104\n",
            "Step 0 avg train loss = 4.2222\n",
            "Step 100 avg train loss = 4.2431\n",
            "Step 200 avg train loss = 4.2607\n",
            "Step 300 avg train loss = 4.2780\n",
            "Step 400 avg train loss = 4.2829\n",
            "Step 500 avg train loss = 4.2874\n",
            "Step 600 avg train loss = 4.2693\n",
            "Step 700 avg train loss = 4.2963\n",
            "Step 800 avg train loss = 4.2916\n",
            "Step 900 avg train loss = 4.3065\n",
            "Step 1000 avg train loss = 4.3078\n",
            "Step 1100 avg train loss = 4.3065\n",
            "Step 1200 avg train loss = 4.3225\n",
            "Step 1300 avg train loss = 4.3066\n",
            "Step 1400 avg train loss = 4.3129\n",
            "Step 1500 avg train loss = 4.3258\n",
            "Step 1600 avg train loss = 4.3162\n",
            "Step 1700 avg train loss = 4.3184\n",
            "Step 1800 avg train loss = 4.3419\n",
            "Step 1900 avg train loss = 4.3227\n",
            "Step 2000 avg train loss = 4.3440\n",
            "Step 2100 avg train loss = 4.3308\n",
            "Step 2200 avg train loss = 4.3603\n",
            "Step 2300 avg train loss = 4.3454\n",
            "Step 2400 avg train loss = 4.3487\n",
            "Validation loss after 12 epoch = 5.3362\n",
            "Step 0 avg train loss = 4.2120\n",
            "Step 100 avg train loss = 4.1886\n",
            "Step 200 avg train loss = 4.1989\n",
            "Step 300 avg train loss = 4.2311\n",
            "Step 400 avg train loss = 4.2388\n",
            "Step 500 avg train loss = 4.2294\n",
            "Step 600 avg train loss = 4.2298\n",
            "Step 700 avg train loss = 4.2463\n",
            "Step 800 avg train loss = 4.2391\n",
            "Step 900 avg train loss = 4.2502\n",
            "Step 1000 avg train loss = 4.2688\n",
            "Step 1100 avg train loss = 4.2590\n",
            "Step 1200 avg train loss = 4.2649\n",
            "Step 1300 avg train loss = 4.2649\n",
            "Step 1400 avg train loss = 4.2731\n",
            "Step 1500 avg train loss = 4.2836\n",
            "Step 1600 avg train loss = 4.2629\n",
            "Step 1700 avg train loss = 4.2678\n",
            "Step 1800 avg train loss = 4.2923\n",
            "Step 1900 avg train loss = 4.2716\n",
            "Step 2000 avg train loss = 4.2761\n",
            "Step 2100 avg train loss = 4.3148\n",
            "Step 2200 avg train loss = 4.2881\n",
            "Step 2300 avg train loss = 4.2699\n",
            "Step 2400 avg train loss = 4.3139\n",
            "Validation loss after 13 epoch = 5.3644\n",
            "Step 0 avg train loss = 4.0577\n",
            "Step 100 avg train loss = 4.1405\n",
            "Step 200 avg train loss = 4.1497\n",
            "Step 300 avg train loss = 4.1604\n",
            "Step 400 avg train loss = 4.1758\n",
            "Step 500 avg train loss = 4.1736\n",
            "Step 600 avg train loss = 4.1823\n",
            "Step 700 avg train loss = 4.1856\n",
            "Step 800 avg train loss = 4.2179\n",
            "Step 900 avg train loss = 4.1874\n",
            "Step 1000 avg train loss = 4.1968\n",
            "Step 1100 avg train loss = 4.2222\n",
            "Step 1200 avg train loss = 4.2174\n",
            "Step 1300 avg train loss = 4.2175\n",
            "Step 1400 avg train loss = 4.2315\n",
            "Step 1500 avg train loss = 4.2371\n",
            "Step 1600 avg train loss = 4.2383\n",
            "Step 1700 avg train loss = 4.2266\n",
            "Step 1800 avg train loss = 4.2499\n",
            "Step 1900 avg train loss = 4.2366\n",
            "Step 2000 avg train loss = 4.2536\n",
            "Step 2100 avg train loss = 4.2439\n",
            "Step 2200 avg train loss = 4.2432\n",
            "Step 2300 avg train loss = 4.2592\n",
            "Step 2400 avg train loss = 4.2655\n",
            "Validation loss after 14 epoch = 5.3933\n",
            "Step 0 avg train loss = 4.0131\n",
            "Step 100 avg train loss = 4.1013\n",
            "Step 200 avg train loss = 4.0988\n",
            "Step 300 avg train loss = 4.1272\n",
            "Step 400 avg train loss = 4.1209\n",
            "Step 500 avg train loss = 4.1275\n",
            "Step 600 avg train loss = 4.1455\n",
            "Step 700 avg train loss = 4.1450\n",
            "Step 800 avg train loss = 4.1420\n",
            "Step 900 avg train loss = 4.1523\n",
            "Step 1000 avg train loss = 4.1660\n",
            "Step 1100 avg train loss = 4.1722\n",
            "Step 1200 avg train loss = 4.1657\n",
            "Step 1300 avg train loss = 4.1743\n",
            "Step 1400 avg train loss = 4.1944\n",
            "Step 1500 avg train loss = 4.2043\n",
            "Step 1600 avg train loss = 4.1986\n",
            "Step 1700 avg train loss = 4.1996\n",
            "Step 1800 avg train loss = 4.1933\n",
            "Step 1900 avg train loss = 4.1965\n",
            "Step 2000 avg train loss = 4.2143\n",
            "Step 2100 avg train loss = 4.1789\n",
            "Step 2200 avg train loss = 4.1946\n",
            "Step 2300 avg train loss = 4.2069\n",
            "Step 2400 avg train loss = 4.2358\n",
            "Validation loss after 15 epoch = 5.4246\n",
            "Step 0 avg train loss = 4.1673\n",
            "Step 100 avg train loss = 4.0547\n",
            "Step 200 avg train loss = 4.0693\n",
            "Step 300 avg train loss = 4.0738\n",
            "Step 400 avg train loss = 4.0841\n",
            "Step 500 avg train loss = 4.0999\n",
            "Step 600 avg train loss = 4.1131\n",
            "Step 700 avg train loss = 4.0992\n",
            "Step 800 avg train loss = 4.0871\n",
            "Step 900 avg train loss = 4.1142\n",
            "Step 1000 avg train loss = 4.1206\n",
            "Step 1100 avg train loss = 4.1252\n",
            "Step 1200 avg train loss = 4.1220\n",
            "Step 1300 avg train loss = 4.1445\n",
            "Step 1400 avg train loss = 4.1459\n",
            "Step 1500 avg train loss = 4.1337\n",
            "Step 1600 avg train loss = 4.1752\n",
            "Step 1700 avg train loss = 4.1666\n",
            "Step 1800 avg train loss = 4.1629\n",
            "Step 1900 avg train loss = 4.1651\n",
            "Step 2000 avg train loss = 4.1737\n",
            "Step 2100 avg train loss = 4.1539\n",
            "Step 2200 avg train loss = 4.1735\n",
            "Step 2300 avg train loss = 4.1523\n",
            "Step 2400 avg train loss = 4.1757\n",
            "Validation loss after 16 epoch = 5.4519\n",
            "Step 0 avg train loss = 3.9297\n",
            "Step 100 avg train loss = 4.0126\n",
            "Step 200 avg train loss = 4.0253\n",
            "Step 300 avg train loss = 4.0391\n",
            "Step 400 avg train loss = 4.0439\n",
            "Step 500 avg train loss = 4.0645\n",
            "Step 600 avg train loss = 4.0549\n",
            "Step 700 avg train loss = 4.0900\n",
            "Step 800 avg train loss = 4.0793\n",
            "Step 900 avg train loss = 4.0775\n",
            "Step 1000 avg train loss = 4.0889\n",
            "Step 1100 avg train loss = 4.0630\n",
            "Step 1200 avg train loss = 4.0886\n",
            "Step 1300 avg train loss = 4.1105\n",
            "Step 1400 avg train loss = 4.1256\n",
            "Step 1500 avg train loss = 4.1118\n",
            "Step 1600 avg train loss = 4.1088\n",
            "Step 1700 avg train loss = 4.1054\n",
            "Step 1800 avg train loss = 4.1225\n",
            "Step 1900 avg train loss = 4.1061\n",
            "Step 2000 avg train loss = 4.1228\n",
            "Step 2100 avg train loss = 4.1372\n",
            "Step 2200 avg train loss = 4.1323\n",
            "Step 2300 avg train loss = 4.1270\n",
            "Step 2400 avg train loss = 4.1334\n",
            "Validation loss after 17 epoch = 5.4837\n",
            "Step 0 avg train loss = 4.0440\n",
            "Step 100 avg train loss = 3.9772\n",
            "Step 200 avg train loss = 3.9905\n",
            "Step 300 avg train loss = 4.0043\n",
            "Step 400 avg train loss = 4.0068\n",
            "Step 500 avg train loss = 4.0165\n",
            "Step 600 avg train loss = 4.0387\n",
            "Step 700 avg train loss = 4.0457\n",
            "Step 800 avg train loss = 4.0179\n",
            "Step 900 avg train loss = 4.0531\n",
            "Step 1000 avg train loss = 4.0661\n",
            "Step 1100 avg train loss = 4.0614\n",
            "Step 1200 avg train loss = 4.0640\n",
            "Step 1300 avg train loss = 4.0802\n",
            "Step 1400 avg train loss = 4.0703\n",
            "Step 1500 avg train loss = 4.0460\n",
            "Step 1600 avg train loss = 4.0697\n",
            "Step 1700 avg train loss = 4.0769\n",
            "Step 1800 avg train loss = 4.0839\n",
            "Step 1900 avg train loss = 4.0770\n",
            "Step 2000 avg train loss = 4.0966\n",
            "Step 2100 avg train loss = 4.0775\n",
            "Step 2200 avg train loss = 4.0849\n",
            "Step 2300 avg train loss = 4.1133\n",
            "Step 2400 avg train loss = 4.0894\n",
            "Validation loss after 18 epoch = 5.5082\n",
            "Step 0 avg train loss = 3.9998\n",
            "Step 100 avg train loss = 3.9558\n",
            "Step 200 avg train loss = 3.9769\n",
            "Step 300 avg train loss = 3.9715\n",
            "Step 400 avg train loss = 3.9490\n",
            "Step 500 avg train loss = 3.9980\n",
            "Step 600 avg train loss = 4.0008\n",
            "Step 700 avg train loss = 3.9940\n",
            "Step 800 avg train loss = 4.0109\n",
            "Step 900 avg train loss = 4.0060\n",
            "Step 1000 avg train loss = 4.0087\n",
            "Step 1100 avg train loss = 4.0202\n",
            "Step 1200 avg train loss = 4.0125\n",
            "Step 1300 avg train loss = 4.0295\n",
            "Step 1400 avg train loss = 4.0254\n",
            "Step 1500 avg train loss = 4.0366\n",
            "Step 1600 avg train loss = 4.0455\n",
            "Step 1700 avg train loss = 4.0576\n",
            "Step 1800 avg train loss = 4.0540\n",
            "Step 1900 avg train loss = 4.0624\n",
            "Step 2000 avg train loss = 4.0473\n",
            "Step 2100 avg train loss = 4.0440\n",
            "Step 2200 avg train loss = 4.0534\n",
            "Step 2300 avg train loss = 4.0765\n",
            "Step 2400 avg train loss = 4.0659\n",
            "Validation loss after 19 epoch = 5.5362\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 400, padding_idx=2)\n",
            "  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4023\n",
            "Step 100 avg train loss = 7.8250\n",
            "Step 200 avg train loss = 7.0826\n",
            "Step 300 avg train loss = 6.8590\n",
            "Step 400 avg train loss = 6.7079\n",
            "Step 500 avg train loss = 6.6157\n",
            "Step 600 avg train loss = 6.5115\n",
            "Step 700 avg train loss = 6.4588\n",
            "Step 800 avg train loss = 6.3831\n",
            "Step 900 avg train loss = 6.3402\n",
            "Step 1000 avg train loss = 6.2948\n",
            "Step 1100 avg train loss = 6.2216\n",
            "Step 1200 avg train loss = 6.1758\n",
            "Step 1300 avg train loss = 6.1454\n",
            "Step 1400 avg train loss = 6.1283\n",
            "Step 1500 avg train loss = 6.0499\n",
            "Step 1600 avg train loss = 6.0247\n",
            "Step 1700 avg train loss = 6.0261\n",
            "Step 1800 avg train loss = 5.9836\n",
            "Step 1900 avg train loss = 5.9606\n",
            "Step 2000 avg train loss = 5.9391\n",
            "Step 2100 avg train loss = 5.9152\n",
            "Step 2200 avg train loss = 5.9022\n",
            "Step 2300 avg train loss = 5.8728\n",
            "Step 2400 avg train loss = 5.8519\n",
            "Validation loss after 0 epoch = 5.6733\n",
            "Step 0 avg train loss = 5.7023\n",
            "Step 100 avg train loss = 5.7328\n",
            "Step 200 avg train loss = 5.7244\n",
            "Step 300 avg train loss = 5.7013\n",
            "Step 400 avg train loss = 5.6960\n",
            "Step 500 avg train loss = 5.7002\n",
            "Step 600 avg train loss = 5.6611\n",
            "Step 700 avg train loss = 5.6550\n",
            "Step 800 avg train loss = 5.6492\n",
            "Step 900 avg train loss = 5.6442\n",
            "Step 1000 avg train loss = 5.6368\n",
            "Step 1100 avg train loss = 5.6299\n",
            "Step 1200 avg train loss = 5.5984\n",
            "Step 1300 avg train loss = 5.5849\n",
            "Step 1400 avg train loss = 5.5605\n",
            "Step 1500 avg train loss = 5.5807\n",
            "Step 1600 avg train loss = 5.5840\n",
            "Step 1700 avg train loss = 5.5296\n",
            "Step 1800 avg train loss = 5.5472\n",
            "Step 1900 avg train loss = 5.5346\n",
            "Step 2000 avg train loss = 5.5158\n",
            "Step 2100 avg train loss = 5.5128\n",
            "Step 2200 avg train loss = 5.5088\n",
            "Step 2300 avg train loss = 5.4913\n",
            "Step 2400 avg train loss = 5.4905\n",
            "Validation loss after 1 epoch = 5.4120\n",
            "Step 0 avg train loss = 5.3730\n",
            "Step 100 avg train loss = 5.3563\n",
            "Step 200 avg train loss = 5.3515\n",
            "Step 300 avg train loss = 5.3500\n",
            "Step 400 avg train loss = 5.3400\n",
            "Step 500 avg train loss = 5.3386\n",
            "Step 600 avg train loss = 5.3290\n",
            "Step 700 avg train loss = 5.3296\n",
            "Step 800 avg train loss = 5.3612\n",
            "Step 900 avg train loss = 5.3523\n",
            "Step 1000 avg train loss = 5.3082\n",
            "Step 1100 avg train loss = 5.3089\n",
            "Step 1200 avg train loss = 5.3216\n",
            "Step 1300 avg train loss = 5.2978\n",
            "Step 1400 avg train loss = 5.3079\n",
            "Step 1500 avg train loss = 5.3146\n",
            "Step 1600 avg train loss = 5.2861\n",
            "Step 1700 avg train loss = 5.2960\n",
            "Step 1800 avg train loss = 5.2927\n",
            "Step 1900 avg train loss = 5.2708\n",
            "Step 2000 avg train loss = 5.2605\n",
            "Step 2100 avg train loss = 5.2534\n",
            "Step 2200 avg train loss = 5.2604\n",
            "Step 2300 avg train loss = 5.2743\n",
            "Step 2400 avg train loss = 5.2619\n",
            "Validation loss after 2 epoch = 5.2892\n",
            "Step 0 avg train loss = 5.1294\n",
            "Step 100 avg train loss = 5.1191\n",
            "Step 200 avg train loss = 5.1321\n",
            "Step 300 avg train loss = 5.1095\n",
            "Step 400 avg train loss = 5.1091\n",
            "Step 500 avg train loss = 5.1229\n",
            "Step 600 avg train loss = 5.1146\n",
            "Step 700 avg train loss = 5.1116\n",
            "Step 800 avg train loss = 5.1080\n",
            "Step 900 avg train loss = 5.1183\n",
            "Step 1000 avg train loss = 5.1070\n",
            "Step 1100 avg train loss = 5.0849\n",
            "Step 1200 avg train loss = 5.1083\n",
            "Step 1300 avg train loss = 5.0937\n",
            "Step 1400 avg train loss = 5.1064\n",
            "Step 1500 avg train loss = 5.0944\n",
            "Step 1600 avg train loss = 5.0899\n",
            "Step 1700 avg train loss = 5.1022\n",
            "Step 1800 avg train loss = 5.0878\n",
            "Step 1900 avg train loss = 5.0824\n",
            "Step 2000 avg train loss = 5.0657\n",
            "Step 2100 avg train loss = 5.1331\n",
            "Step 2200 avg train loss = 5.1054\n",
            "Step 2300 avg train loss = 5.0902\n",
            "Step 2400 avg train loss = 5.0703\n",
            "Validation loss after 3 epoch = 5.2324\n",
            "Step 0 avg train loss = 5.0786\n",
            "Step 100 avg train loss = 4.9287\n",
            "Step 200 avg train loss = 4.9257\n",
            "Step 300 avg train loss = 4.9590\n",
            "Step 400 avg train loss = 4.9518\n",
            "Step 500 avg train loss = 4.9382\n",
            "Step 600 avg train loss = 4.9572\n",
            "Step 700 avg train loss = 4.9459\n",
            "Step 800 avg train loss = 4.9467\n",
            "Step 900 avg train loss = 4.9565\n",
            "Step 1000 avg train loss = 4.9423\n",
            "Step 1100 avg train loss = 4.9230\n",
            "Step 1200 avg train loss = 4.9397\n",
            "Step 1300 avg train loss = 4.9609\n",
            "Step 1400 avg train loss = 4.9469\n",
            "Step 1500 avg train loss = 4.9456\n",
            "Step 1600 avg train loss = 4.9634\n",
            "Step 1700 avg train loss = 4.9259\n",
            "Step 1800 avg train loss = 4.9511\n",
            "Step 1900 avg train loss = 4.9285\n",
            "Step 2000 avg train loss = 4.9521\n",
            "Step 2100 avg train loss = 4.9684\n",
            "Step 2200 avg train loss = 4.9520\n",
            "Step 2300 avg train loss = 4.9504\n",
            "Step 2400 avg train loss = 4.9484\n",
            "Validation loss after 4 epoch = 5.2095\n",
            "Step 0 avg train loss = 4.7277\n",
            "Step 100 avg train loss = 4.8169\n",
            "Step 200 avg train loss = 4.8071\n",
            "Step 300 avg train loss = 4.8053\n",
            "Step 400 avg train loss = 4.8019\n",
            "Step 500 avg train loss = 4.8195\n",
            "Step 600 avg train loss = 4.8038\n",
            "Step 700 avg train loss = 4.8275\n",
            "Step 800 avg train loss = 4.8186\n",
            "Step 900 avg train loss = 4.8242\n",
            "Step 1000 avg train loss = 4.8007\n",
            "Step 1100 avg train loss = 4.8289\n",
            "Step 1200 avg train loss = 4.8057\n",
            "Step 1300 avg train loss = 4.8150\n",
            "Step 1400 avg train loss = 4.8164\n",
            "Step 1500 avg train loss = 4.8503\n",
            "Step 1600 avg train loss = 4.8219\n",
            "Step 1700 avg train loss = 4.8282\n",
            "Step 1800 avg train loss = 4.8472\n",
            "Step 1900 avg train loss = 4.8101\n",
            "Step 2000 avg train loss = 4.8301\n",
            "Step 2100 avg train loss = 4.8470\n",
            "Step 2200 avg train loss = 4.8083\n",
            "Step 2300 avg train loss = 4.8183\n",
            "Step 2400 avg train loss = 4.8298\n",
            "Validation loss after 5 epoch = 5.2055\n",
            "Step 0 avg train loss = 4.6173\n",
            "Step 100 avg train loss = 4.6568\n",
            "Step 200 avg train loss = 4.6956\n",
            "Step 300 avg train loss = 4.6825\n",
            "Step 400 avg train loss = 4.6839\n",
            "Step 500 avg train loss = 4.6862\n",
            "Step 600 avg train loss = 4.6935\n",
            "Step 700 avg train loss = 4.7037\n",
            "Step 800 avg train loss = 4.7234\n",
            "Step 900 avg train loss = 4.7086\n",
            "Step 1000 avg train loss = 4.7280\n",
            "Step 1100 avg train loss = 4.7211\n",
            "Step 1200 avg train loss = 4.7215\n",
            "Step 1300 avg train loss = 4.7231\n",
            "Step 1400 avg train loss = 4.7352\n",
            "Step 1500 avg train loss = 4.7258\n",
            "Step 1600 avg train loss = 4.7247\n",
            "Step 1700 avg train loss = 4.7210\n",
            "Step 1800 avg train loss = 4.7167\n",
            "Step 1900 avg train loss = 4.7216\n",
            "Step 2000 avg train loss = 4.7386\n",
            "Step 2100 avg train loss = 4.7191\n",
            "Step 2200 avg train loss = 4.7308\n",
            "Step 2300 avg train loss = 4.7362\n",
            "Step 2400 avg train loss = 4.7295\n",
            "Validation loss after 6 epoch = 5.2143\n",
            "Step 0 avg train loss = 4.4967\n",
            "Step 100 avg train loss = 4.5824\n",
            "Step 200 avg train loss = 4.5770\n",
            "Step 300 avg train loss = 4.6073\n",
            "Step 400 avg train loss = 4.6023\n",
            "Step 500 avg train loss = 4.5821\n",
            "Step 600 avg train loss = 4.6027\n",
            "Step 700 avg train loss = 4.6088\n",
            "Step 800 avg train loss = 4.6071\n",
            "Step 900 avg train loss = 4.6067\n",
            "Step 1000 avg train loss = 4.6221\n",
            "Step 1100 avg train loss = 4.6301\n",
            "Step 1200 avg train loss = 4.6302\n",
            "Step 1300 avg train loss = 4.6181\n",
            "Step 1400 avg train loss = 4.6172\n",
            "Step 1500 avg train loss = 4.6505\n",
            "Step 1600 avg train loss = 4.6231\n",
            "Step 1700 avg train loss = 4.6233\n",
            "Step 1800 avg train loss = 4.6405\n",
            "Step 1900 avg train loss = 4.6459\n",
            "Step 2000 avg train loss = 4.6493\n",
            "Step 2100 avg train loss = 4.6637\n",
            "Step 2200 avg train loss = 4.6425\n",
            "Step 2300 avg train loss = 4.6438\n",
            "Step 2400 avg train loss = 4.6461\n",
            "Validation loss after 7 epoch = 5.2345\n",
            "Step 0 avg train loss = 4.5057\n",
            "Step 100 avg train loss = 4.4964\n",
            "Step 200 avg train loss = 4.5077\n",
            "Step 300 avg train loss = 4.5161\n",
            "Step 400 avg train loss = 4.5153\n",
            "Step 500 avg train loss = 4.5260\n",
            "Step 600 avg train loss = 4.5185\n",
            "Step 700 avg train loss = 4.5190\n",
            "Step 800 avg train loss = 4.5210\n",
            "Step 900 avg train loss = 4.5432\n",
            "Step 1000 avg train loss = 4.5360\n",
            "Step 1100 avg train loss = 4.5590\n",
            "Step 1200 avg train loss = 4.5447\n",
            "Step 1300 avg train loss = 4.5484\n",
            "Step 1400 avg train loss = 4.5494\n",
            "Step 1500 avg train loss = 4.5325\n",
            "Step 1600 avg train loss = 4.5551\n",
            "Step 1700 avg train loss = 4.5559\n",
            "Step 1800 avg train loss = 4.5546\n",
            "Step 1900 avg train loss = 4.5681\n",
            "Step 2000 avg train loss = 4.5703\n",
            "Step 2100 avg train loss = 4.5479\n",
            "Step 2200 avg train loss = 4.5708\n",
            "Step 2300 avg train loss = 4.5673\n",
            "Step 2400 avg train loss = 4.5588\n",
            "Validation loss after 8 epoch = 5.2575\n",
            "Step 0 avg train loss = 4.4800\n",
            "Step 100 avg train loss = 4.4264\n",
            "Step 200 avg train loss = 4.4187\n",
            "Step 300 avg train loss = 4.4199\n",
            "Step 400 avg train loss = 4.4340\n",
            "Step 500 avg train loss = 4.4527\n",
            "Step 600 avg train loss = 4.4404\n",
            "Step 700 avg train loss = 4.4569\n",
            "Step 800 avg train loss = 4.4562\n",
            "Step 900 avg train loss = 4.4733\n",
            "Step 1000 avg train loss = 4.4654\n",
            "Step 1100 avg train loss = 4.4655\n",
            "Step 1200 avg train loss = 4.5027\n",
            "Step 1300 avg train loss = 4.4709\n",
            "Step 1400 avg train loss = 4.4676\n",
            "Step 1500 avg train loss = 4.4754\n",
            "Step 1600 avg train loss = 4.4837\n",
            "Step 1700 avg train loss = 4.4964\n",
            "Step 1800 avg train loss = 4.4928\n",
            "Step 1900 avg train loss = 4.4742\n",
            "Step 2000 avg train loss = 4.4999\n",
            "Step 2100 avg train loss = 4.4681\n",
            "Step 2200 avg train loss = 4.4924\n",
            "Step 2300 avg train loss = 4.4783\n",
            "Step 2400 avg train loss = 4.4943\n",
            "Validation loss after 9 epoch = 5.2754\n",
            "Step 0 avg train loss = 4.2336\n",
            "Step 100 avg train loss = 4.3511\n",
            "Step 200 avg train loss = 4.3681\n",
            "Step 300 avg train loss = 4.3397\n",
            "Step 400 avg train loss = 4.3713\n",
            "Step 500 avg train loss = 4.3796\n",
            "Step 600 avg train loss = 4.3920\n",
            "Step 700 avg train loss = 4.3778\n",
            "Step 800 avg train loss = 4.3816\n",
            "Step 900 avg train loss = 4.3859\n",
            "Step 1000 avg train loss = 4.4069\n",
            "Step 1100 avg train loss = 4.4107\n",
            "Step 1200 avg train loss = 4.4034\n",
            "Step 1300 avg train loss = 4.3945\n",
            "Step 1400 avg train loss = 4.4096\n",
            "Step 1500 avg train loss = 4.4292\n",
            "Step 1600 avg train loss = 4.4149\n",
            "Step 1700 avg train loss = 4.4073\n",
            "Step 1800 avg train loss = 4.4219\n",
            "Step 1900 avg train loss = 4.4168\n",
            "Step 2000 avg train loss = 4.4408\n",
            "Step 2100 avg train loss = 4.4232\n",
            "Step 2200 avg train loss = 4.4429\n",
            "Step 2300 avg train loss = 4.4264\n",
            "Step 2400 avg train loss = 4.4314\n",
            "Validation loss after 10 epoch = 5.3052\n",
            "Step 0 avg train loss = 4.0679\n",
            "Step 100 avg train loss = 4.2737\n",
            "Step 200 avg train loss = 4.2835\n",
            "Step 300 avg train loss = 4.3065\n",
            "Step 400 avg train loss = 4.3031\n",
            "Step 500 avg train loss = 4.3113\n",
            "Step 600 avg train loss = 4.3217\n",
            "Step 700 avg train loss = 4.3066\n",
            "Step 800 avg train loss = 4.3148\n",
            "Step 900 avg train loss = 4.3189\n",
            "Step 1000 avg train loss = 4.3407\n",
            "Step 1100 avg train loss = 4.3562\n",
            "Step 1200 avg train loss = 4.3371\n",
            "Step 1300 avg train loss = 4.3485\n",
            "Step 1400 avg train loss = 4.3647\n",
            "Step 1500 avg train loss = 4.3523\n",
            "Step 1600 avg train loss = 4.3602\n",
            "Step 1700 avg train loss = 4.3741\n",
            "Step 1800 avg train loss = 4.3640\n",
            "Step 1900 avg train loss = 4.3903\n",
            "Step 2000 avg train loss = 4.3647\n",
            "Step 2100 avg train loss = 4.3828\n",
            "Step 2200 avg train loss = 4.3678\n",
            "Step 2300 avg train loss = 4.3636\n",
            "Step 2400 avg train loss = 4.3780\n",
            "Validation loss after 11 epoch = 5.3374\n",
            "Step 0 avg train loss = 4.1806\n",
            "Step 100 avg train loss = 4.2266\n",
            "Step 200 avg train loss = 4.2401\n",
            "Step 300 avg train loss = 4.2223\n",
            "Step 400 avg train loss = 4.2504\n",
            "Step 500 avg train loss = 4.2576\n",
            "Step 600 avg train loss = 4.2517\n",
            "Step 700 avg train loss = 4.2779\n",
            "Step 800 avg train loss = 4.2727\n",
            "Step 900 avg train loss = 4.2747\n",
            "Step 1000 avg train loss = 4.2642\n",
            "Step 1100 avg train loss = 4.2626\n",
            "Step 1200 avg train loss = 4.2924\n",
            "Step 1300 avg train loss = 4.2941\n",
            "Step 1400 avg train loss = 4.2995\n",
            "Step 1500 avg train loss = 4.2985\n",
            "Step 1600 avg train loss = 4.3283\n",
            "Step 1700 avg train loss = 4.3317\n",
            "Step 1800 avg train loss = 4.3120\n",
            "Step 1900 avg train loss = 4.3245\n",
            "Step 2000 avg train loss = 4.3050\n",
            "Step 2100 avg train loss = 4.3261\n",
            "Step 2200 avg train loss = 4.3321\n",
            "Step 2300 avg train loss = 4.3333\n",
            "Step 2400 avg train loss = 4.2995\n",
            "Validation loss after 12 epoch = 5.3689\n",
            "Step 0 avg train loss = 3.9983\n",
            "Step 100 avg train loss = 4.1634\n",
            "Step 200 avg train loss = 4.1947\n",
            "Step 300 avg train loss = 4.2063\n",
            "Step 400 avg train loss = 4.2014\n",
            "Step 500 avg train loss = 4.1926\n",
            "Step 600 avg train loss = 4.2148\n",
            "Step 700 avg train loss = 4.2078\n",
            "Step 800 avg train loss = 4.2368\n",
            "Step 900 avg train loss = 4.2238\n",
            "Step 1000 avg train loss = 4.2245\n",
            "Step 1100 avg train loss = 4.2284\n",
            "Step 1200 avg train loss = 4.2186\n",
            "Step 1300 avg train loss = 4.2408\n",
            "Step 1400 avg train loss = 4.2398\n",
            "Step 1500 avg train loss = 4.2612\n",
            "Step 1600 avg train loss = 4.2374\n",
            "Step 1700 avg train loss = 4.2797\n",
            "Step 1800 avg train loss = 4.2634\n",
            "Step 1900 avg train loss = 4.2838\n",
            "Step 2000 avg train loss = 4.2659\n",
            "Step 2100 avg train loss = 4.2516\n",
            "Step 2200 avg train loss = 4.2725\n",
            "Step 2300 avg train loss = 4.2687\n",
            "Step 2400 avg train loss = 4.2819\n",
            "Validation loss after 13 epoch = 5.3945\n",
            "Step 0 avg train loss = 4.0700\n",
            "Step 100 avg train loss = 4.1193\n",
            "Step 200 avg train loss = 4.1229\n",
            "Step 300 avg train loss = 4.1187\n",
            "Step 400 avg train loss = 4.1553\n",
            "Step 500 avg train loss = 4.1485\n",
            "Step 600 avg train loss = 4.1475\n",
            "Step 700 avg train loss = 4.1741\n",
            "Step 800 avg train loss = 4.1715\n",
            "Step 900 avg train loss = 4.1588\n",
            "Step 1000 avg train loss = 4.1828\n",
            "Step 1100 avg train loss = 4.2007\n",
            "Step 1200 avg train loss = 4.1766\n",
            "Step 1300 avg train loss = 4.2048\n",
            "Step 1400 avg train loss = 4.2149\n",
            "Step 1500 avg train loss = 4.2264\n",
            "Step 1600 avg train loss = 4.2042\n",
            "Step 1700 avg train loss = 4.2115\n",
            "Step 1800 avg train loss = 4.2200\n",
            "Step 1900 avg train loss = 4.1981\n",
            "Step 2000 avg train loss = 4.2197\n",
            "Step 2100 avg train loss = 4.2273\n",
            "Step 2200 avg train loss = 4.2347\n",
            "Step 2300 avg train loss = 4.2447\n",
            "Step 2400 avg train loss = 4.2305\n",
            "Validation loss after 14 epoch = 5.4292\n",
            "Step 0 avg train loss = 4.1498\n",
            "Step 100 avg train loss = 4.0650\n",
            "Step 200 avg train loss = 4.0763\n",
            "Step 300 avg train loss = 4.0947\n",
            "Step 400 avg train loss = 4.0923\n",
            "Step 500 avg train loss = 4.0890\n",
            "Step 600 avg train loss = 4.1183\n",
            "Step 700 avg train loss = 4.1046\n",
            "Step 800 avg train loss = 4.1245\n",
            "Step 900 avg train loss = 4.1294\n",
            "Step 1000 avg train loss = 4.1524\n",
            "Step 1100 avg train loss = 4.1669\n",
            "Step 1200 avg train loss = 4.1534\n",
            "Step 1300 avg train loss = 4.1338\n",
            "Step 1400 avg train loss = 4.1487\n",
            "Step 1500 avg train loss = 4.1673\n",
            "Step 1600 avg train loss = 4.1707\n",
            "Step 1700 avg train loss = 4.1834\n",
            "Step 1800 avg train loss = 4.1754\n",
            "Step 1900 avg train loss = 4.1873\n",
            "Step 2000 avg train loss = 4.1936\n",
            "Step 2100 avg train loss = 4.1599\n",
            "Step 2200 avg train loss = 4.1868\n",
            "Step 2300 avg train loss = 4.2023\n",
            "Step 2400 avg train loss = 4.1757\n",
            "Validation loss after 15 epoch = 5.4660\n",
            "Step 0 avg train loss = 4.0273\n",
            "Step 100 avg train loss = 4.0220\n",
            "Step 200 avg train loss = 4.0549\n",
            "Step 300 avg train loss = 4.0614\n",
            "Step 400 avg train loss = 4.0537\n",
            "Step 500 avg train loss = 4.0514\n",
            "Step 600 avg train loss = 4.0865\n",
            "Step 700 avg train loss = 4.0546\n",
            "Step 800 avg train loss = 4.0985\n",
            "Step 900 avg train loss = 4.1136\n",
            "Step 1000 avg train loss = 4.0781\n",
            "Step 1100 avg train loss = 4.1131\n",
            "Step 1200 avg train loss = 4.1265\n",
            "Step 1300 avg train loss = 4.1093\n",
            "Step 1400 avg train loss = 4.0922\n",
            "Step 1500 avg train loss = 4.1161\n",
            "Step 1600 avg train loss = 4.1181\n",
            "Step 1700 avg train loss = 4.1335\n",
            "Step 1800 avg train loss = 4.1290\n",
            "Step 1900 avg train loss = 4.1249\n",
            "Step 2000 avg train loss = 4.1470\n",
            "Step 2100 avg train loss = 4.1427\n",
            "Step 2200 avg train loss = 4.1465\n",
            "Step 2300 avg train loss = 4.1532\n",
            "Step 2400 avg train loss = 4.1267\n",
            "Validation loss after 16 epoch = 5.4926\n",
            "Step 0 avg train loss = 4.0648\n",
            "Step 100 avg train loss = 3.9927\n",
            "Step 200 avg train loss = 3.9955\n",
            "Step 300 avg train loss = 3.9995\n",
            "Step 400 avg train loss = 4.0166\n",
            "Step 500 avg train loss = 4.0329\n",
            "Step 600 avg train loss = 4.0125\n",
            "Step 700 avg train loss = 4.0438\n",
            "Step 800 avg train loss = 4.0544\n",
            "Step 900 avg train loss = 4.0643\n",
            "Step 1000 avg train loss = 4.0626\n",
            "Step 1100 avg train loss = 4.0750\n",
            "Step 1200 avg train loss = 4.0636\n",
            "Step 1300 avg train loss = 4.0804\n",
            "Step 1400 avg train loss = 4.0882\n",
            "Step 1500 avg train loss = 4.0944\n",
            "Step 1600 avg train loss = 4.0834\n",
            "Step 1700 avg train loss = 4.1077\n",
            "Step 1800 avg train loss = 4.0756\n",
            "Step 1900 avg train loss = 4.0918\n",
            "Step 2000 avg train loss = 4.0872\n",
            "Step 2100 avg train loss = 4.0858\n",
            "Step 2200 avg train loss = 4.0997\n",
            "Step 2300 avg train loss = 4.1257\n",
            "Step 2400 avg train loss = 4.1121\n",
            "Validation loss after 17 epoch = 5.5328\n",
            "Step 0 avg train loss = 3.9079\n",
            "Step 100 avg train loss = 3.9593\n",
            "Step 200 avg train loss = 3.9547\n",
            "Step 300 avg train loss = 3.9700\n",
            "Step 400 avg train loss = 4.0013\n",
            "Step 500 avg train loss = 3.9737\n",
            "Step 600 avg train loss = 3.9953\n",
            "Step 700 avg train loss = 4.0029\n",
            "Step 800 avg train loss = 4.0087\n",
            "Step 900 avg train loss = 4.0131\n",
            "Step 1000 avg train loss = 4.0181\n",
            "Step 1100 avg train loss = 4.0232\n",
            "Step 1200 avg train loss = 4.0233\n",
            "Step 1300 avg train loss = 4.0317\n",
            "Step 1400 avg train loss = 4.0616\n",
            "Step 1500 avg train loss = 4.0395\n",
            "Step 1600 avg train loss = 4.0304\n",
            "Step 1700 avg train loss = 4.0634\n",
            "Step 1800 avg train loss = 4.0594\n",
            "Step 1900 avg train loss = 4.0613\n",
            "Step 2000 avg train loss = 4.0799\n",
            "Step 2100 avg train loss = 4.0716\n",
            "Step 2200 avg train loss = 4.0788\n",
            "Step 2300 avg train loss = 4.0840\n",
            "Step 2400 avg train loss = 4.0640\n",
            "Validation loss after 18 epoch = 5.5573\n",
            "Step 0 avg train loss = 3.9781\n",
            "Step 100 avg train loss = 3.9200\n",
            "Step 200 avg train loss = 3.9366\n",
            "Step 300 avg train loss = 3.9145\n",
            "Step 400 avg train loss = 3.9230\n",
            "Step 500 avg train loss = 3.9579\n",
            "Step 600 avg train loss = 3.9589\n",
            "Step 700 avg train loss = 3.9782\n",
            "Step 800 avg train loss = 3.9825\n",
            "Step 900 avg train loss = 4.0048\n",
            "Step 1000 avg train loss = 3.9992\n",
            "Step 1100 avg train loss = 3.9936\n",
            "Step 1200 avg train loss = 4.0119\n",
            "Step 1300 avg train loss = 4.0024\n",
            "Step 1400 avg train loss = 4.0293\n",
            "Step 1500 avg train loss = 4.0182\n",
            "Step 1600 avg train loss = 4.0272\n",
            "Step 1700 avg train loss = 4.0067\n",
            "Step 1800 avg train loss = 4.0090\n",
            "Step 1900 avg train loss = 4.0225\n",
            "Step 2000 avg train loss = 4.0266\n",
            "Step 2100 avg train loss = 4.0301\n",
            "Step 2200 avg train loss = 4.0415\n",
            "Step 2300 avg train loss = 4.0565\n",
            "Step 2400 avg train loss = 4.0321\n",
            "Validation loss after 19 epoch = 5.5895\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 500, padding_idx=2)\n",
            "  (lstm): LSTM(500, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4104\n",
            "Step 100 avg train loss = 7.8140\n",
            "Step 200 avg train loss = 7.1067\n",
            "Step 300 avg train loss = 6.9097\n",
            "Step 400 avg train loss = 6.7177\n",
            "Step 500 avg train loss = 6.5911\n",
            "Step 600 avg train loss = 6.5139\n",
            "Step 700 avg train loss = 6.4147\n",
            "Step 800 avg train loss = 6.3649\n",
            "Step 900 avg train loss = 6.3038\n",
            "Step 1000 avg train loss = 6.2399\n",
            "Step 1100 avg train loss = 6.1846\n",
            "Step 1200 avg train loss = 6.1361\n",
            "Step 1300 avg train loss = 6.1052\n",
            "Step 1400 avg train loss = 6.0749\n",
            "Step 1500 avg train loss = 6.0425\n",
            "Step 1600 avg train loss = 6.0194\n",
            "Step 1700 avg train loss = 5.9650\n",
            "Step 1800 avg train loss = 5.9335\n",
            "Step 1900 avg train loss = 5.9212\n",
            "Step 2000 avg train loss = 5.8957\n",
            "Step 2100 avg train loss = 5.8850\n",
            "Step 2200 avg train loss = 5.8551\n",
            "Step 2300 avg train loss = 5.8543\n",
            "Step 2400 avg train loss = 5.8184\n",
            "Validation loss after 0 epoch = 5.6485\n",
            "Step 0 avg train loss = 5.8281\n",
            "Step 100 avg train loss = 5.7175\n",
            "Step 200 avg train loss = 5.7072\n",
            "Step 300 avg train loss = 5.6913\n",
            "Step 400 avg train loss = 5.6363\n",
            "Step 500 avg train loss = 5.6574\n",
            "Step 600 avg train loss = 5.6538\n",
            "Step 700 avg train loss = 5.6532\n",
            "Step 800 avg train loss = 5.6453\n",
            "Step 900 avg train loss = 5.6162\n",
            "Step 1000 avg train loss = 5.6160\n",
            "Step 1100 avg train loss = 5.5937\n",
            "Step 1200 avg train loss = 5.5852\n",
            "Step 1300 avg train loss = 5.5843\n",
            "Step 1400 avg train loss = 5.5713\n",
            "Step 1500 avg train loss = 5.5722\n",
            "Step 1600 avg train loss = 5.5399\n",
            "Step 1700 avg train loss = 5.5537\n",
            "Step 1800 avg train loss = 5.5514\n",
            "Step 1900 avg train loss = 5.5409\n",
            "Step 2000 avg train loss = 5.5255\n",
            "Step 2100 avg train loss = 5.4920\n",
            "Step 2200 avg train loss = 5.5099\n",
            "Step 2300 avg train loss = 5.4721\n",
            "Step 2400 avg train loss = 5.4798\n",
            "Validation loss after 1 epoch = 5.4089\n",
            "Step 0 avg train loss = 5.4125\n",
            "Step 100 avg train loss = 5.3591\n",
            "Step 200 avg train loss = 5.3388\n",
            "Step 300 avg train loss = 5.3616\n",
            "Step 400 avg train loss = 5.3527\n",
            "Step 500 avg train loss = 5.3481\n",
            "Step 600 avg train loss = 5.3444\n",
            "Step 700 avg train loss = 5.3326\n",
            "Step 800 avg train loss = 5.3339\n",
            "Step 900 avg train loss = 5.3152\n",
            "Step 1000 avg train loss = 5.3282\n",
            "Step 1100 avg train loss = 5.3124\n",
            "Step 1200 avg train loss = 5.3176\n",
            "Step 1300 avg train loss = 5.2889\n",
            "Step 1400 avg train loss = 5.3018\n",
            "Step 1500 avg train loss = 5.3203\n",
            "Step 1600 avg train loss = 5.2899\n",
            "Step 1700 avg train loss = 5.2979\n",
            "Step 1800 avg train loss = 5.3036\n",
            "Step 1900 avg train loss = 5.3054\n",
            "Step 2000 avg train loss = 5.2615\n",
            "Step 2100 avg train loss = 5.2804\n",
            "Step 2200 avg train loss = 5.2600\n",
            "Step 2300 avg train loss = 5.2708\n",
            "Step 2400 avg train loss = 5.2673\n",
            "Validation loss after 2 epoch = 5.3074\n",
            "Step 0 avg train loss = 5.0767\n",
            "Step 100 avg train loss = 5.1266\n",
            "Step 200 avg train loss = 5.1151\n",
            "Step 300 avg train loss = 5.1240\n",
            "Step 400 avg train loss = 5.1216\n",
            "Step 500 avg train loss = 5.1241\n",
            "Step 600 avg train loss = 5.1294\n",
            "Step 700 avg train loss = 5.0884\n",
            "Step 800 avg train loss = 5.1153\n",
            "Step 900 avg train loss = 5.1383\n",
            "Step 1000 avg train loss = 5.1243\n",
            "Step 1100 avg train loss = 5.1352\n",
            "Step 1200 avg train loss = 5.1264\n",
            "Step 1300 avg train loss = 5.1205\n",
            "Step 1400 avg train loss = 5.1058\n",
            "Step 1500 avg train loss = 5.1228\n",
            "Step 1600 avg train loss = 5.1138\n",
            "Step 1700 avg train loss = 5.0996\n",
            "Step 1800 avg train loss = 5.0850\n",
            "Step 1900 avg train loss = 5.1024\n",
            "Step 2000 avg train loss = 5.1001\n",
            "Step 2100 avg train loss = 5.0977\n",
            "Step 2200 avg train loss = 5.1067\n",
            "Step 2300 avg train loss = 5.1118\n",
            "Step 2400 avg train loss = 5.0855\n",
            "Validation loss after 3 epoch = 5.2516\n",
            "Step 0 avg train loss = 4.8916\n",
            "Step 100 avg train loss = 4.9340\n",
            "Step 200 avg train loss = 4.9251\n",
            "Step 300 avg train loss = 4.9643\n",
            "Step 400 avg train loss = 4.9438\n",
            "Step 500 avg train loss = 4.9536\n",
            "Step 600 avg train loss = 4.9610\n",
            "Step 700 avg train loss = 4.9635\n",
            "Step 800 avg train loss = 4.9538\n",
            "Step 900 avg train loss = 4.9599\n",
            "Step 1000 avg train loss = 4.9603\n",
            "Step 1100 avg train loss = 4.9615\n",
            "Step 1200 avg train loss = 4.9598\n",
            "Step 1300 avg train loss = 4.9628\n",
            "Step 1400 avg train loss = 4.9625\n",
            "Step 1500 avg train loss = 4.9733\n",
            "Step 1600 avg train loss = 4.9727\n",
            "Step 1700 avg train loss = 4.9474\n",
            "Step 1800 avg train loss = 4.9597\n",
            "Step 1900 avg train loss = 4.9616\n",
            "Step 2000 avg train loss = 4.9618\n",
            "Step 2100 avg train loss = 4.9740\n",
            "Step 2200 avg train loss = 4.9539\n",
            "Step 2300 avg train loss = 4.9563\n",
            "Step 2400 avg train loss = 4.9462\n",
            "Validation loss after 4 epoch = 5.2255\n",
            "Step 0 avg train loss = 4.8789\n",
            "Step 100 avg train loss = 4.8030\n",
            "Step 200 avg train loss = 4.7898\n",
            "Step 300 avg train loss = 4.8188\n",
            "Step 400 avg train loss = 4.7895\n",
            "Step 500 avg train loss = 4.8342\n",
            "Step 600 avg train loss = 4.8247\n",
            "Step 700 avg train loss = 4.8340\n",
            "Step 800 avg train loss = 4.8207\n",
            "Step 900 avg train loss = 4.8372\n",
            "Step 1000 avg train loss = 4.8188\n",
            "Step 1100 avg train loss = 4.8174\n",
            "Step 1200 avg train loss = 4.8070\n",
            "Step 1300 avg train loss = 4.8299\n",
            "Step 1400 avg train loss = 4.8296\n",
            "Step 1500 avg train loss = 4.8325\n",
            "Step 1600 avg train loss = 4.8128\n",
            "Step 1700 avg train loss = 4.8312\n",
            "Step 1800 avg train loss = 4.8536\n",
            "Step 1900 avg train loss = 4.8203\n",
            "Step 2000 avg train loss = 4.8576\n",
            "Step 2100 avg train loss = 4.8289\n",
            "Step 2200 avg train loss = 4.8614\n",
            "Step 2300 avg train loss = 4.8477\n",
            "Step 2400 avg train loss = 4.8676\n",
            "Validation loss after 5 epoch = 5.2284\n",
            "Step 0 avg train loss = 4.5773\n",
            "Step 100 avg train loss = 4.6798\n",
            "Step 200 avg train loss = 4.7082\n",
            "Step 300 avg train loss = 4.6748\n",
            "Step 400 avg train loss = 4.7208\n",
            "Step 500 avg train loss = 4.6965\n",
            "Step 600 avg train loss = 4.7072\n",
            "Step 700 avg train loss = 4.7018\n",
            "Step 800 avg train loss = 4.7185\n",
            "Step 900 avg train loss = 4.7290\n",
            "Step 1000 avg train loss = 4.7098\n",
            "Step 1100 avg train loss = 4.7224\n",
            "Step 1200 avg train loss = 4.7319\n",
            "Step 1300 avg train loss = 4.7295\n",
            "Step 1400 avg train loss = 4.7316\n",
            "Step 1500 avg train loss = 4.7341\n",
            "Step 1600 avg train loss = 4.7190\n",
            "Step 1700 avg train loss = 4.7110\n",
            "Step 1800 avg train loss = 4.7179\n",
            "Step 1900 avg train loss = 4.7443\n",
            "Step 2000 avg train loss = 4.7150\n",
            "Step 2100 avg train loss = 4.7446\n",
            "Step 2200 avg train loss = 4.7489\n",
            "Step 2300 avg train loss = 4.7345\n",
            "Step 2400 avg train loss = 4.7348\n",
            "Validation loss after 6 epoch = 5.2331\n",
            "Step 0 avg train loss = 4.8415\n",
            "Step 100 avg train loss = 4.5721\n",
            "Step 200 avg train loss = 4.5975\n",
            "Step 300 avg train loss = 4.6016\n",
            "Step 400 avg train loss = 4.5927\n",
            "Step 500 avg train loss = 4.6195\n",
            "Step 600 avg train loss = 4.6020\n",
            "Step 700 avg train loss = 4.6284\n",
            "Step 800 avg train loss = 4.6068\n",
            "Step 900 avg train loss = 4.6244\n",
            "Step 1000 avg train loss = 4.6261\n",
            "Step 1100 avg train loss = 4.6268\n",
            "Step 1200 avg train loss = 4.6069\n",
            "Step 1300 avg train loss = 4.6294\n",
            "Step 1400 avg train loss = 4.6227\n",
            "Step 1500 avg train loss = 4.6318\n",
            "Step 1600 avg train loss = 4.6364\n",
            "Step 1700 avg train loss = 4.6440\n",
            "Step 1800 avg train loss = 4.6583\n",
            "Step 1900 avg train loss = 4.6565\n",
            "Step 2000 avg train loss = 4.6325\n",
            "Step 2100 avg train loss = 4.6490\n",
            "Step 2200 avg train loss = 4.6406\n",
            "Step 2300 avg train loss = 4.6380\n",
            "Step 2400 avg train loss = 4.6417\n",
            "Validation loss after 7 epoch = 5.2567\n",
            "Step 0 avg train loss = 4.5436\n",
            "Step 100 avg train loss = 4.4922\n",
            "Step 200 avg train loss = 4.4902\n",
            "Step 300 avg train loss = 4.5199\n",
            "Step 400 avg train loss = 4.5199\n",
            "Step 500 avg train loss = 4.5139\n",
            "Step 600 avg train loss = 4.5100\n",
            "Step 700 avg train loss = 4.5091\n",
            "Step 800 avg train loss = 4.5325\n",
            "Step 900 avg train loss = 4.5307\n",
            "Step 1000 avg train loss = 4.5282\n",
            "Step 1100 avg train loss = 4.5397\n",
            "Step 1200 avg train loss = 4.5475\n",
            "Step 1300 avg train loss = 4.5560\n",
            "Step 1400 avg train loss = 4.5652\n",
            "Step 1500 avg train loss = 4.5537\n",
            "Step 1600 avg train loss = 4.5599\n",
            "Step 1700 avg train loss = 4.5678\n",
            "Step 1800 avg train loss = 4.5613\n",
            "Step 1900 avg train loss = 4.5457\n",
            "Step 2000 avg train loss = 4.5818\n",
            "Step 2100 avg train loss = 4.5654\n",
            "Step 2200 avg train loss = 4.5745\n",
            "Step 2300 avg train loss = 4.5649\n",
            "Step 2400 avg train loss = 4.5682\n",
            "Validation loss after 8 epoch = 5.2793\n",
            "Step 0 avg train loss = 4.1581\n",
            "Step 100 avg train loss = 4.3905\n",
            "Step 200 avg train loss = 4.4177\n",
            "Step 300 avg train loss = 4.4165\n",
            "Step 400 avg train loss = 4.4440\n",
            "Step 500 avg train loss = 4.4392\n",
            "Step 600 avg train loss = 4.4577\n",
            "Step 700 avg train loss = 4.4636\n",
            "Step 800 avg train loss = 4.4497\n",
            "Step 900 avg train loss = 4.4558\n",
            "Step 1000 avg train loss = 4.4725\n",
            "Step 1100 avg train loss = 4.4647\n",
            "Step 1200 avg train loss = 4.4746\n",
            "Step 1300 avg train loss = 4.4898\n",
            "Step 1400 avg train loss = 4.4802\n",
            "Step 1500 avg train loss = 4.4724\n",
            "Step 1600 avg train loss = 4.4944\n",
            "Step 1700 avg train loss = 4.4806\n",
            "Step 1800 avg train loss = 4.4810\n",
            "Step 1900 avg train loss = 4.5078\n",
            "Step 2000 avg train loss = 4.4798\n",
            "Step 2100 avg train loss = 4.4784\n",
            "Step 2200 avg train loss = 4.4975\n",
            "Step 2300 avg train loss = 4.5086\n",
            "Step 2400 avg train loss = 4.5145\n",
            "Validation loss after 9 epoch = 5.3026\n",
            "Step 0 avg train loss = 4.3879\n",
            "Step 100 avg train loss = 4.3463\n",
            "Step 200 avg train loss = 4.3374\n",
            "Step 300 avg train loss = 4.3608\n",
            "Step 400 avg train loss = 4.3780\n",
            "Step 500 avg train loss = 4.3618\n",
            "Step 600 avg train loss = 4.3782\n",
            "Step 700 avg train loss = 4.3804\n",
            "Step 800 avg train loss = 4.3948\n",
            "Step 900 avg train loss = 4.3831\n",
            "Step 1000 avg train loss = 4.3858\n",
            "Step 1100 avg train loss = 4.3975\n",
            "Step 1200 avg train loss = 4.4239\n",
            "Step 1300 avg train loss = 4.4028\n",
            "Step 1400 avg train loss = 4.3959\n",
            "Step 1500 avg train loss = 4.4076\n",
            "Step 1600 avg train loss = 4.4203\n",
            "Step 1700 avg train loss = 4.4108\n",
            "Step 1800 avg train loss = 4.4262\n",
            "Step 1900 avg train loss = 4.4142\n",
            "Step 2000 avg train loss = 4.4230\n",
            "Step 2100 avg train loss = 4.4381\n",
            "Step 2200 avg train loss = 4.4578\n",
            "Step 2300 avg train loss = 4.4329\n",
            "Step 2400 avg train loss = 4.4334\n",
            "Validation loss after 10 epoch = 5.3355\n",
            "Step 0 avg train loss = 4.1833\n",
            "Step 100 avg train loss = 4.2862\n",
            "Step 200 avg train loss = 4.2857\n",
            "Step 300 avg train loss = 4.2926\n",
            "Step 400 avg train loss = 4.2991\n",
            "Step 500 avg train loss = 4.3355\n",
            "Step 600 avg train loss = 4.3363\n",
            "Step 700 avg train loss = 4.3314\n",
            "Step 800 avg train loss = 4.3262\n",
            "Step 900 avg train loss = 4.3328\n",
            "Step 1000 avg train loss = 4.3232\n",
            "Step 1100 avg train loss = 4.3425\n",
            "Step 1200 avg train loss = 4.3417\n",
            "Step 1300 avg train loss = 4.3405\n",
            "Step 1400 avg train loss = 4.3359\n",
            "Step 1500 avg train loss = 4.3534\n",
            "Step 1600 avg train loss = 4.3499\n",
            "Step 1700 avg train loss = 4.3575\n",
            "Step 1800 avg train loss = 4.3692\n",
            "Step 1900 avg train loss = 4.3625\n",
            "Step 2000 avg train loss = 4.3554\n",
            "Step 2100 avg train loss = 4.3652\n",
            "Step 2200 avg train loss = 4.3823\n",
            "Step 2300 avg train loss = 4.3568\n",
            "Step 2400 avg train loss = 4.3828\n",
            "Validation loss after 11 epoch = 5.3643\n",
            "Step 0 avg train loss = 3.9015\n",
            "Step 100 avg train loss = 4.2156\n",
            "Step 200 avg train loss = 4.2325\n",
            "Step 300 avg train loss = 4.2363\n",
            "Step 400 avg train loss = 4.2456\n",
            "Step 500 avg train loss = 4.2602\n",
            "Step 600 avg train loss = 4.2600\n",
            "Step 700 avg train loss = 4.2783\n",
            "Step 800 avg train loss = 4.2670\n",
            "Step 900 avg train loss = 4.2730\n",
            "Step 1000 avg train loss = 4.2804\n",
            "Step 1100 avg train loss = 4.2945\n",
            "Step 1200 avg train loss = 4.2802\n",
            "Step 1300 avg train loss = 4.2655\n",
            "Step 1400 avg train loss = 4.2836\n",
            "Step 1500 avg train loss = 4.3148\n",
            "Step 1600 avg train loss = 4.3065\n",
            "Step 1700 avg train loss = 4.3043\n",
            "Step 1800 avg train loss = 4.3097\n",
            "Step 1900 avg train loss = 4.3220\n",
            "Step 2000 avg train loss = 4.2960\n",
            "Step 2100 avg train loss = 4.3156\n",
            "Step 2200 avg train loss = 4.3082\n",
            "Step 2300 avg train loss = 4.3340\n",
            "Step 2400 avg train loss = 4.3314\n",
            "Validation loss after 12 epoch = 5.4011\n",
            "Step 0 avg train loss = 4.2097\n",
            "Step 100 avg train loss = 4.1546\n",
            "Step 200 avg train loss = 4.1888\n",
            "Step 300 avg train loss = 4.2055\n",
            "Step 400 avg train loss = 4.1842\n",
            "Step 500 avg train loss = 4.1929\n",
            "Step 600 avg train loss = 4.1959\n",
            "Step 700 avg train loss = 4.2058\n",
            "Step 800 avg train loss = 4.2134\n",
            "Step 900 avg train loss = 4.2351\n",
            "Step 1000 avg train loss = 4.2454\n",
            "Step 1100 avg train loss = 4.2207\n",
            "Step 1200 avg train loss = 4.2419\n",
            "Step 1300 avg train loss = 4.2376\n",
            "Step 1400 avg train loss = 4.2236\n",
            "Step 1500 avg train loss = 4.2341\n",
            "Step 1600 avg train loss = 4.2361\n",
            "Step 1700 avg train loss = 4.2438\n",
            "Step 1800 avg train loss = 4.2593\n",
            "Step 1900 avg train loss = 4.2627\n",
            "Step 2000 avg train loss = 4.2648\n",
            "Step 2100 avg train loss = 4.2647\n",
            "Step 2200 avg train loss = 4.2852\n",
            "Step 2300 avg train loss = 4.2904\n",
            "Step 2400 avg train loss = 4.2588\n",
            "Validation loss after 13 epoch = 5.4329\n",
            "Step 0 avg train loss = 3.9968\n",
            "Step 100 avg train loss = 4.1199\n",
            "Step 200 avg train loss = 4.1053\n",
            "Step 300 avg train loss = 4.1341\n",
            "Step 400 avg train loss = 4.1318\n",
            "Step 500 avg train loss = 4.1580\n",
            "Step 600 avg train loss = 4.1598\n",
            "Step 700 avg train loss = 4.1517\n",
            "Step 800 avg train loss = 4.1523\n",
            "Step 900 avg train loss = 4.1582\n",
            "Step 1000 avg train loss = 4.1723\n",
            "Step 1100 avg train loss = 4.1901\n",
            "Step 1200 avg train loss = 4.1927\n",
            "Step 1300 avg train loss = 4.1922\n",
            "Step 1400 avg train loss = 4.2144\n",
            "Step 1500 avg train loss = 4.2117\n",
            "Step 1600 avg train loss = 4.2004\n",
            "Step 1700 avg train loss = 4.2220\n",
            "Step 1800 avg train loss = 4.2221\n",
            "Step 1900 avg train loss = 4.2202\n",
            "Step 2000 avg train loss = 4.1999\n",
            "Step 2100 avg train loss = 4.2317\n",
            "Step 2200 avg train loss = 4.2206\n",
            "Step 2300 avg train loss = 4.2274\n",
            "Step 2400 avg train loss = 4.2367\n",
            "Validation loss after 14 epoch = 5.4609\n",
            "Step 0 avg train loss = 4.1878\n",
            "Step 100 avg train loss = 4.0696\n",
            "Step 200 avg train loss = 4.0643\n",
            "Step 300 avg train loss = 4.1074\n",
            "Step 400 avg train loss = 4.1168\n",
            "Step 500 avg train loss = 4.1210\n",
            "Step 600 avg train loss = 4.1072\n",
            "Step 700 avg train loss = 4.1094\n",
            "Step 800 avg train loss = 4.1289\n",
            "Step 900 avg train loss = 4.1048\n",
            "Step 1000 avg train loss = 4.1479\n",
            "Step 1100 avg train loss = 4.1435\n",
            "Step 1200 avg train loss = 4.1333\n",
            "Step 1300 avg train loss = 4.1374\n",
            "Step 1400 avg train loss = 4.1694\n",
            "Step 1500 avg train loss = 4.1767\n",
            "Step 1600 avg train loss = 4.1574\n",
            "Step 1700 avg train loss = 4.1465\n",
            "Step 1800 avg train loss = 4.1458\n",
            "Step 1900 avg train loss = 4.1754\n",
            "Step 2000 avg train loss = 4.1502\n",
            "Step 2100 avg train loss = 4.1709\n",
            "Step 2200 avg train loss = 4.1770\n",
            "Step 2300 avg train loss = 4.1867\n",
            "Step 2400 avg train loss = 4.1882\n",
            "Validation loss after 15 epoch = 5.4983\n",
            "Step 0 avg train loss = 4.1218\n",
            "Step 100 avg train loss = 4.0318\n",
            "Step 200 avg train loss = 4.0299\n",
            "Step 300 avg train loss = 4.0406\n",
            "Step 400 avg train loss = 4.0497\n",
            "Step 500 avg train loss = 4.0483\n",
            "Step 600 avg train loss = 4.0781\n",
            "Step 700 avg train loss = 4.0655\n",
            "Step 800 avg train loss = 4.0720\n",
            "Step 900 avg train loss = 4.0732\n",
            "Step 1000 avg train loss = 4.0911\n",
            "Step 1100 avg train loss = 4.1002\n",
            "Step 1200 avg train loss = 4.0899\n",
            "Step 1300 avg train loss = 4.0931\n",
            "Step 1400 avg train loss = 4.1244\n",
            "Step 1500 avg train loss = 4.1223\n",
            "Step 1600 avg train loss = 4.1194\n",
            "Step 1700 avg train loss = 4.1170\n",
            "Step 1800 avg train loss = 4.1561\n",
            "Step 1900 avg train loss = 4.1252\n",
            "Step 2000 avg train loss = 4.1402\n",
            "Step 2100 avg train loss = 4.1437\n",
            "Step 2200 avg train loss = 4.1330\n",
            "Step 2300 avg train loss = 4.1470\n",
            "Step 2400 avg train loss = 4.1463\n",
            "Validation loss after 16 epoch = 5.5334\n",
            "Step 0 avg train loss = 3.7316\n",
            "Step 100 avg train loss = 4.0013\n",
            "Step 200 avg train loss = 3.9938\n",
            "Step 300 avg train loss = 4.0058\n",
            "Step 400 avg train loss = 4.0110\n",
            "Step 500 avg train loss = 4.0128\n",
            "Step 600 avg train loss = 4.0476\n",
            "Step 700 avg train loss = 4.0376\n",
            "Step 800 avg train loss = 4.0620\n",
            "Step 900 avg train loss = 4.0469\n",
            "Step 1000 avg train loss = 4.0585\n",
            "Step 1100 avg train loss = 4.0540\n",
            "Step 1200 avg train loss = 4.0533\n",
            "Step 1300 avg train loss = 4.0616\n",
            "Step 1400 avg train loss = 4.0754\n",
            "Step 1500 avg train loss = 4.0883\n",
            "Step 1600 avg train loss = 4.0759\n",
            "Step 1700 avg train loss = 4.0898\n",
            "Step 1800 avg train loss = 4.0750\n",
            "Step 1900 avg train loss = 4.0744\n",
            "Step 2000 avg train loss = 4.1101\n",
            "Step 2100 avg train loss = 4.0871\n",
            "Step 2200 avg train loss = 4.0985\n",
            "Step 2300 avg train loss = 4.0938\n",
            "Step 2400 avg train loss = 4.1244\n",
            "Validation loss after 17 epoch = 5.5626\n",
            "Step 0 avg train loss = 4.0586\n",
            "Step 100 avg train loss = 3.9622\n",
            "Step 200 avg train loss = 3.9616\n",
            "Step 300 avg train loss = 3.9661\n",
            "Step 400 avg train loss = 3.9781\n",
            "Step 500 avg train loss = 3.9821\n",
            "Step 600 avg train loss = 3.9901\n",
            "Step 700 avg train loss = 3.9958\n",
            "Step 800 avg train loss = 3.9904\n",
            "Step 900 avg train loss = 4.0086\n",
            "Step 1000 avg train loss = 4.0141\n",
            "Step 1100 avg train loss = 4.0295\n",
            "Step 1200 avg train loss = 4.0179\n",
            "Step 1300 avg train loss = 4.0372\n",
            "Step 1400 avg train loss = 4.0217\n",
            "Step 1500 avg train loss = 4.0407\n",
            "Step 1600 avg train loss = 4.0298\n",
            "Step 1700 avg train loss = 4.0544\n",
            "Step 1800 avg train loss = 4.0632\n",
            "Step 1900 avg train loss = 4.0674\n",
            "Step 2000 avg train loss = 4.0524\n",
            "Step 2100 avg train loss = 4.0640\n",
            "Step 2200 avg train loss = 4.0670\n",
            "Step 2300 avg train loss = 4.0740\n",
            "Step 2400 avg train loss = 4.0513\n",
            "Validation loss after 18 epoch = 5.6016\n",
            "Step 0 avg train loss = 3.9841\n",
            "Step 100 avg train loss = 3.9164\n",
            "Step 200 avg train loss = 3.9314\n",
            "Step 300 avg train loss = 3.9329\n",
            "Step 400 avg train loss = 3.9385\n",
            "Step 500 avg train loss = 3.9649\n",
            "Step 600 avg train loss = 3.9697\n",
            "Step 700 avg train loss = 3.9678\n",
            "Step 800 avg train loss = 3.9723\n",
            "Step 900 avg train loss = 3.9918\n",
            "Step 1000 avg train loss = 3.9726\n",
            "Step 1100 avg train loss = 3.9727\n",
            "Step 1200 avg train loss = 3.9829\n",
            "Step 1300 avg train loss = 3.9852\n",
            "Step 1400 avg train loss = 3.9998\n",
            "Step 1500 avg train loss = 4.0064\n",
            "Step 1600 avg train loss = 4.0153\n",
            "Step 1700 avg train loss = 4.0046\n",
            "Step 1800 avg train loss = 4.0104\n",
            "Step 1900 avg train loss = 3.9953\n",
            "Step 2000 avg train loss = 4.0229\n",
            "Step 2100 avg train loss = 4.0129\n",
            "Step 2200 avg train loss = 4.0377\n",
            "Step 2300 avg train loss = 4.0442\n",
            "Step 2400 avg train loss = 4.0593\n",
            "Validation loss after 19 epoch = 5.6320\n",
            "Saving best model with best embedding dimension...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVH3KCO9_UW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cfe354d-4565-49d0-9837-5976f53a364d"
      },
      "source": [
        "# second hyperparameter tuning\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "# single grid search for the hyperparameter tuning\n",
        "param_grid_hid_size = {\"hidden_size\": [300,400,500]}       \n",
        "\n",
        "# list of parameters=========== \n",
        "epoch_num = 20\n",
        "embedding_size = 300 # find the best embedding size 5.2010 after 5 epoch\n",
        "learning_rate=0.001\n",
        "overall_min_val_loss = 20\n",
        "# Loop over embedding size:\n",
        "for hidden_size in param_grid_hid_size[\"hidden_size\"]:\n",
        "  \n",
        "  embedding_size = embedding_size\n",
        "  hidden_size = hidden_size # output of dimension \n",
        "  num_layers = 2\n",
        "  lstm_dropout = 0.1\n",
        "# input_size = lookup.weight.size(1)\n",
        "  vocab_size = len(train_dict)\n",
        "  \n",
        "  options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "  \n",
        "  model = LSTMModel(options).to(current_device)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "  model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = optim.Adam(model_parameters, lr=learning_rate)\n",
        "  \n",
        "  print(model)\n",
        "  \n",
        "  plot_cache = []\n",
        "  min_val_loss = 20   \n",
        "\n",
        "  for epoch_number in range(epoch_num):\n",
        "    \n",
        "    # do train \n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "            print(\"update best model with this parameter to :\")\n",
        "            print(best_model)\n",
        "    \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "    \n",
        "    \n",
        "    if load_pretrained:\n",
        "        break\n",
        "  \n",
        "  if (min_val_loss < overall_min_val_loss):\n",
        "    best_model_overall = best_model\n",
        "    overall_min_val_loss = min_val_loss\n",
        "    print(\"update overall best model to :\")\n",
        "    print(best_model_overall)\n",
        "    \n",
        "# save the best model in this single grid search:         \n",
        "print('Saving best model with best hidden size...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model_overall.state_dict()\n",
        "        }, './hid_tune_best_LSTM.pt')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4099\n",
            "Step 100 avg train loss = 7.6503\n",
            "Step 200 avg train loss = 7.1205\n",
            "Step 300 avg train loss = 6.9481\n",
            "Step 400 avg train loss = 6.7299\n",
            "Step 500 avg train loss = 6.5959\n",
            "Step 600 avg train loss = 6.4814\n",
            "Step 700 avg train loss = 6.3755\n",
            "Step 800 avg train loss = 6.3012\n",
            "Step 900 avg train loss = 6.2247\n",
            "Step 1000 avg train loss = 6.1656\n",
            "Step 1100 avg train loss = 6.1052\n",
            "Step 1200 avg train loss = 6.0663\n",
            "Step 1300 avg train loss = 6.0285\n",
            "Step 1400 avg train loss = 5.9922\n",
            "Step 1500 avg train loss = 5.9431\n",
            "Step 1600 avg train loss = 5.9030\n",
            "Step 1700 avg train loss = 5.8825\n",
            "Step 1800 avg train loss = 5.8530\n",
            "Step 1900 avg train loss = 5.8168\n",
            "Step 2000 avg train loss = 5.7784\n",
            "Step 2100 avg train loss = 5.7565\n",
            "Step 2200 avg train loss = 5.7259\n",
            "Step 2300 avg train loss = 5.7185\n",
            "Step 2400 avg train loss = 5.7055\n",
            "Validation loss after 0 epoch = 5.5290\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.4427\n",
            "Step 100 avg train loss = 5.5335\n",
            "Step 200 avg train loss = 5.5159\n",
            "Step 300 avg train loss = 5.5119\n",
            "Step 400 avg train loss = 5.5175\n",
            "Step 500 avg train loss = 5.5026\n",
            "Step 600 avg train loss = 5.4787\n",
            "Step 700 avg train loss = 5.4554\n",
            "Step 800 avg train loss = 5.4593\n",
            "Step 900 avg train loss = 5.4301\n",
            "Step 1000 avg train loss = 5.4231\n",
            "Step 1100 avg train loss = 5.4054\n",
            "Step 1200 avg train loss = 5.3733\n",
            "Step 1300 avg train loss = 5.3645\n",
            "Step 1400 avg train loss = 5.3550\n",
            "Step 1500 avg train loss = 5.3633\n",
            "Step 1600 avg train loss = 5.3374\n",
            "Step 1700 avg train loss = 5.3263\n",
            "Step 1800 avg train loss = 5.3229\n",
            "Step 1900 avg train loss = 5.3222\n",
            "Step 2000 avg train loss = 5.2895\n",
            "Step 2100 avg train loss = 5.2699\n",
            "Step 2200 avg train loss = 5.2679\n",
            "Step 2300 avg train loss = 5.2731\n",
            "Step 2400 avg train loss = 5.2633\n",
            "Validation loss after 1 epoch = 5.2606\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.1397\n",
            "Step 100 avg train loss = 5.0791\n",
            "Step 200 avg train loss = 5.0573\n",
            "Step 300 avg train loss = 5.0570\n",
            "Step 400 avg train loss = 5.0327\n",
            "Step 500 avg train loss = 5.0358\n",
            "Step 600 avg train loss = 5.0562\n",
            "Step 700 avg train loss = 5.0531\n",
            "Step 800 avg train loss = 5.0421\n",
            "Step 900 avg train loss = 5.0305\n",
            "Step 1000 avg train loss = 5.0268\n",
            "Step 1100 avg train loss = 5.0174\n",
            "Step 1200 avg train loss = 5.0098\n",
            "Step 1300 avg train loss = 4.9993\n",
            "Step 1400 avg train loss = 5.0077\n",
            "Step 1500 avg train loss = 5.0213\n",
            "Step 1600 avg train loss = 4.9785\n",
            "Step 1700 avg train loss = 5.0014\n",
            "Step 1800 avg train loss = 4.9807\n",
            "Step 1900 avg train loss = 5.0027\n",
            "Step 2000 avg train loss = 4.9883\n",
            "Step 2100 avg train loss = 4.9746\n",
            "Step 2200 avg train loss = 4.9813\n",
            "Step 2300 avg train loss = 4.9706\n",
            "Step 2400 avg train loss = 4.9184\n",
            "Validation loss after 2 epoch = 5.1541\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.7694\n",
            "Step 100 avg train loss = 4.7595\n",
            "Step 200 avg train loss = 4.7478\n",
            "Step 300 avg train loss = 4.7492\n",
            "Step 400 avg train loss = 4.7560\n",
            "Step 500 avg train loss = 4.7574\n",
            "Step 600 avg train loss = 4.7632\n",
            "Step 700 avg train loss = 4.7486\n",
            "Step 800 avg train loss = 4.7542\n",
            "Step 900 avg train loss = 4.7281\n",
            "Step 1000 avg train loss = 4.7470\n",
            "Step 1100 avg train loss = 4.7408\n",
            "Step 1200 avg train loss = 4.7567\n",
            "Step 1300 avg train loss = 4.7272\n",
            "Step 1400 avg train loss = 4.7395\n",
            "Step 1500 avg train loss = 4.7376\n",
            "Step 1600 avg train loss = 4.7607\n",
            "Step 1700 avg train loss = 4.7169\n",
            "Step 1800 avg train loss = 4.7630\n",
            "Step 1900 avg train loss = 4.7081\n",
            "Step 2000 avg train loss = 4.7493\n",
            "Step 2100 avg train loss = 4.7253\n",
            "Step 2200 avg train loss = 4.7502\n",
            "Step 2300 avg train loss = 4.7248\n",
            "Step 2400 avg train loss = 4.7259\n",
            "Validation loss after 3 epoch = 5.1226\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.4741\n",
            "Step 100 avg train loss = 4.4803\n",
            "Step 200 avg train loss = 4.4952\n",
            "Step 300 avg train loss = 4.5141\n",
            "Step 400 avg train loss = 4.5000\n",
            "Step 500 avg train loss = 4.5007\n",
            "Step 600 avg train loss = 4.5440\n",
            "Step 700 avg train loss = 4.5412\n",
            "Step 800 avg train loss = 4.4976\n",
            "Step 900 avg train loss = 4.5229\n",
            "Step 1000 avg train loss = 4.5425\n",
            "Step 1100 avg train loss = 4.5192\n",
            "Step 1200 avg train loss = 4.5280\n",
            "Step 1300 avg train loss = 4.5341\n",
            "Step 1400 avg train loss = 4.5197\n",
            "Step 1500 avg train loss = 4.5343\n",
            "Step 1600 avg train loss = 4.5381\n",
            "Step 1700 avg train loss = 4.5367\n",
            "Step 1800 avg train loss = 4.5439\n",
            "Step 1900 avg train loss = 4.5416\n",
            "Step 2000 avg train loss = 4.5388\n",
            "Step 2100 avg train loss = 4.5345\n",
            "Step 2200 avg train loss = 4.5394\n",
            "Step 2300 avg train loss = 4.5295\n",
            "Step 2400 avg train loss = 4.5709\n",
            "Validation loss after 4 epoch = 5.1462\n",
            "Step 0 avg train loss = 4.3837\n",
            "Step 100 avg train loss = 4.3057\n",
            "Step 200 avg train loss = 4.3158\n",
            "Step 300 avg train loss = 4.2948\n",
            "Step 400 avg train loss = 4.3173\n",
            "Step 500 avg train loss = 4.3366\n",
            "Step 600 avg train loss = 4.3305\n",
            "Step 700 avg train loss = 4.3099\n",
            "Step 800 avg train loss = 4.3415\n",
            "Step 900 avg train loss = 4.3177\n",
            "Step 1000 avg train loss = 4.3623\n",
            "Step 1100 avg train loss = 4.3501\n",
            "Step 1200 avg train loss = 4.3291\n",
            "Step 1300 avg train loss = 4.3666\n",
            "Step 1400 avg train loss = 4.3728\n",
            "Step 1500 avg train loss = 4.3527\n",
            "Step 1600 avg train loss = 4.3682\n",
            "Step 1700 avg train loss = 4.3785\n",
            "Step 1800 avg train loss = 4.3579\n",
            "Step 1900 avg train loss = 4.3822\n",
            "Step 2000 avg train loss = 4.3779\n",
            "Step 2100 avg train loss = 4.3613\n",
            "Step 2200 avg train loss = 4.3698\n",
            "Step 2300 avg train loss = 4.3592\n",
            "Step 2400 avg train loss = 4.3895\n",
            "Validation loss after 5 epoch = 5.2028\n",
            "Step 0 avg train loss = 4.0526\n",
            "Step 100 avg train loss = 4.1325\n",
            "Step 200 avg train loss = 4.1482\n",
            "Step 300 avg train loss = 4.1528\n",
            "Step 400 avg train loss = 4.1402\n",
            "Step 500 avg train loss = 4.1841\n",
            "Step 600 avg train loss = 4.1917\n",
            "Step 700 avg train loss = 4.1770\n",
            "Step 800 avg train loss = 4.1738\n",
            "Step 900 avg train loss = 4.1912\n",
            "Step 1000 avg train loss = 4.1658\n",
            "Step 1100 avg train loss = 4.1895\n",
            "Step 1200 avg train loss = 4.1859\n",
            "Step 1300 avg train loss = 4.2089\n",
            "Step 1400 avg train loss = 4.2021\n",
            "Step 1500 avg train loss = 4.2086\n",
            "Step 1600 avg train loss = 4.2127\n",
            "Step 1700 avg train loss = 4.2212\n",
            "Step 1800 avg train loss = 4.2368\n",
            "Step 1900 avg train loss = 4.2285\n",
            "Step 2000 avg train loss = 4.2108\n",
            "Step 2100 avg train loss = 4.2291\n",
            "Step 2200 avg train loss = 4.2241\n",
            "Step 2300 avg train loss = 4.2448\n",
            "Step 2400 avg train loss = 4.2500\n",
            "Validation loss after 6 epoch = 5.2643\n",
            "Step 0 avg train loss = 4.1608\n",
            "Step 100 avg train loss = 3.9926\n",
            "Step 200 avg train loss = 3.9853\n",
            "Step 300 avg train loss = 4.0101\n",
            "Step 400 avg train loss = 4.0226\n",
            "Step 500 avg train loss = 4.0400\n",
            "Step 600 avg train loss = 4.0514\n",
            "Step 700 avg train loss = 4.0365\n",
            "Step 800 avg train loss = 4.0351\n",
            "Step 900 avg train loss = 4.0683\n",
            "Step 1000 avg train loss = 4.0576\n",
            "Step 1100 avg train loss = 4.0519\n",
            "Step 1200 avg train loss = 4.0592\n",
            "Step 1300 avg train loss = 4.0794\n",
            "Step 1400 avg train loss = 4.0862\n",
            "Step 1500 avg train loss = 4.0734\n",
            "Step 1600 avg train loss = 4.0884\n",
            "Step 1700 avg train loss = 4.0832\n",
            "Step 1800 avg train loss = 4.0857\n",
            "Step 1900 avg train loss = 4.1060\n",
            "Step 2000 avg train loss = 4.1110\n",
            "Step 2100 avg train loss = 4.1013\n",
            "Step 2200 avg train loss = 4.0906\n",
            "Step 2300 avg train loss = 4.1083\n",
            "Step 2400 avg train loss = 4.1312\n",
            "Validation loss after 7 epoch = 5.3327\n",
            "Step 0 avg train loss = 3.7683\n",
            "Step 100 avg train loss = 3.8570\n",
            "Step 200 avg train loss = 3.8763\n",
            "Step 300 avg train loss = 3.8780\n",
            "Step 400 avg train loss = 3.8904\n",
            "Step 500 avg train loss = 3.9068\n",
            "Step 600 avg train loss = 3.9278\n",
            "Step 700 avg train loss = 3.9140\n",
            "Step 800 avg train loss = 3.9463\n",
            "Step 900 avg train loss = 3.9398\n",
            "Step 1000 avg train loss = 3.9402\n",
            "Step 1100 avg train loss = 3.9359\n",
            "Step 1200 avg train loss = 3.9579\n",
            "Step 1300 avg train loss = 3.9682\n",
            "Step 1400 avg train loss = 3.9625\n",
            "Step 1500 avg train loss = 3.9634\n",
            "Step 1600 avg train loss = 3.9992\n",
            "Step 1700 avg train loss = 3.9688\n",
            "Step 1800 avg train loss = 3.9685\n",
            "Step 1900 avg train loss = 4.0123\n",
            "Step 2000 avg train loss = 4.0018\n",
            "Step 2100 avg train loss = 3.9943\n",
            "Step 2200 avg train loss = 3.9933\n",
            "Step 2300 avg train loss = 3.9897\n",
            "Step 2400 avg train loss = 3.9900\n",
            "Validation loss after 8 epoch = 5.3975\n",
            "Step 0 avg train loss = 3.7222\n",
            "Step 100 avg train loss = 3.7571\n",
            "Step 200 avg train loss = 3.7694\n",
            "Step 300 avg train loss = 3.7698\n",
            "Step 400 avg train loss = 3.7991\n",
            "Step 500 avg train loss = 3.8114\n",
            "Step 600 avg train loss = 3.7970\n",
            "Step 700 avg train loss = 3.8228\n",
            "Step 800 avg train loss = 3.8290\n",
            "Step 900 avg train loss = 3.8350\n",
            "Step 1000 avg train loss = 3.8275\n",
            "Step 1100 avg train loss = 3.8465\n",
            "Step 1200 avg train loss = 3.8479\n",
            "Step 1300 avg train loss = 3.8568\n",
            "Step 1400 avg train loss = 3.8392\n",
            "Step 1500 avg train loss = 3.8794\n",
            "Step 1600 avg train loss = 3.8874\n",
            "Step 1700 avg train loss = 3.8756\n",
            "Step 1800 avg train loss = 3.8972\n",
            "Step 1900 avg train loss = 3.8759\n",
            "Step 2000 avg train loss = 3.8924\n",
            "Step 2100 avg train loss = 3.8987\n",
            "Step 2200 avg train loss = 3.8825\n",
            "Step 2300 avg train loss = 3.9162\n",
            "Step 2400 avg train loss = 3.8946\n",
            "Validation loss after 9 epoch = 5.4741\n",
            "Step 0 avg train loss = 3.7291\n",
            "Step 100 avg train loss = 3.6551\n",
            "Step 200 avg train loss = 3.6756\n",
            "Step 300 avg train loss = 3.6913\n",
            "Step 400 avg train loss = 3.6820\n",
            "Step 500 avg train loss = 3.7193\n",
            "Step 600 avg train loss = 3.6884\n",
            "Step 700 avg train loss = 3.7278\n",
            "Step 800 avg train loss = 3.7154\n",
            "Step 900 avg train loss = 3.7263\n",
            "Step 1000 avg train loss = 3.7529\n",
            "Step 1100 avg train loss = 3.7575\n",
            "Step 1200 avg train loss = 3.7585\n",
            "Step 1300 avg train loss = 3.7535\n",
            "Step 1400 avg train loss = 3.7745\n",
            "Step 1500 avg train loss = 3.7748\n",
            "Step 1600 avg train loss = 3.7892\n",
            "Step 1700 avg train loss = 3.7992\n",
            "Step 1800 avg train loss = 3.7850\n",
            "Step 1900 avg train loss = 3.7759\n",
            "Step 2000 avg train loss = 3.7892\n",
            "Step 2100 avg train loss = 3.7872\n",
            "Step 2200 avg train loss = 3.8131\n",
            "Step 2300 avg train loss = 3.8323\n",
            "Step 2400 avg train loss = 3.8356\n",
            "Validation loss after 10 epoch = 5.5322\n",
            "Step 0 avg train loss = 3.8113\n",
            "Step 100 avg train loss = 3.5682\n",
            "Step 200 avg train loss = 3.5750\n",
            "Step 300 avg train loss = 3.5833\n",
            "Step 400 avg train loss = 3.6055\n",
            "Step 500 avg train loss = 3.6171\n",
            "Step 600 avg train loss = 3.6419\n",
            "Step 700 avg train loss = 3.6146\n",
            "Step 800 avg train loss = 3.6351\n",
            "Step 900 avg train loss = 3.6484\n",
            "Step 1000 avg train loss = 3.6540\n",
            "Step 1100 avg train loss = 3.6774\n",
            "Step 1200 avg train loss = 3.6741\n",
            "Step 1300 avg train loss = 3.6976\n",
            "Step 1400 avg train loss = 3.6839\n",
            "Step 1500 avg train loss = 3.6895\n",
            "Step 1600 avg train loss = 3.7053\n",
            "Step 1700 avg train loss = 3.6926\n",
            "Step 1800 avg train loss = 3.7059\n",
            "Step 1900 avg train loss = 3.7257\n",
            "Step 2000 avg train loss = 3.7028\n",
            "Step 2100 avg train loss = 3.7292\n",
            "Step 2200 avg train loss = 3.7338\n",
            "Step 2300 avg train loss = 3.7201\n",
            "Step 2400 avg train loss = 3.7461\n",
            "Validation loss after 11 epoch = 5.6184\n",
            "Step 0 avg train loss = 3.3071\n",
            "Step 100 avg train loss = 3.4697\n",
            "Step 200 avg train loss = 3.4999\n",
            "Step 300 avg train loss = 3.5252\n",
            "Step 400 avg train loss = 3.5207\n",
            "Step 500 avg train loss = 3.5351\n",
            "Step 600 avg train loss = 3.5686\n",
            "Step 700 avg train loss = 3.5591\n",
            "Step 800 avg train loss = 3.5615\n",
            "Step 900 avg train loss = 3.5776\n",
            "Step 1000 avg train loss = 3.5719\n",
            "Step 1100 avg train loss = 3.5880\n",
            "Step 1200 avg train loss = 3.6035\n",
            "Step 1300 avg train loss = 3.5975\n",
            "Step 1400 avg train loss = 3.6062\n",
            "Step 1500 avg train loss = 3.6110\n",
            "Step 1600 avg train loss = 3.6164\n",
            "Step 1700 avg train loss = 3.6234\n",
            "Step 1800 avg train loss = 3.6323\n",
            "Step 1900 avg train loss = 3.6218\n",
            "Step 2000 avg train loss = 3.6716\n",
            "Step 2100 avg train loss = 3.6399\n",
            "Step 2200 avg train loss = 3.6409\n",
            "Step 2300 avg train loss = 3.6468\n",
            "Step 2400 avg train loss = 3.6594\n",
            "Validation loss after 12 epoch = 5.6740\n",
            "Step 0 avg train loss = 3.4354\n",
            "Step 100 avg train loss = 3.4094\n",
            "Step 200 avg train loss = 3.4397\n",
            "Step 300 avg train loss = 3.4415\n",
            "Step 400 avg train loss = 3.4573\n",
            "Step 500 avg train loss = 3.4640\n",
            "Step 600 avg train loss = 3.4705\n",
            "Step 700 avg train loss = 3.4894\n",
            "Step 800 avg train loss = 3.4910\n",
            "Step 900 avg train loss = 3.5019\n",
            "Step 1000 avg train loss = 3.5078\n",
            "Step 1100 avg train loss = 3.5457\n",
            "Step 1200 avg train loss = 3.5079\n",
            "Step 1300 avg train loss = 3.5296\n",
            "Step 1400 avg train loss = 3.5421\n",
            "Step 1500 avg train loss = 3.5358\n",
            "Step 1600 avg train loss = 3.5525\n",
            "Step 1700 avg train loss = 3.5517\n",
            "Step 1800 avg train loss = 3.5501\n",
            "Step 1900 avg train loss = 3.5790\n",
            "Step 2000 avg train loss = 3.5600\n",
            "Step 2100 avg train loss = 3.5644\n",
            "Step 2200 avg train loss = 3.5797\n",
            "Step 2300 avg train loss = 3.5731\n",
            "Step 2400 avg train loss = 3.5812\n",
            "Validation loss after 13 epoch = 5.7475\n",
            "Step 0 avg train loss = 3.2863\n",
            "Step 100 avg train loss = 3.3622\n",
            "Step 200 avg train loss = 3.3642\n",
            "Step 300 avg train loss = 3.3426\n",
            "Step 400 avg train loss = 3.3910\n",
            "Step 500 avg train loss = 3.4220\n",
            "Step 600 avg train loss = 3.4062\n",
            "Step 700 avg train loss = 3.4011\n",
            "Step 800 avg train loss = 3.4230\n",
            "Step 900 avg train loss = 3.4452\n",
            "Step 1000 avg train loss = 3.4464\n",
            "Step 1100 avg train loss = 3.4487\n",
            "Step 1200 avg train loss = 3.4500\n",
            "Step 1300 avg train loss = 3.4658\n",
            "Step 1400 avg train loss = 3.4717\n",
            "Step 1500 avg train loss = 3.4795\n",
            "Step 1600 avg train loss = 3.4666\n",
            "Step 1700 avg train loss = 3.4916\n",
            "Step 1800 avg train loss = 3.4915\n",
            "Step 1900 avg train loss = 3.5139\n",
            "Step 2000 avg train loss = 3.5037\n",
            "Step 2100 avg train loss = 3.4985\n",
            "Step 2200 avg train loss = 3.5241\n",
            "Step 2300 avg train loss = 3.5092\n",
            "Step 2400 avg train loss = 3.5168\n",
            "Validation loss after 14 epoch = 5.8012\n",
            "Step 0 avg train loss = 3.1319\n",
            "Step 100 avg train loss = 3.2800\n",
            "Step 200 avg train loss = 3.3174\n",
            "Step 300 avg train loss = 3.3142\n",
            "Step 400 avg train loss = 3.2987\n",
            "Step 500 avg train loss = 3.3539\n",
            "Step 600 avg train loss = 3.3492\n",
            "Step 700 avg train loss = 3.3694\n",
            "Step 800 avg train loss = 3.3564\n",
            "Step 900 avg train loss = 3.3689\n",
            "Step 1000 avg train loss = 3.3779\n",
            "Step 1100 avg train loss = 3.3759\n",
            "Step 1200 avg train loss = 3.3902\n",
            "Step 1300 avg train loss = 3.3965\n",
            "Step 1400 avg train loss = 3.4062\n",
            "Step 1500 avg train loss = 3.3951\n",
            "Step 1600 avg train loss = 3.4263\n",
            "Step 1700 avg train loss = 3.4413\n",
            "Step 1800 avg train loss = 3.4143\n",
            "Step 1900 avg train loss = 3.4398\n",
            "Step 2000 avg train loss = 3.4500\n",
            "Step 2100 avg train loss = 3.4363\n",
            "Step 2200 avg train loss = 3.4307\n",
            "Step 2300 avg train loss = 3.4724\n",
            "Step 2400 avg train loss = 3.4862\n",
            "Validation loss after 15 epoch = 5.8743\n",
            "Step 0 avg train loss = 3.2559\n",
            "Step 100 avg train loss = 3.2319\n",
            "Step 200 avg train loss = 3.2329\n",
            "Step 300 avg train loss = 3.2552\n",
            "Step 400 avg train loss = 3.2595\n",
            "Step 500 avg train loss = 3.2810\n",
            "Step 600 avg train loss = 3.2685\n",
            "Step 700 avg train loss = 3.3044\n",
            "Step 800 avg train loss = 3.3060\n",
            "Step 900 avg train loss = 3.3145\n",
            "Step 1000 avg train loss = 3.3244\n",
            "Step 1100 avg train loss = 3.3450\n",
            "Step 1200 avg train loss = 3.3607\n",
            "Step 1300 avg train loss = 3.3391\n",
            "Step 1400 avg train loss = 3.3500\n",
            "Step 1500 avg train loss = 3.3405\n",
            "Step 1600 avg train loss = 3.3485\n",
            "Step 1700 avg train loss = 3.3663\n",
            "Step 1800 avg train loss = 3.3645\n",
            "Step 1900 avg train loss = 3.3857\n",
            "Step 2000 avg train loss = 3.4084\n",
            "Step 2100 avg train loss = 3.3891\n",
            "Step 2200 avg train loss = 3.3910\n",
            "Step 2300 avg train loss = 3.3972\n",
            "Step 2400 avg train loss = 3.4180\n",
            "Validation loss after 16 epoch = 5.9382\n",
            "Step 0 avg train loss = 3.1351\n",
            "Step 100 avg train loss = 3.1690\n",
            "Step 200 avg train loss = 3.1953\n",
            "Step 300 avg train loss = 3.2114\n",
            "Step 400 avg train loss = 3.2002\n",
            "Step 500 avg train loss = 3.2336\n",
            "Step 600 avg train loss = 3.2433\n",
            "Step 700 avg train loss = 3.2336\n",
            "Step 800 avg train loss = 3.2417\n",
            "Step 900 avg train loss = 3.2495\n",
            "Step 1000 avg train loss = 3.2415\n",
            "Step 1100 avg train loss = 3.2660\n",
            "Step 1200 avg train loss = 3.2834\n",
            "Step 1300 avg train loss = 3.2966\n",
            "Step 1400 avg train loss = 3.3176\n",
            "Step 1500 avg train loss = 3.3167\n",
            "Step 1600 avg train loss = 3.3291\n",
            "Step 1700 avg train loss = 3.3274\n",
            "Step 1800 avg train loss = 3.3178\n",
            "Step 1900 avg train loss = 3.3335\n",
            "Step 2000 avg train loss = 3.3340\n",
            "Step 2100 avg train loss = 3.3309\n",
            "Step 2200 avg train loss = 3.3299\n",
            "Step 2300 avg train loss = 3.3410\n",
            "Step 2400 avg train loss = 3.3604\n",
            "Validation loss after 17 epoch = 6.0043\n",
            "Step 0 avg train loss = 3.2853\n",
            "Step 100 avg train loss = 3.1190\n",
            "Step 200 avg train loss = 3.1357\n",
            "Step 300 avg train loss = 3.1676\n",
            "Step 400 avg train loss = 3.1814\n",
            "Step 500 avg train loss = 3.1903\n",
            "Step 600 avg train loss = 3.1763\n",
            "Step 700 avg train loss = 3.1932\n",
            "Step 800 avg train loss = 3.1999\n",
            "Step 900 avg train loss = 3.1999\n",
            "Step 1000 avg train loss = 3.2263\n",
            "Step 1100 avg train loss = 3.2124\n",
            "Step 1200 avg train loss = 3.2228\n",
            "Step 1300 avg train loss = 3.2570\n",
            "Step 1400 avg train loss = 3.2456\n",
            "Step 1500 avg train loss = 3.2563\n",
            "Step 1600 avg train loss = 3.2409\n",
            "Step 1700 avg train loss = 3.2554\n",
            "Step 1800 avg train loss = 3.2732\n",
            "Step 1900 avg train loss = 3.2781\n",
            "Step 2000 avg train loss = 3.2878\n",
            "Step 2100 avg train loss = 3.2644\n",
            "Step 2200 avg train loss = 3.2989\n",
            "Step 2300 avg train loss = 3.2963\n",
            "Step 2400 avg train loss = 3.3218\n",
            "Validation loss after 18 epoch = 6.0666\n",
            "Step 0 avg train loss = 3.0786\n",
            "Step 100 avg train loss = 3.0703\n",
            "Step 200 avg train loss = 3.1080\n",
            "Step 300 avg train loss = 3.0987\n",
            "Step 400 avg train loss = 3.1110\n",
            "Step 500 avg train loss = 3.1162\n",
            "Step 600 avg train loss = 3.1442\n",
            "Step 700 avg train loss = 3.1542\n",
            "Step 800 avg train loss = 3.1599\n",
            "Step 900 avg train loss = 3.1698\n",
            "Step 1000 avg train loss = 3.1614\n",
            "Step 1100 avg train loss = 3.1674\n",
            "Step 1200 avg train loss = 3.1715\n",
            "Step 1300 avg train loss = 3.1744\n",
            "Step 1400 avg train loss = 3.1836\n",
            "Step 1500 avg train loss = 3.1990\n",
            "Step 1600 avg train loss = 3.1874\n",
            "Step 1700 avg train loss = 3.2311\n",
            "Step 1800 avg train loss = 3.2313\n",
            "Step 1900 avg train loss = 3.2201\n",
            "Step 2000 avg train loss = 3.2436\n",
            "Step 2100 avg train loss = 3.2652\n",
            "Step 2200 avg train loss = 3.2567\n",
            "Step 2300 avg train loss = 3.2652\n",
            "Step 2400 avg train loss = 3.2681\n",
            "Validation loss after 19 epoch = 6.1196\n",
            "update overall best model to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4098\n",
            "Step 100 avg train loss = 7.6492\n",
            "Step 200 avg train loss = 7.0818\n",
            "Step 300 avg train loss = 6.8396\n",
            "Step 400 avg train loss = 6.6375\n",
            "Step 500 avg train loss = 6.4900\n",
            "Step 600 avg train loss = 6.4086\n",
            "Step 700 avg train loss = 6.2855\n",
            "Step 800 avg train loss = 6.2186\n",
            "Step 900 avg train loss = 6.1665\n",
            "Step 1000 avg train loss = 6.1015\n",
            "Step 1100 avg train loss = 6.0557\n",
            "Step 1200 avg train loss = 6.0004\n",
            "Step 1300 avg train loss = 5.9636\n",
            "Step 1400 avg train loss = 5.9022\n",
            "Step 1500 avg train loss = 5.8735\n",
            "Step 1600 avg train loss = 5.8570\n",
            "Step 1700 avg train loss = 5.8130\n",
            "Step 1800 avg train loss = 5.7860\n",
            "Step 1900 avg train loss = 5.7499\n",
            "Step 2000 avg train loss = 5.7413\n",
            "Step 2100 avg train loss = 5.7078\n",
            "Step 2200 avg train loss = 5.6742\n",
            "Step 2300 avg train loss = 5.6488\n",
            "Step 2400 avg train loss = 5.6270\n",
            "Validation loss after 0 epoch = 5.4853\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.4978\n",
            "Step 100 avg train loss = 5.4801\n",
            "Step 200 avg train loss = 5.4537\n",
            "Step 300 avg train loss = 5.4182\n",
            "Step 400 avg train loss = 5.4334\n",
            "Step 500 avg train loss = 5.4009\n",
            "Step 600 avg train loss = 5.4067\n",
            "Step 700 avg train loss = 5.3802\n",
            "Step 800 avg train loss = 5.3669\n",
            "Step 900 avg train loss = 5.3725\n",
            "Step 1000 avg train loss = 5.3187\n",
            "Step 1100 avg train loss = 5.3181\n",
            "Step 1200 avg train loss = 5.3102\n",
            "Step 1300 avg train loss = 5.2929\n",
            "Step 1400 avg train loss = 5.2853\n",
            "Step 1500 avg train loss = 5.2878\n",
            "Step 1600 avg train loss = 5.2364\n",
            "Step 1700 avg train loss = 5.2652\n",
            "Step 1800 avg train loss = 5.2310\n",
            "Step 1900 avg train loss = 5.2376\n",
            "Step 2000 avg train loss = 5.2195\n",
            "Step 2100 avg train loss = 5.1981\n",
            "Step 2200 avg train loss = 5.1933\n",
            "Step 2300 avg train loss = 5.2065\n",
            "Step 2400 avg train loss = 5.1798\n",
            "Validation loss after 1 epoch = 5.2252\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.9563\n",
            "Step 100 avg train loss = 4.9419\n",
            "Step 200 avg train loss = 4.9624\n",
            "Step 300 avg train loss = 4.9590\n",
            "Step 400 avg train loss = 4.9417\n",
            "Step 500 avg train loss = 4.9319\n",
            "Step 600 avg train loss = 4.9172\n",
            "Step 700 avg train loss = 4.9420\n",
            "Step 800 avg train loss = 4.9540\n",
            "Step 900 avg train loss = 4.9483\n",
            "Step 1000 avg train loss = 4.9140\n",
            "Step 1100 avg train loss = 4.9100\n",
            "Step 1200 avg train loss = 4.9191\n",
            "Step 1300 avg train loss = 4.9099\n",
            "Step 1400 avg train loss = 4.9314\n",
            "Step 1500 avg train loss = 4.9077\n",
            "Step 1600 avg train loss = 4.9266\n",
            "Step 1700 avg train loss = 4.9105\n",
            "Step 1800 avg train loss = 4.9041\n",
            "Step 1900 avg train loss = 4.9077\n",
            "Step 2000 avg train loss = 4.8442\n",
            "Step 2100 avg train loss = 4.8740\n",
            "Step 2200 avg train loss = 4.8757\n",
            "Step 2300 avg train loss = 4.8494\n",
            "Step 2400 avg train loss = 4.8564\n",
            "Validation loss after 2 epoch = 5.1418\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.6708\n",
            "Step 100 avg train loss = 4.6182\n",
            "Step 200 avg train loss = 4.6018\n",
            "Step 300 avg train loss = 4.6213\n",
            "Step 400 avg train loss = 4.5899\n",
            "Step 500 avg train loss = 4.6245\n",
            "Step 600 avg train loss = 4.6162\n",
            "Step 700 avg train loss = 4.6388\n",
            "Step 800 avg train loss = 4.6055\n",
            "Step 900 avg train loss = 4.6350\n",
            "Step 1000 avg train loss = 4.5953\n",
            "Step 1100 avg train loss = 4.6177\n",
            "Step 1200 avg train loss = 4.6018\n",
            "Step 1300 avg train loss = 4.6222\n",
            "Step 1400 avg train loss = 4.6504\n",
            "Step 1500 avg train loss = 4.6123\n",
            "Step 1600 avg train loss = 4.6034\n",
            "Step 1700 avg train loss = 4.6200\n",
            "Step 1800 avg train loss = 4.6032\n",
            "Step 1900 avg train loss = 4.6230\n",
            "Step 2000 avg train loss = 4.6191\n",
            "Step 2100 avg train loss = 4.6030\n",
            "Step 2200 avg train loss = 4.6272\n",
            "Step 2300 avg train loss = 4.6126\n",
            "Step 2400 avg train loss = 4.6244\n",
            "Validation loss after 3 epoch = 5.1319\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.1984\n",
            "Step 100 avg train loss = 4.3257\n",
            "Step 200 avg train loss = 4.3248\n",
            "Step 300 avg train loss = 4.3606\n",
            "Step 400 avg train loss = 4.3569\n",
            "Step 500 avg train loss = 4.3419\n",
            "Step 600 avg train loss = 4.3553\n",
            "Step 700 avg train loss = 4.3596\n",
            "Step 800 avg train loss = 4.3687\n",
            "Step 900 avg train loss = 4.3636\n",
            "Step 1000 avg train loss = 4.3722\n",
            "Step 1100 avg train loss = 4.3623\n",
            "Step 1200 avg train loss = 4.3746\n",
            "Step 1300 avg train loss = 4.3710\n",
            "Step 1400 avg train loss = 4.4135\n",
            "Step 1500 avg train loss = 4.3609\n",
            "Step 1600 avg train loss = 4.3793\n",
            "Step 1700 avg train loss = 4.3827\n",
            "Step 1800 avg train loss = 4.3834\n",
            "Step 1900 avg train loss = 4.3835\n",
            "Step 2000 avg train loss = 4.3726\n",
            "Step 2100 avg train loss = 4.3558\n",
            "Step 2200 avg train loss = 4.3982\n",
            "Step 2300 avg train loss = 4.3978\n",
            "Step 2400 avg train loss = 4.3891\n",
            "Validation loss after 4 epoch = 5.1783\n",
            "Step 0 avg train loss = 4.3267\n",
            "Step 100 avg train loss = 4.0815\n",
            "Step 200 avg train loss = 4.1013\n",
            "Step 300 avg train loss = 4.1208\n",
            "Step 400 avg train loss = 4.1366\n",
            "Step 500 avg train loss = 4.1387\n",
            "Step 600 avg train loss = 4.1273\n",
            "Step 700 avg train loss = 4.1667\n",
            "Step 800 avg train loss = 4.1553\n",
            "Step 900 avg train loss = 4.1509\n",
            "Step 1000 avg train loss = 4.1352\n",
            "Step 1100 avg train loss = 4.1615\n",
            "Step 1200 avg train loss = 4.1634\n",
            "Step 1300 avg train loss = 4.1712\n",
            "Step 1400 avg train loss = 4.1358\n",
            "Step 1500 avg train loss = 4.1743\n",
            "Step 1600 avg train loss = 4.1838\n",
            "Step 1700 avg train loss = 4.1937\n",
            "Step 1800 avg train loss = 4.1910\n",
            "Step 1900 avg train loss = 4.2073\n",
            "Step 2000 avg train loss = 4.1915\n",
            "Step 2100 avg train loss = 4.2064\n",
            "Step 2200 avg train loss = 4.2135\n",
            "Step 2300 avg train loss = 4.2095\n",
            "Step 2400 avg train loss = 4.1995\n",
            "Validation loss after 5 epoch = 5.2598\n",
            "Step 0 avg train loss = 4.0770\n",
            "Step 100 avg train loss = 3.8953\n",
            "Step 200 avg train loss = 3.9058\n",
            "Step 300 avg train loss = 3.9239\n",
            "Step 400 avg train loss = 3.9259\n",
            "Step 500 avg train loss = 3.9415\n",
            "Step 600 avg train loss = 3.9482\n",
            "Step 700 avg train loss = 3.9858\n",
            "Step 800 avg train loss = 3.9854\n",
            "Step 900 avg train loss = 3.9786\n",
            "Step 1000 avg train loss = 3.9942\n",
            "Step 1100 avg train loss = 3.9807\n",
            "Step 1200 avg train loss = 3.9844\n",
            "Step 1300 avg train loss = 3.9762\n",
            "Step 1400 avg train loss = 4.0142\n",
            "Step 1500 avg train loss = 4.0333\n",
            "Step 1600 avg train loss = 3.9999\n",
            "Step 1700 avg train loss = 4.0310\n",
            "Step 1800 avg train loss = 4.0176\n",
            "Step 1900 avg train loss = 4.0273\n",
            "Step 2000 avg train loss = 4.0134\n",
            "Step 2100 avg train loss = 4.0241\n",
            "Step 2200 avg train loss = 4.0345\n",
            "Step 2300 avg train loss = 4.0351\n",
            "Step 2400 avg train loss = 4.0457\n",
            "Validation loss after 6 epoch = 5.3545\n",
            "Step 0 avg train loss = 3.6888\n",
            "Step 100 avg train loss = 3.7567\n",
            "Step 200 avg train loss = 3.7618\n",
            "Step 300 avg train loss = 3.7711\n",
            "Step 400 avg train loss = 3.7734\n",
            "Step 500 avg train loss = 3.7867\n",
            "Step 600 avg train loss = 3.8039\n",
            "Step 700 avg train loss = 3.8021\n",
            "Step 800 avg train loss = 3.8081\n",
            "Step 900 avg train loss = 3.8029\n",
            "Step 1000 avg train loss = 3.8465\n",
            "Step 1100 avg train loss = 3.8210\n",
            "Step 1200 avg train loss = 3.8431\n",
            "Step 1300 avg train loss = 3.8501\n",
            "Step 1400 avg train loss = 3.8391\n",
            "Step 1500 avg train loss = 3.8666\n",
            "Step 1600 avg train loss = 3.8492\n",
            "Step 1700 avg train loss = 3.8766\n",
            "Step 1800 avg train loss = 3.8715\n",
            "Step 1900 avg train loss = 3.8747\n",
            "Step 2000 avg train loss = 3.9033\n",
            "Step 2100 avg train loss = 3.8794\n",
            "Step 2200 avg train loss = 3.8816\n",
            "Step 2300 avg train loss = 3.8944\n",
            "Step 2400 avg train loss = 3.8788\n",
            "Validation loss after 7 epoch = 5.4317\n",
            "Step 0 avg train loss = 3.5293\n",
            "Step 100 avg train loss = 3.5914\n",
            "Step 200 avg train loss = 3.6155\n",
            "Step 300 avg train loss = 3.6266\n",
            "Step 400 avg train loss = 3.6419\n",
            "Step 500 avg train loss = 3.6547\n",
            "Step 600 avg train loss = 3.6716\n",
            "Step 700 avg train loss = 3.6686\n",
            "Step 800 avg train loss = 3.6737\n",
            "Step 900 avg train loss = 3.6960\n",
            "Step 1000 avg train loss = 3.6889\n",
            "Step 1100 avg train loss = 3.6950\n",
            "Step 1200 avg train loss = 3.7026\n",
            "Step 1300 avg train loss = 3.7058\n",
            "Step 1400 avg train loss = 3.7133\n",
            "Step 1500 avg train loss = 3.7117\n",
            "Step 1600 avg train loss = 3.7329\n",
            "Step 1700 avg train loss = 3.7520\n",
            "Step 1800 avg train loss = 3.7565\n",
            "Step 1900 avg train loss = 3.7602\n",
            "Step 2000 avg train loss = 3.7511\n",
            "Step 2100 avg train loss = 3.7588\n",
            "Step 2200 avg train loss = 3.7471\n",
            "Step 2300 avg train loss = 3.7601\n",
            "Step 2400 avg train loss = 3.7692\n",
            "Validation loss after 8 epoch = 5.5335\n",
            "Step 0 avg train loss = 3.3417\n",
            "Step 100 avg train loss = 3.4848\n",
            "Step 200 avg train loss = 3.4885\n",
            "Step 300 avg train loss = 3.4967\n",
            "Step 400 avg train loss = 3.5262\n",
            "Step 500 avg train loss = 3.5265\n",
            "Step 600 avg train loss = 3.5474\n",
            "Step 700 avg train loss = 3.5607\n",
            "Step 800 avg train loss = 3.5524\n",
            "Step 900 avg train loss = 3.5694\n",
            "Step 1000 avg train loss = 3.5705\n",
            "Step 1100 avg train loss = 3.5858\n",
            "Step 1200 avg train loss = 3.5875\n",
            "Step 1300 avg train loss = 3.5775\n",
            "Step 1400 avg train loss = 3.6043\n",
            "Step 1500 avg train loss = 3.6175\n",
            "Step 1600 avg train loss = 3.6140\n",
            "Step 1700 avg train loss = 3.6320\n",
            "Step 1800 avg train loss = 3.6224\n",
            "Step 1900 avg train loss = 3.6364\n",
            "Step 2000 avg train loss = 3.6624\n",
            "Step 2100 avg train loss = 3.6389\n",
            "Step 2200 avg train loss = 3.6535\n",
            "Step 2300 avg train loss = 3.6378\n",
            "Step 2400 avg train loss = 3.6603\n",
            "Validation loss after 9 epoch = 5.6182\n",
            "Step 0 avg train loss = 3.3034\n",
            "Step 100 avg train loss = 3.3584\n",
            "Step 200 avg train loss = 3.3782\n",
            "Step 300 avg train loss = 3.4030\n",
            "Step 400 avg train loss = 3.4027\n",
            "Step 500 avg train loss = 3.4284\n",
            "Step 600 avg train loss = 3.4111\n",
            "Step 700 avg train loss = 3.4428\n",
            "Step 800 avg train loss = 3.4594\n",
            "Step 900 avg train loss = 3.4710\n",
            "Step 1000 avg train loss = 3.4843\n",
            "Step 1100 avg train loss = 3.4743\n",
            "Step 1200 avg train loss = 3.4900\n",
            "Step 1300 avg train loss = 3.4779\n",
            "Step 1400 avg train loss = 3.5020\n",
            "Step 1500 avg train loss = 3.5063\n",
            "Step 1600 avg train loss = 3.5033\n",
            "Step 1700 avg train loss = 3.5028\n",
            "Step 1800 avg train loss = 3.5285\n",
            "Step 1900 avg train loss = 3.5348\n",
            "Step 2000 avg train loss = 3.5361\n",
            "Step 2100 avg train loss = 3.5453\n",
            "Step 2200 avg train loss = 3.5496\n",
            "Step 2300 avg train loss = 3.5588\n",
            "Step 2400 avg train loss = 3.5547\n",
            "Validation loss after 10 epoch = 5.7020\n",
            "Step 0 avg train loss = 3.3197\n",
            "Step 100 avg train loss = 3.2687\n",
            "Step 200 avg train loss = 3.2940\n",
            "Step 300 avg train loss = 3.3024\n",
            "Step 400 avg train loss = 3.2912\n",
            "Step 500 avg train loss = 3.3248\n",
            "Step 600 avg train loss = 3.3277\n",
            "Step 700 avg train loss = 3.3432\n",
            "Step 800 avg train loss = 3.3481\n",
            "Step 900 avg train loss = 3.3556\n",
            "Step 1000 avg train loss = 3.3667\n",
            "Step 1100 avg train loss = 3.3778\n",
            "Step 1200 avg train loss = 3.3893\n",
            "Step 1300 avg train loss = 3.3955\n",
            "Step 1400 avg train loss = 3.4064\n",
            "Step 1500 avg train loss = 3.4020\n",
            "Step 1600 avg train loss = 3.4201\n",
            "Step 1700 avg train loss = 3.4314\n",
            "Step 1800 avg train loss = 3.4327\n",
            "Step 1900 avg train loss = 3.4203\n",
            "Step 2000 avg train loss = 3.4670\n",
            "Step 2100 avg train loss = 3.4326\n",
            "Step 2200 avg train loss = 3.4685\n",
            "Step 2300 avg train loss = 3.4706\n",
            "Step 2400 avg train loss = 3.4718\n",
            "Validation loss after 11 epoch = 5.7811\n",
            "Step 0 avg train loss = 3.1593\n",
            "Step 100 avg train loss = 3.1740\n",
            "Step 200 avg train loss = 3.1943\n",
            "Step 300 avg train loss = 3.2210\n",
            "Step 400 avg train loss = 3.2295\n",
            "Step 500 avg train loss = 3.2266\n",
            "Step 600 avg train loss = 3.2484\n",
            "Step 700 avg train loss = 3.2586\n",
            "Step 800 avg train loss = 3.2983\n",
            "Step 900 avg train loss = 3.2702\n",
            "Step 1000 avg train loss = 3.2896\n",
            "Step 1100 avg train loss = 3.2868\n",
            "Step 1200 avg train loss = 3.2988\n",
            "Step 1300 avg train loss = 3.2955\n",
            "Step 1400 avg train loss = 3.3128\n",
            "Step 1500 avg train loss = 3.3274\n",
            "Step 1600 avg train loss = 3.3191\n",
            "Step 1700 avg train loss = 3.3373\n",
            "Step 1800 avg train loss = 3.3478\n",
            "Step 1900 avg train loss = 3.3784\n",
            "Step 2000 avg train loss = 3.3317\n",
            "Step 2100 avg train loss = 3.3659\n",
            "Step 2200 avg train loss = 3.3760\n",
            "Step 2300 avg train loss = 3.3654\n",
            "Step 2400 avg train loss = 3.3821\n",
            "Validation loss after 12 epoch = 5.8643\n",
            "Step 0 avg train loss = 3.0546\n",
            "Step 100 avg train loss = 3.0860\n",
            "Step 200 avg train loss = 3.1035\n",
            "Step 300 avg train loss = 3.1314\n",
            "Step 400 avg train loss = 3.1468\n",
            "Step 500 avg train loss = 3.1111\n",
            "Step 600 avg train loss = 3.1740\n",
            "Step 700 avg train loss = 3.1720\n",
            "Step 800 avg train loss = 3.1812\n",
            "Step 900 avg train loss = 3.2001\n",
            "Step 1000 avg train loss = 3.2121\n",
            "Step 1100 avg train loss = 3.2036\n",
            "Step 1200 avg train loss = 3.2392\n",
            "Step 1300 avg train loss = 3.2119\n",
            "Step 1400 avg train loss = 3.2444\n",
            "Step 1500 avg train loss = 3.2563\n",
            "Step 1600 avg train loss = 3.2646\n",
            "Step 1700 avg train loss = 3.2761\n",
            "Step 1800 avg train loss = 3.2711\n",
            "Step 1900 avg train loss = 3.2788\n",
            "Step 2000 avg train loss = 3.2643\n",
            "Step 2100 avg train loss = 3.3107\n",
            "Step 2200 avg train loss = 3.2955\n",
            "Step 2300 avg train loss = 3.2918\n",
            "Step 2400 avg train loss = 3.2972\n",
            "Validation loss after 13 epoch = 5.9623\n",
            "Step 0 avg train loss = 2.9659\n",
            "Step 100 avg train loss = 3.0193\n",
            "Step 200 avg train loss = 3.0386\n",
            "Step 300 avg train loss = 3.0568\n",
            "Step 400 avg train loss = 3.0749\n",
            "Step 500 avg train loss = 3.0801\n",
            "Step 600 avg train loss = 3.0783\n",
            "Step 700 avg train loss = 3.0918\n",
            "Step 800 avg train loss = 3.1381\n",
            "Step 900 avg train loss = 3.1109\n",
            "Step 1000 avg train loss = 3.1379\n",
            "Step 1100 avg train loss = 3.1388\n",
            "Step 1200 avg train loss = 3.1538\n",
            "Step 1300 avg train loss = 3.1400\n",
            "Step 1400 avg train loss = 3.1523\n",
            "Step 1500 avg train loss = 3.1817\n",
            "Step 1600 avg train loss = 3.1791\n",
            "Step 1700 avg train loss = 3.1988\n",
            "Step 1800 avg train loss = 3.1831\n",
            "Step 1900 avg train loss = 3.2086\n",
            "Step 2000 avg train loss = 3.2190\n",
            "Step 2100 avg train loss = 3.1977\n",
            "Step 2200 avg train loss = 3.2158\n",
            "Step 2300 avg train loss = 3.2463\n",
            "Step 2400 avg train loss = 3.2277\n",
            "Validation loss after 14 epoch = 6.0368\n",
            "Step 0 avg train loss = 2.9817\n",
            "Step 100 avg train loss = 2.9552\n",
            "Step 200 avg train loss = 2.9650\n",
            "Step 300 avg train loss = 2.9853\n",
            "Step 400 avg train loss = 2.9887\n",
            "Step 500 avg train loss = 3.0032\n",
            "Step 600 avg train loss = 3.0215\n",
            "Step 700 avg train loss = 3.0294\n",
            "Step 800 avg train loss = 3.0399\n",
            "Step 900 avg train loss = 3.0567\n",
            "Step 1000 avg train loss = 3.0624\n",
            "Step 1100 avg train loss = 3.0673\n",
            "Step 1200 avg train loss = 3.0825\n",
            "Step 1300 avg train loss = 3.0895\n",
            "Step 1400 avg train loss = 3.1259\n",
            "Step 1500 avg train loss = 3.0896\n",
            "Step 1600 avg train loss = 3.1016\n",
            "Step 1700 avg train loss = 3.1299\n",
            "Step 1800 avg train loss = 3.1363\n",
            "Step 1900 avg train loss = 3.1474\n",
            "Step 2000 avg train loss = 3.1495\n",
            "Step 2100 avg train loss = 3.1356\n",
            "Step 2200 avg train loss = 3.1607\n",
            "Step 2300 avg train loss = 3.1621\n",
            "Step 2400 avg train loss = 3.1593\n",
            "Validation loss after 15 epoch = 6.1278\n",
            "Step 0 avg train loss = 2.9548\n",
            "Step 100 avg train loss = 2.8902\n",
            "Step 200 avg train loss = 2.9097\n",
            "Step 300 avg train loss = 2.9183\n",
            "Step 400 avg train loss = 2.9435\n",
            "Step 500 avg train loss = 2.9313\n",
            "Step 600 avg train loss = 2.9565\n",
            "Step 700 avg train loss = 2.9946\n",
            "Step 800 avg train loss = 2.9731\n",
            "Step 900 avg train loss = 2.9919\n",
            "Step 1000 avg train loss = 2.9844\n",
            "Step 1100 avg train loss = 3.0177\n",
            "Step 1200 avg train loss = 3.0334\n",
            "Step 1300 avg train loss = 3.0319\n",
            "Step 1400 avg train loss = 3.0392\n",
            "Step 1500 avg train loss = 3.0507\n",
            "Step 1600 avg train loss = 3.0552\n",
            "Step 1700 avg train loss = 3.0480\n",
            "Step 1800 avg train loss = 3.0520\n",
            "Step 1900 avg train loss = 3.0781\n",
            "Step 2000 avg train loss = 3.0756\n",
            "Step 2100 avg train loss = 3.0740\n",
            "Step 2200 avg train loss = 3.0842\n",
            "Step 2300 avg train loss = 3.1047\n",
            "Step 2400 avg train loss = 3.0946\n",
            "Validation loss after 16 epoch = 6.1878\n",
            "Step 0 avg train loss = 2.9692\n",
            "Step 100 avg train loss = 2.8490\n",
            "Step 200 avg train loss = 2.8594\n",
            "Step 300 avg train loss = 2.8711\n",
            "Step 400 avg train loss = 2.8764\n",
            "Step 500 avg train loss = 2.8777\n",
            "Step 600 avg train loss = 2.8987\n",
            "Step 700 avg train loss = 2.9009\n",
            "Step 800 avg train loss = 2.9115\n",
            "Step 900 avg train loss = 2.9392\n",
            "Step 1000 avg train loss = 2.9389\n",
            "Step 1100 avg train loss = 2.9538\n",
            "Step 1200 avg train loss = 2.9782\n",
            "Step 1300 avg train loss = 2.9729\n",
            "Step 1400 avg train loss = 2.9739\n",
            "Step 1500 avg train loss = 2.9782\n",
            "Step 1600 avg train loss = 2.9799\n",
            "Step 1700 avg train loss = 3.0012\n",
            "Step 1800 avg train loss = 3.0020\n",
            "Step 1900 avg train loss = 3.0152\n",
            "Step 2000 avg train loss = 3.0033\n",
            "Step 2100 avg train loss = 3.0122\n",
            "Step 2200 avg train loss = 3.0447\n",
            "Step 2300 avg train loss = 3.0528\n",
            "Step 2400 avg train loss = 3.0611\n",
            "Validation loss after 17 epoch = 6.2703\n",
            "Step 0 avg train loss = 2.6015\n",
            "Step 100 avg train loss = 2.7929\n",
            "Step 200 avg train loss = 2.8007\n",
            "Step 300 avg train loss = 2.8053\n",
            "Step 400 avg train loss = 2.8242\n",
            "Step 500 avg train loss = 2.8374\n",
            "Step 600 avg train loss = 2.8434\n",
            "Step 700 avg train loss = 2.8758\n",
            "Step 800 avg train loss = 2.8585\n",
            "Step 900 avg train loss = 2.8912\n",
            "Step 1000 avg train loss = 2.8690\n",
            "Step 1100 avg train loss = 2.8939\n",
            "Step 1200 avg train loss = 2.8830\n",
            "Step 1300 avg train loss = 2.9047\n",
            "Step 1400 avg train loss = 2.9210\n",
            "Step 1500 avg train loss = 2.9172\n",
            "Step 1600 avg train loss = 2.9197\n",
            "Step 1700 avg train loss = 2.9678\n",
            "Step 1800 avg train loss = 2.9587\n",
            "Step 1900 avg train loss = 2.9517\n",
            "Step 2000 avg train loss = 2.9617\n",
            "Step 2100 avg train loss = 2.9888\n",
            "Step 2200 avg train loss = 2.9582\n",
            "Step 2300 avg train loss = 3.0080\n",
            "Step 2400 avg train loss = 2.9950\n",
            "Validation loss after 18 epoch = 6.3557\n",
            "Step 0 avg train loss = 2.8078\n",
            "Step 100 avg train loss = 2.7176\n",
            "Step 200 avg train loss = 2.7478\n",
            "Step 300 avg train loss = 2.7561\n",
            "Step 400 avg train loss = 2.7776\n",
            "Step 500 avg train loss = 2.7945\n",
            "Step 600 avg train loss = 2.7926\n",
            "Step 700 avg train loss = 2.8061\n",
            "Step 800 avg train loss = 2.8010\n",
            "Step 900 avg train loss = 2.8376\n",
            "Step 1000 avg train loss = 2.8438\n",
            "Step 1100 avg train loss = 2.8430\n",
            "Step 1200 avg train loss = 2.8473\n",
            "Step 1300 avg train loss = 2.8606\n",
            "Step 1400 avg train loss = 2.8584\n",
            "Step 1500 avg train loss = 2.8948\n",
            "Step 1600 avg train loss = 2.8853\n",
            "Step 1700 avg train loss = 2.8977\n",
            "Step 1800 avg train loss = 2.8820\n",
            "Step 1900 avg train loss = 2.9080\n",
            "Step 2000 avg train loss = 2.9208\n",
            "Step 2100 avg train loss = 2.9194\n",
            "Step 2200 avg train loss = 2.9315\n",
            "Step 2300 avg train loss = 2.9370\n",
            "Step 2400 avg train loss = 2.9421\n",
            "Validation loss after 19 epoch = 6.4216\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4077\n",
            "Step 100 avg train loss = 7.6064\n",
            "Step 200 avg train loss = 7.1188\n",
            "Step 300 avg train loss = 6.8872\n",
            "Step 400 avg train loss = 6.7077\n",
            "Step 500 avg train loss = 6.5587\n",
            "Step 600 avg train loss = 6.4424\n",
            "Step 700 avg train loss = 6.3522\n",
            "Step 800 avg train loss = 6.2640\n",
            "Step 900 avg train loss = 6.1831\n",
            "Step 1000 avg train loss = 6.1223\n",
            "Step 1100 avg train loss = 6.0599\n",
            "Step 1200 avg train loss = 5.9943\n",
            "Step 1300 avg train loss = 5.9851\n",
            "Step 1400 avg train loss = 5.9378\n",
            "Step 1500 avg train loss = 5.9196\n",
            "Step 1600 avg train loss = 5.8563\n",
            "Step 1700 avg train loss = 5.8292\n",
            "Step 1800 avg train loss = 5.8014\n",
            "Step 1900 avg train loss = 5.7638\n",
            "Step 2000 avg train loss = 5.7581\n",
            "Step 2100 avg train loss = 5.7021\n",
            "Step 2200 avg train loss = 5.6820\n",
            "Step 2300 avg train loss = 5.6952\n",
            "Step 2400 avg train loss = 5.6331\n",
            "Validation loss after 0 epoch = 5.4937\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.7384\n",
            "Step 100 avg train loss = 5.4760\n",
            "Step 200 avg train loss = 5.4694\n",
            "Step 300 avg train loss = 5.4564\n",
            "Step 400 avg train loss = 5.4394\n",
            "Step 500 avg train loss = 5.4185\n",
            "Step 600 avg train loss = 5.3846\n",
            "Step 700 avg train loss = 5.3973\n",
            "Step 800 avg train loss = 5.3728\n",
            "Step 900 avg train loss = 5.3445\n",
            "Step 1000 avg train loss = 5.3464\n",
            "Step 1100 avg train loss = 5.3261\n",
            "Step 1200 avg train loss = 5.3276\n",
            "Step 1300 avg train loss = 5.2857\n",
            "Step 1400 avg train loss = 5.3209\n",
            "Step 1500 avg train loss = 5.2787\n",
            "Step 1600 avg train loss = 5.2963\n",
            "Step 1700 avg train loss = 5.2553\n",
            "Step 1800 avg train loss = 5.2516\n",
            "Step 1900 avg train loss = 5.2371\n",
            "Step 2000 avg train loss = 5.2500\n",
            "Step 2100 avg train loss = 5.2220\n",
            "Step 2200 avg train loss = 5.1821\n",
            "Step 2300 avg train loss = 5.1965\n",
            "Step 2400 avg train loss = 5.2011\n",
            "Validation loss after 1 epoch = 5.2352\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.1050\n",
            "Step 100 avg train loss = 4.9629\n",
            "Step 200 avg train loss = 4.9514\n",
            "Step 300 avg train loss = 4.9862\n",
            "Step 400 avg train loss = 4.9708\n",
            "Step 500 avg train loss = 4.9591\n",
            "Step 600 avg train loss = 4.9485\n",
            "Step 700 avg train loss = 4.9427\n",
            "Step 800 avg train loss = 4.9337\n",
            "Step 900 avg train loss = 4.9361\n",
            "Step 1000 avg train loss = 4.9320\n",
            "Step 1100 avg train loss = 4.9463\n",
            "Step 1200 avg train loss = 4.9101\n",
            "Step 1300 avg train loss = 4.8964\n",
            "Step 1400 avg train loss = 4.9356\n",
            "Step 1500 avg train loss = 4.9182\n",
            "Step 1600 avg train loss = 4.8843\n",
            "Step 1700 avg train loss = 4.9028\n",
            "Step 1800 avg train loss = 4.8957\n",
            "Step 1900 avg train loss = 4.8639\n",
            "Step 2000 avg train loss = 4.8747\n",
            "Step 2100 avg train loss = 4.8480\n",
            "Step 2200 avg train loss = 4.8580\n",
            "Step 2300 avg train loss = 4.8615\n",
            "Step 2400 avg train loss = 4.8667\n",
            "Validation loss after 2 epoch = 5.1417\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.7328\n",
            "Step 100 avg train loss = 4.6224\n",
            "Step 200 avg train loss = 4.6219\n",
            "Step 300 avg train loss = 4.5910\n",
            "Step 400 avg train loss = 4.6119\n",
            "Step 500 avg train loss = 4.6219\n",
            "Step 600 avg train loss = 4.6087\n",
            "Step 700 avg train loss = 4.5887\n",
            "Step 800 avg train loss = 4.6004\n",
            "Step 900 avg train loss = 4.6072\n",
            "Step 1000 avg train loss = 4.6042\n",
            "Step 1100 avg train loss = 4.5923\n",
            "Step 1200 avg train loss = 4.6117\n",
            "Step 1300 avg train loss = 4.5944\n",
            "Step 1400 avg train loss = 4.6115\n",
            "Step 1500 avg train loss = 4.6043\n",
            "Step 1600 avg train loss = 4.6114\n",
            "Step 1700 avg train loss = 4.5812\n",
            "Step 1800 avg train loss = 4.5925\n",
            "Step 1900 avg train loss = 4.5814\n",
            "Step 2000 avg train loss = 4.5873\n",
            "Step 2100 avg train loss = 4.5977\n",
            "Step 2200 avg train loss = 4.5885\n",
            "Step 2300 avg train loss = 4.5981\n",
            "Step 2400 avg train loss = 4.5930\n",
            "Validation loss after 3 epoch = 5.1347\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.3048\n",
            "Step 100 avg train loss = 4.3094\n",
            "Step 200 avg train loss = 4.2887\n",
            "Step 300 avg train loss = 4.3226\n",
            "Step 400 avg train loss = 4.3107\n",
            "Step 500 avg train loss = 4.3007\n",
            "Step 600 avg train loss = 4.3311\n",
            "Step 700 avg train loss = 4.3349\n",
            "Step 800 avg train loss = 4.3399\n",
            "Step 900 avg train loss = 4.3247\n",
            "Step 1000 avg train loss = 4.3379\n",
            "Step 1100 avg train loss = 4.3310\n",
            "Step 1200 avg train loss = 4.3613\n",
            "Step 1300 avg train loss = 4.3321\n",
            "Step 1400 avg train loss = 4.3506\n",
            "Step 1500 avg train loss = 4.3295\n",
            "Step 1600 avg train loss = 4.3580\n",
            "Step 1700 avg train loss = 4.3405\n",
            "Step 1800 avg train loss = 4.3327\n",
            "Step 1900 avg train loss = 4.3413\n",
            "Step 2000 avg train loss = 4.3332\n",
            "Step 2100 avg train loss = 4.3441\n",
            "Step 2200 avg train loss = 4.3529\n",
            "Step 2300 avg train loss = 4.3523\n",
            "Step 2400 avg train loss = 4.3593\n",
            "Validation loss after 4 epoch = 5.1925\n",
            "Step 0 avg train loss = 4.0341\n",
            "Step 100 avg train loss = 4.0648\n",
            "Step 200 avg train loss = 4.0773\n",
            "Step 300 avg train loss = 4.0565\n",
            "Step 400 avg train loss = 4.0659\n",
            "Step 500 avg train loss = 4.0704\n",
            "Step 600 avg train loss = 4.0727\n",
            "Step 700 avg train loss = 4.0718\n",
            "Step 800 avg train loss = 4.0730\n",
            "Step 900 avg train loss = 4.1037\n",
            "Step 1000 avg train loss = 4.1036\n",
            "Step 1100 avg train loss = 4.1140\n",
            "Step 1200 avg train loss = 4.1229\n",
            "Step 1300 avg train loss = 4.1157\n",
            "Step 1400 avg train loss = 4.1211\n",
            "Step 1500 avg train loss = 4.1316\n",
            "Step 1600 avg train loss = 4.1158\n",
            "Step 1700 avg train loss = 4.1222\n",
            "Step 1800 avg train loss = 4.1366\n",
            "Step 1900 avg train loss = 4.1232\n",
            "Step 2000 avg train loss = 4.1374\n",
            "Step 2100 avg train loss = 4.1356\n",
            "Step 2200 avg train loss = 4.1401\n",
            "Step 2300 avg train loss = 4.1441\n",
            "Step 2400 avg train loss = 4.1589\n",
            "Validation loss after 5 epoch = 5.2773\n",
            "Step 0 avg train loss = 3.7871\n",
            "Step 100 avg train loss = 3.8495\n",
            "Step 200 avg train loss = 3.8627\n",
            "Step 300 avg train loss = 3.8637\n",
            "Step 400 avg train loss = 3.8582\n",
            "Step 500 avg train loss = 3.8678\n",
            "Step 600 avg train loss = 3.8964\n",
            "Step 700 avg train loss = 3.8671\n",
            "Step 800 avg train loss = 3.9013\n",
            "Step 900 avg train loss = 3.9142\n",
            "Step 1000 avg train loss = 3.8942\n",
            "Step 1100 avg train loss = 3.9272\n",
            "Step 1200 avg train loss = 3.9077\n",
            "Step 1300 avg train loss = 3.9375\n",
            "Step 1400 avg train loss = 3.9307\n",
            "Step 1500 avg train loss = 3.9406\n",
            "Step 1600 avg train loss = 3.9097\n",
            "Step 1700 avg train loss = 3.9314\n",
            "Step 1800 avg train loss = 3.9388\n",
            "Step 1900 avg train loss = 3.9174\n",
            "Step 2000 avg train loss = 3.9537\n",
            "Step 2100 avg train loss = 3.9427\n",
            "Step 2200 avg train loss = 3.9539\n",
            "Step 2300 avg train loss = 3.9729\n",
            "Step 2400 avg train loss = 3.9640\n",
            "Validation loss after 6 epoch = 5.3755\n",
            "Step 0 avg train loss = 3.7822\n",
            "Step 100 avg train loss = 3.6365\n",
            "Step 200 avg train loss = 3.6531\n",
            "Step 300 avg train loss = 3.6741\n",
            "Step 400 avg train loss = 3.6637\n",
            "Step 500 avg train loss = 3.7043\n",
            "Step 600 avg train loss = 3.6737\n",
            "Step 700 avg train loss = 3.7231\n",
            "Step 800 avg train loss = 3.7235\n",
            "Step 900 avg train loss = 3.7163\n",
            "Step 1000 avg train loss = 3.7501\n",
            "Step 1100 avg train loss = 3.7475\n",
            "Step 1200 avg train loss = 3.7483\n",
            "Step 1300 avg train loss = 3.7670\n",
            "Step 1400 avg train loss = 3.7549\n",
            "Step 1500 avg train loss = 3.7761\n",
            "Step 1600 avg train loss = 3.7613\n",
            "Step 1700 avg train loss = 3.7767\n",
            "Step 1800 avg train loss = 3.7785\n",
            "Step 1900 avg train loss = 3.7836\n",
            "Step 2000 avg train loss = 3.8006\n",
            "Step 2100 avg train loss = 3.7794\n",
            "Step 2200 avg train loss = 3.7850\n",
            "Step 2300 avg train loss = 3.8056\n",
            "Step 2400 avg train loss = 3.8014\n",
            "Validation loss after 7 epoch = 5.4719\n",
            "Step 0 avg train loss = 3.5151\n",
            "Step 100 avg train loss = 3.4982\n",
            "Step 200 avg train loss = 3.5009\n",
            "Step 300 avg train loss = 3.5243\n",
            "Step 400 avg train loss = 3.5202\n",
            "Step 500 avg train loss = 3.5344\n",
            "Step 600 avg train loss = 3.5566\n",
            "Step 700 avg train loss = 3.5698\n",
            "Step 800 avg train loss = 3.5740\n",
            "Step 900 avg train loss = 3.5883\n",
            "Step 1000 avg train loss = 3.5904\n",
            "Step 1100 avg train loss = 3.5711\n",
            "Step 1200 avg train loss = 3.5960\n",
            "Step 1300 avg train loss = 3.6224\n",
            "Step 1400 avg train loss = 3.6314\n",
            "Step 1500 avg train loss = 3.6115\n",
            "Step 1600 avg train loss = 3.6116\n",
            "Step 1700 avg train loss = 3.6117\n",
            "Step 1800 avg train loss = 3.6321\n",
            "Step 1900 avg train loss = 3.6276\n",
            "Step 2000 avg train loss = 3.6327\n",
            "Step 2100 avg train loss = 3.6326\n",
            "Step 2200 avg train loss = 3.6578\n",
            "Step 2300 avg train loss = 3.6335\n",
            "Step 2400 avg train loss = 3.6607\n",
            "Validation loss after 8 epoch = 5.5715\n",
            "Step 0 avg train loss = 3.3935\n",
            "Step 100 avg train loss = 3.3383\n",
            "Step 200 avg train loss = 3.3612\n",
            "Step 300 avg train loss = 3.3737\n",
            "Step 400 avg train loss = 3.3933\n",
            "Step 500 avg train loss = 3.3982\n",
            "Step 600 avg train loss = 3.4095\n",
            "Step 700 avg train loss = 3.4026\n",
            "Step 800 avg train loss = 3.4239\n",
            "Step 900 avg train loss = 3.4404\n",
            "Step 1000 avg train loss = 3.4640\n",
            "Step 1100 avg train loss = 3.4526\n",
            "Step 1200 avg train loss = 3.4546\n",
            "Step 1300 avg train loss = 3.4692\n",
            "Step 1400 avg train loss = 3.4936\n",
            "Step 1500 avg train loss = 3.4688\n",
            "Step 1600 avg train loss = 3.4947\n",
            "Step 1700 avg train loss = 3.5107\n",
            "Step 1800 avg train loss = 3.5144\n",
            "Step 1900 avg train loss = 3.4911\n",
            "Step 2000 avg train loss = 3.5131\n",
            "Step 2100 avg train loss = 3.5350\n",
            "Step 2200 avg train loss = 3.5237\n",
            "Step 2300 avg train loss = 3.5331\n",
            "Step 2400 avg train loss = 3.5294\n",
            "Validation loss after 9 epoch = 5.6814\n",
            "Step 0 avg train loss = 3.0327\n",
            "Step 100 avg train loss = 3.2090\n",
            "Step 200 avg train loss = 3.2490\n",
            "Step 300 avg train loss = 3.2668\n",
            "Step 400 avg train loss = 3.2697\n",
            "Step 500 avg train loss = 3.2815\n",
            "Step 600 avg train loss = 3.2876\n",
            "Step 700 avg train loss = 3.3146\n",
            "Step 800 avg train loss = 3.3071\n",
            "Step 900 avg train loss = 3.3215\n",
            "Step 1000 avg train loss = 3.3224\n",
            "Step 1100 avg train loss = 3.3219\n",
            "Step 1200 avg train loss = 3.3379\n",
            "Step 1300 avg train loss = 3.3503\n",
            "Step 1400 avg train loss = 3.3554\n",
            "Step 1500 avg train loss = 3.3446\n",
            "Step 1600 avg train loss = 3.3939\n",
            "Step 1700 avg train loss = 3.3791\n",
            "Step 1800 avg train loss = 3.3890\n",
            "Step 1900 avg train loss = 3.3869\n",
            "Step 2000 avg train loss = 3.4056\n",
            "Step 2100 avg train loss = 3.3935\n",
            "Step 2200 avg train loss = 3.3943\n",
            "Step 2300 avg train loss = 3.4037\n",
            "Step 2400 avg train loss = 3.4180\n",
            "Validation loss after 10 epoch = 5.7895\n",
            "Step 0 avg train loss = 2.8903\n",
            "Step 100 avg train loss = 3.1383\n",
            "Step 200 avg train loss = 3.1143\n",
            "Step 300 avg train loss = 3.1353\n",
            "Step 400 avg train loss = 3.1768\n",
            "Step 500 avg train loss = 3.1719\n",
            "Step 600 avg train loss = 3.1761\n",
            "Step 700 avg train loss = 3.1903\n",
            "Step 800 avg train loss = 3.1938\n",
            "Step 900 avg train loss = 3.2128\n",
            "Step 1000 avg train loss = 3.1902\n",
            "Step 1100 avg train loss = 3.2145\n",
            "Step 1200 avg train loss = 3.2526\n",
            "Step 1300 avg train loss = 3.2469\n",
            "Step 1400 avg train loss = 3.2426\n",
            "Step 1500 avg train loss = 3.2509\n",
            "Step 1600 avg train loss = 3.2589\n",
            "Step 1700 avg train loss = 3.2584\n",
            "Step 1800 avg train loss = 3.2709\n",
            "Step 1900 avg train loss = 3.2852\n",
            "Step 2000 avg train loss = 3.2846\n",
            "Step 2100 avg train loss = 3.2778\n",
            "Step 2200 avg train loss = 3.2944\n",
            "Step 2300 avg train loss = 3.3308\n",
            "Step 2400 avg train loss = 3.3094\n",
            "Validation loss after 11 epoch = 5.8828\n",
            "Step 0 avg train loss = 3.0910\n",
            "Step 100 avg train loss = 3.0178\n",
            "Step 200 avg train loss = 3.0330\n",
            "Step 300 avg train loss = 3.0473\n",
            "Step 400 avg train loss = 3.0549\n",
            "Step 500 avg train loss = 3.0530\n",
            "Step 600 avg train loss = 3.0847\n",
            "Step 700 avg train loss = 3.0864\n",
            "Step 800 avg train loss = 3.0970\n",
            "Step 900 avg train loss = 3.1050\n",
            "Step 1000 avg train loss = 3.1211\n",
            "Step 1100 avg train loss = 3.1404\n",
            "Step 1200 avg train loss = 3.1259\n",
            "Step 1300 avg train loss = 3.1181\n",
            "Step 1400 avg train loss = 3.1375\n",
            "Step 1500 avg train loss = 3.1584\n",
            "Step 1600 avg train loss = 3.1662\n",
            "Step 1700 avg train loss = 3.1725\n",
            "Step 1800 avg train loss = 3.1680\n",
            "Step 1900 avg train loss = 3.1846\n",
            "Step 2000 avg train loss = 3.1896\n",
            "Step 2100 avg train loss = 3.1954\n",
            "Step 2200 avg train loss = 3.1878\n",
            "Step 2300 avg train loss = 3.2092\n",
            "Step 2400 avg train loss = 3.2150\n",
            "Validation loss after 12 epoch = 5.9675\n",
            "Step 0 avg train loss = 2.9783\n",
            "Step 100 avg train loss = 2.9048\n",
            "Step 200 avg train loss = 2.9429\n",
            "Step 300 avg train loss = 2.9448\n",
            "Step 400 avg train loss = 2.9676\n",
            "Step 500 avg train loss = 2.9684\n",
            "Step 600 avg train loss = 2.9781\n",
            "Step 700 avg train loss = 2.9828\n",
            "Step 800 avg train loss = 2.9920\n",
            "Step 900 avg train loss = 3.0052\n",
            "Step 1000 avg train loss = 3.0129\n",
            "Step 1100 avg train loss = 3.0453\n",
            "Step 1200 avg train loss = 3.0511\n",
            "Step 1300 avg train loss = 3.0580\n",
            "Step 1400 avg train loss = 3.0458\n",
            "Step 1500 avg train loss = 3.0590\n",
            "Step 1600 avg train loss = 3.0758\n",
            "Step 1700 avg train loss = 3.0930\n",
            "Step 1800 avg train loss = 3.0658\n",
            "Step 1900 avg train loss = 3.1115\n",
            "Step 2000 avg train loss = 3.1061\n",
            "Step 2100 avg train loss = 3.1086\n",
            "Step 2200 avg train loss = 3.1080\n",
            "Step 2300 avg train loss = 3.1236\n",
            "Step 2400 avg train loss = 3.1132\n",
            "Validation loss after 13 epoch = 6.0703\n",
            "Step 0 avg train loss = 2.8731\n",
            "Step 100 avg train loss = 2.8398\n",
            "Step 200 avg train loss = 2.8621\n",
            "Step 300 avg train loss = 2.8618\n",
            "Step 400 avg train loss = 2.8679\n",
            "Step 500 avg train loss = 2.8782\n",
            "Step 600 avg train loss = 2.8914\n",
            "Step 700 avg train loss = 2.9137\n",
            "Step 800 avg train loss = 2.9158\n",
            "Step 900 avg train loss = 2.9425\n",
            "Step 1000 avg train loss = 2.9437\n",
            "Step 1100 avg train loss = 2.9451\n",
            "Step 1200 avg train loss = 2.9310\n",
            "Step 1300 avg train loss = 2.9597\n",
            "Step 1400 avg train loss = 2.9745\n",
            "Step 1500 avg train loss = 2.9727\n",
            "Step 1600 avg train loss = 2.9810\n",
            "Step 1700 avg train loss = 2.9899\n",
            "Step 1800 avg train loss = 3.0147\n",
            "Step 1900 avg train loss = 3.0164\n",
            "Step 2000 avg train loss = 3.0084\n",
            "Step 2100 avg train loss = 3.0050\n",
            "Step 2200 avg train loss = 3.0206\n",
            "Step 2300 avg train loss = 3.0476\n",
            "Step 2400 avg train loss = 3.0498\n",
            "Validation loss after 14 epoch = 6.1553\n",
            "Step 0 avg train loss = 2.7818\n",
            "Step 100 avg train loss = 2.7490\n",
            "Step 200 avg train loss = 2.7735\n",
            "Step 300 avg train loss = 2.7666\n",
            "Step 400 avg train loss = 2.7970\n",
            "Step 500 avg train loss = 2.8040\n",
            "Step 600 avg train loss = 2.8197\n",
            "Step 700 avg train loss = 2.8474\n",
            "Step 800 avg train loss = 2.8349\n",
            "Step 900 avg train loss = 2.8583\n",
            "Step 1000 avg train loss = 2.8626\n",
            "Step 1100 avg train loss = 2.8696\n",
            "Step 1200 avg train loss = 2.8801\n",
            "Step 1300 avg train loss = 2.8791\n",
            "Step 1400 avg train loss = 2.9030\n",
            "Step 1500 avg train loss = 2.9135\n",
            "Step 1600 avg train loss = 2.9102\n",
            "Step 1700 avg train loss = 2.9090\n",
            "Step 1800 avg train loss = 2.9311\n",
            "Step 1900 avg train loss = 2.9086\n",
            "Step 2000 avg train loss = 2.9217\n",
            "Step 2100 avg train loss = 2.9261\n",
            "Step 2200 avg train loss = 2.9566\n",
            "Step 2300 avg train loss = 2.9619\n",
            "Step 2400 avg train loss = 2.9721\n",
            "Validation loss after 15 epoch = 6.2485\n",
            "Step 0 avg train loss = 2.5668\n",
            "Step 100 avg train loss = 2.6929\n",
            "Step 200 avg train loss = 2.6929\n",
            "Step 300 avg train loss = 2.7125\n",
            "Step 400 avg train loss = 2.7135\n",
            "Step 500 avg train loss = 2.7303\n",
            "Step 600 avg train loss = 2.7504\n",
            "Step 700 avg train loss = 2.7528\n",
            "Step 800 avg train loss = 2.7709\n",
            "Step 900 avg train loss = 2.7720\n",
            "Step 1000 avg train loss = 2.7981\n",
            "Step 1100 avg train loss = 2.7916\n",
            "Step 1200 avg train loss = 2.7920\n",
            "Step 1300 avg train loss = 2.8145\n",
            "Step 1400 avg train loss = 2.8174\n",
            "Step 1500 avg train loss = 2.8180\n",
            "Step 1600 avg train loss = 2.8438\n",
            "Step 1700 avg train loss = 2.8491\n",
            "Step 1800 avg train loss = 2.8554\n",
            "Step 1900 avg train loss = 2.8658\n",
            "Step 2000 avg train loss = 2.8525\n",
            "Step 2100 avg train loss = 2.8779\n",
            "Step 2200 avg train loss = 2.8815\n",
            "Step 2300 avg train loss = 2.8795\n",
            "Step 2400 avg train loss = 2.8964\n",
            "Validation loss after 16 epoch = 6.3413\n",
            "Step 0 avg train loss = 2.6056\n",
            "Step 100 avg train loss = 2.5963\n",
            "Step 200 avg train loss = 2.6145\n",
            "Step 300 avg train loss = 2.6291\n",
            "Step 400 avg train loss = 2.6377\n",
            "Step 500 avg train loss = 2.6703\n",
            "Step 600 avg train loss = 2.6745\n",
            "Step 700 avg train loss = 2.6891\n",
            "Step 800 avg train loss = 2.6998\n",
            "Step 900 avg train loss = 2.7105\n",
            "Step 1000 avg train loss = 2.7068\n",
            "Step 1100 avg train loss = 2.7122\n",
            "Step 1200 avg train loss = 2.7520\n",
            "Step 1300 avg train loss = 2.7555\n",
            "Step 1400 avg train loss = 2.7477\n",
            "Step 1500 avg train loss = 2.7717\n",
            "Step 1600 avg train loss = 2.7634\n",
            "Step 1700 avg train loss = 2.7909\n",
            "Step 1800 avg train loss = 2.7942\n",
            "Step 1900 avg train loss = 2.7838\n",
            "Step 2000 avg train loss = 2.8128\n",
            "Step 2100 avg train loss = 2.8115\n",
            "Step 2200 avg train loss = 2.8254\n",
            "Step 2300 avg train loss = 2.8114\n",
            "Step 2400 avg train loss = 2.8193\n",
            "Validation loss after 17 epoch = 6.4270\n",
            "Step 0 avg train loss = 2.5140\n",
            "Step 100 avg train loss = 2.5560\n",
            "Step 200 avg train loss = 2.5445\n",
            "Step 300 avg train loss = 2.5535\n",
            "Step 400 avg train loss = 2.5887\n",
            "Step 500 avg train loss = 2.6163\n",
            "Step 600 avg train loss = 2.6158\n",
            "Step 700 avg train loss = 2.6324\n",
            "Step 800 avg train loss = 2.6554\n",
            "Step 900 avg train loss = 2.6444\n",
            "Step 1000 avg train loss = 2.6562\n",
            "Step 1100 avg train loss = 2.6605\n",
            "Step 1200 avg train loss = 2.6738\n",
            "Step 1300 avg train loss = 2.6762\n",
            "Step 1400 avg train loss = 2.6952\n",
            "Step 1500 avg train loss = 2.6882\n",
            "Step 1600 avg train loss = 2.7029\n",
            "Step 1700 avg train loss = 2.7159\n",
            "Step 1800 avg train loss = 2.7199\n",
            "Step 1900 avg train loss = 2.7224\n",
            "Step 2000 avg train loss = 2.7396\n",
            "Step 2100 avg train loss = 2.7458\n",
            "Step 2200 avg train loss = 2.7488\n",
            "Step 2300 avg train loss = 2.7504\n",
            "Step 2400 avg train loss = 2.7764\n",
            "Validation loss after 18 epoch = 6.5052\n",
            "Step 0 avg train loss = 2.2892\n",
            "Step 100 avg train loss = 2.4944\n",
            "Step 200 avg train loss = 2.5011\n",
            "Step 300 avg train loss = 2.5263\n",
            "Step 400 avg train loss = 2.5102\n",
            "Step 500 avg train loss = 2.5455\n",
            "Step 600 avg train loss = 2.5584\n",
            "Step 700 avg train loss = 2.5731\n",
            "Step 800 avg train loss = 2.5781\n",
            "Step 900 avg train loss = 2.5899\n",
            "Step 1000 avg train loss = 2.5859\n",
            "Step 1100 avg train loss = 2.6234\n",
            "Step 1200 avg train loss = 2.6151\n",
            "Step 1300 avg train loss = 2.6376\n",
            "Step 1400 avg train loss = 2.6130\n",
            "Step 1500 avg train loss = 2.6328\n",
            "Step 1600 avg train loss = 2.6496\n",
            "Step 1700 avg train loss = 2.6566\n",
            "Step 1800 avg train loss = 2.6552\n",
            "Step 1900 avg train loss = 2.6707\n",
            "Step 2000 avg train loss = 2.6854\n",
            "Step 2100 avg train loss = 2.6866\n",
            "Step 2200 avg train loss = 2.6947\n",
            "Step 2300 avg train loss = 2.7072\n",
            "Step 2400 avg train loss = 2.6940\n",
            "Validation loss after 19 epoch = 6.6090\n",
            "Saving best model with best hidden size...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c0b13b17a497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m'loss_cache'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplot_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m'model_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_model_overall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         }, './drive/My Drive/NLP/hid_tune_best_LSTM.pt')\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/My Drive/NLP/hid_tune_best_LSTM.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHWEAFKaMMuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQux1ZwmIFbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "733a29b3-9415-46ab-e83d-37d8f3db2565"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZK_pPW8Bvw",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "LcBmcCR_8Bvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtUgQcLw8Bv1",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcC--G6w8Bv3",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmYE5Zx8Bv4",
        "colab_type": "code",
        "outputId": "c2a7bf07-2026-4a35-c131-8b2805447f4b",
        "colab": {}
      },
      "source": [
        "# TODO: Get the best model parameter from 1.2.1 \n",
        "options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "model = LSTMModel(options)\n",
        "model.load_state_dict(torch.load('baseline_LSTM.pt')['model_dict'])\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 64, padding_idx=2)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQK27bsS8Bv-",
        "colab_type": "code",
        "outputId": "51a5b526-73aa-4b21-de21-dca3f4c3bd67",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy()) \n",
        "    # transfer tensor back to cpu\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['split']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDAT-P2L8BwC",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Pq1WyoUK8BwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(weight_matrix, words):\n",
        "#make sure at least appear in the text once \n",
        "    whole_word = list()\n",
        "    for word in words:\n",
        "        if word not in vocab:\n",
        "            raise NotImplementedError(\"selected token must appeared in the corpus once, please replace {}\".format(word))\n",
        "            \n",
        "    # define my cosine similarity. each input is 1 x 64. Therefore evaluate along x'axis returns a scalar \n",
        "    cos = nn.CosineSimilarity(dim = 0, eps = 1e-6)\n",
        "\n",
        "    dis_list = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    for w in words:\n",
        "        wid = train_dict.get_id(w)\n",
        "        temp_list = list()\n",
        "        for i in range(len(weight_matrix)):\n",
        "            if i == wid:\n",
        "                continue\n",
        "            else: \n",
        "                dis = cos(weight_matrix[wid], weight_matrix[i])\n",
        "                temp_list.append(tuple((train_dict.get_token(i), dis.item())))\n",
        "        temp_list = sorted(temp_list, key = lambda x: x[-1])\n",
        "        dis_list[w]['worst'] = temp_list[:10]\n",
        "        dis_list[w]['best'] = temp_list[-10:]\n",
        "        worst_words = [x[0] for x in temp_list[:10]]\n",
        "        best_words = [x[0] for x in temp_list[-10:]]\n",
        "        whole_word += worst_words + best_words\n",
        "    return dis_list, whole_word + words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9OXVOFX8BwI",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThaWEAgk8BwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_matrix_lkup = model.lookup.weight\n",
        "words = ['productive', 'teenage','south','antelope','smart']\n",
        "# some verbs, and nouns\n",
        "lkup_list, whole_words_lkup = cos_similarity(weight_matrix_lkup, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqtEzK2y8Bw5",
        "colab_type": "code",
        "outputId": "48a3ba75-814d-44ec-c47a-9f8fa6313b92",
        "colab": {}
      },
      "source": [
        "lkup_list['smart']['best']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fuelled', 0.4145849049091339),\n",
              " ('Bethlehem', 0.4171598553657532),\n",
              " ('window', 0.42124074697494507),\n",
              " ('drills', 0.4226926565170288),\n",
              " ('gamma', 0.4253535270690918),\n",
              " ('Trujillo', 0.4524407684803009),\n",
              " ('Helikopter', 0.45847856998443604),\n",
              " ('sufficient', 0.4664282500743866),\n",
              " ('gorgeously', 0.472387433052063),\n",
              " ('pyrite', 0.4758983552455902)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxt08B68Bw_",
        "colab_type": "code",
        "outputId": "e619a683-f4b7-412f-8590-08c77696a5c6",
        "colab": {}
      },
      "source": [
        "lkup_list['smart']['worst']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hartington', -0.4613579511642456),\n",
              " ('astonished', -0.45143312215805054),\n",
              " ('none', -0.446800172328949),\n",
              " ('Kannada', -0.4442085027694702),\n",
              " ('508', -0.43341436982154846),\n",
              " ('Moltke', -0.42936182022094727),\n",
              " ('ambush', -0.42904070019721985),\n",
              " ('ligand', -0.4222765266895294),\n",
              " ('skyline', -0.4182506203651428),\n",
              " ('surge', -0.4146639108657837)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyW51O608BxE",
        "colab_type": "code",
        "outputId": "e4b592bb-955f-43d0-f6c7-314b2cb7909e",
        "colab": {}
      },
      "source": [
        "whole_word_ids_lkup = train_dict.encode_token_seq(whole_words_lkup)   # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(weight_matrix_lkup, whole_word_ids_lkup, whole_words_lkup)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: \n",
            "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
            "\n",
            "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
            "\n",
            "File \"../../.conda/envs/myenv/lib/python3.6/site-packages/umap/rp_tree.py\", line 135:\n",
            "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
            "def euclidean_random_projection_split(data, indices, rng_state):\n",
            "^\n",
            "\n",
            "  self.func_ir.loc))\n",
            "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
            "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
            "\n",
            "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
            "\n",
            "File \"../../.conda/envs/myenv/lib/python3.6/site-packages/umap/utils.py\", line 409:\n",
            "@numba.njit(parallel=True)\n",
            "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
            "^\n",
            "\n",
            "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
            "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: \n",
            "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
            "\n",
            "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
            "\n",
            "File \"../../.conda/envs/myenv/lib/python3.6/site-packages/umap/nndescent.py\", line 47:\n",
            "    @numba.njit(parallel=True)\n",
            "    def nn_descent(\n",
            "    ^\n",
            "\n",
            "  self.func_ir.loc))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIMAAARiCAYAAAA3EzfQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2ab173bbb908>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8a_-xZz8BxG",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vUnIC2tQ8BxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_matrix_proj = model.projection.weight\n",
        "words = ['productive', 'teenage','south','antelope','smart']\n",
        "# some verbs, and nouns\n",
        "proj_list, whole_words_proj = cos_similarity(weight_matrix_proj, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThISdsEb8BxI",
        "colab_type": "code",
        "outputId": "60bbea9d-6f22-409b-e851-c33fb71000e4",
        "colab": {}
      },
      "source": [
        "proj_list['productive']['best']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rudders', 0.7861390113830566),\n",
              " ('dihedral', 0.7878164052963257),\n",
              " ('cruciform', 0.7883051633834839),\n",
              " ('webisode', 0.7900282144546509),\n",
              " ('dose', 0.7945231795310974),\n",
              " ('recourse', 0.7983407974243164),\n",
              " ('cun', 0.8006885647773743),\n",
              " ('moult', 0.8121520280838013),\n",
              " ('epidemic', 0.8251773715019226),\n",
              " ('smile', 0.8261529207229614)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGTpi0718BxK",
        "colab_type": "code",
        "outputId": "c67e2451-ae47-4c29-c131-b24856be4ea0",
        "colab": {}
      },
      "source": [
        "proj_list['productive']['worst']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', -0.34913504123687744),\n",
              " ('was', -0.26380446553230286),\n",
              " ('is', -0.23950541019439697),\n",
              " ('\"', -0.22181227803230286),\n",
              " ('the', -0.18868812918663025),\n",
              " ('<unk>', -0.17997287213802338),\n",
              " ('The', -0.16852892935276031),\n",
              " (',', -0.15113075077533722),\n",
              " ('it', -0.12440523505210876),\n",
              " ('<eos>', -0.11839520931243896)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y80MFDgD8BxM",
        "colab_type": "code",
        "outputId": "bb1d38b7-b75e-43a0-c468-b92b18cb35a3",
        "colab": {}
      },
      "source": [
        "whole_word_ids_proj = train_dict.encode_token_seq(whole_words_proj)   # e.g. use dictionary.get_id on a list of words\n",
        "umap_plot(weight_matrix_proj, whole_word_ids, whole_words_proj)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/sl7085/.conda/envs/myenv/lib/python3.6/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: \n",
            "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
            "\n",
            "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
            "\n",
            "File \"../../.conda/envs/myenv/lib/python3.6/site-packages/umap/nndescent.py\", line 47:\n",
            "    @numba.njit(parallel=True)\n",
            "    def nn_descent(\n",
            "    ^\n",
            "\n",
            "  self.func_ir.loc))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHsAAARiCAYAAAA0iPu9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x2ab172a061d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9TM9ui8BxQ",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PpWeHqeu8BxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tUpJJzp8BxT",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9H4dxIV88BxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iat6XrhD8BxW",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cUmH3BiQ8BxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElfxRup68Bxc",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0UQnM7M18Bxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wGTu5G_8Bxh",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qa0ig2zh8Bxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd3bYBH78Bxl",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RPWkp7I88Bxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}