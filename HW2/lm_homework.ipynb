{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1011 Homework 2\n",
    "## N-Gram and Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
    "    \n",
    "    datasets = json.load(open(filename, 'r'))\n",
    "    for name in datasets:\n",
    "        datasets[name] = [x.split() for x in datasets[name]]\n",
    "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return datasets, vocab\n",
    "\n",
    "def perplexity(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        logp_total += model.sequence_logp(sequence)\n",
    "        n_total += len(sequence) + 1  \n",
    "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramAdditive(object):\n",
    "    def __init__(self, n, delta, vsize):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "        self.vsize = vsize\n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "            for i in range(len(padded_sequence) - self.n+1):\n",
    "                ngram = tuple(padded_sequence[i:i+self.n])\n",
    "                prefix, word = ngram[:-1], ngram[-1]\n",
    "                self.count[prefix][word] += 1\n",
    "                self.total[prefix] += 1\n",
    "                \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_prob(ngram))\n",
    "        return total_logp\n",
    "\n",
    "    def ngram_prob(self, ngram):\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        prob = ((self.delta + self.count[prefix][word]) / \n",
    "                (self.total[prefix] + self.delta*self.vsize))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33175\n"
     ]
    }
   ],
   "source": [
    "datasets, vocab = load_wikitext()\n",
    "\n",
    "# delta = 0.0005\n",
    "# for n in [2, 3, 4]:\n",
    "#     lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "#     lm.estimate(datasets['train'])\n",
    "\n",
    "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
    "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<unk>' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramInterpolation(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Neural Language Modeling with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler,DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>') # validation token is not seen in the training dataset\n",
    "        \n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [03:09<00:00, 412.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By 7 September , most of <unk> 's troops had started marching from <\n",
      "\n",
      " encoded - [830, 2280, 753, 10, 402, 5, 3, 115, 1225, 510, 1360, 7393, 107, 361]\n",
      "\n",
      " decoded - ['By', '7', 'September', ',', 'most', 'of', '<unk>', \"'s\", 'troops', 'had', 'started', 'marching', 'from', '<']\n",
      "length of train_dict is  33178\n",
      "unique words in training dataset is  33175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dict=Dictionary(datasets) # excluding validation dataset #109\n",
    "#all_dict=Dictionary(datasets, include_valid = True)\n",
    "\n",
    "# example\n",
    "rand_int = np.random.randint(1, len(datasets['valid']))\n",
    "print(' '.join(datasets['valid'][rand_int]))\n",
    "encoded = train_dict.encode_token_seq(datasets['valid'][rand_int])\n",
    "print(f'\\n encoded - {encoded}')\n",
    "decoded = train_dict.decode_idx_seq(encoded)\n",
    "print(f'\\n decoded - {decoded}')\n",
    "\n",
    "# checking\n",
    "print('length of train_dict is ', len(train_dict)) # that's because <unk> is already in the dataset \n",
    "print('unique words in training dataset is ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given the dictionary from above, now write a function that tokenize the all datasets into id's\n",
    "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "        \n",
    "    return tokenized_datasets\n",
    "\n",
    "# Given a tokenzied dataset with ngram defined, slice the input sequences into n-grams \n",
    "# [0,1,2,3,4,5], 2 -> [0,1], [1,2], [2,3], [3,4], [4,5]\n",
    "def slice_sequences_given_order(tokenized_dataset_with_spec, ngram_order=2):\n",
    "    sliced_datasets = {}\n",
    "    for split, dataset in tokenized_dataset_with_spec.items():\n",
    "        _list_of_sliced_ngrams = []\n",
    "        for seq in tqdm(dataset):\n",
    "            ngrams = [seq[i:i+ngram_order] for i in range(len(seq)-ngram_order+1)]\n",
    "            _list_of_sliced_ngrams.extend(ngrams)\n",
    "        sliced_datasets[split] = _list_of_sliced_ngrams\n",
    "\n",
    "    return sliced_datasets\n",
    "\n",
    "# # Now we create a dataset\n",
    "class NgramDataset(Dataset):\n",
    "    def __init__(self, sliced_dataset_split):\n",
    "        super().__init__()\n",
    "\n",
    "        # for each sample: [:-1] is input, [-1] is target\n",
    "        self.sequences = [torch.tensor(i, dtype=torch.long) for i in sliced_dataset_split]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.sequences[i]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "def batchify(list_minibatch):\n",
    "    inp_list = [i[:-1] for i in list_minibatch]\n",
    "    tar_list = [i[-1] for i in list_minibatch]\n",
    "\n",
    "    inp_tensor = torch.stack(inp_list, dim=0) # list of tensors and create a new tensor a u-dimension specified by dim\n",
    "    tar_tensor = torch.stack(tar_list, dim=0)\n",
    "    # cat: take a list of tensors and use existing dimension to concatent. Cannot create a new u-dimension speicfied. \n",
    "\n",
    "    return inp_tensor, tar_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:01<00:00, 76501.93it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 80940.14it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 84714.55it/s]\n",
      "100%|██████████| 78274/78274 [00:02<00:00, 33064.56it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 97930.77it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 23711.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " decoded with spec - ['<bos>', 'The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "#  returns dictionary of three items with 'train', 'valid' and 'test' with lists of token ids \n",
    "tokenized_ngram = tokenize_dataset(datasets, train_dict, ngram_order=2)\n",
    "\n",
    "# returns dictionary of three and lists of sliced n-grams\n",
    "sliced_ngram = slice_sequences_given_order(tokenized_ngram, ngram_order=2)\n",
    "\n",
    "# check that the sentence is encoded with (n-1)<bos> and can be decoded back to tokens \n",
    "decoded_with_spec = train_dict.decode_idx_seq(tokenized_ngram['train'][3010])\n",
    "print(f'\\n decoded with spec - {decoded_with_spec}')\n",
    "\n",
    "ngram_datasets = {}\n",
    "ngram_loaders = {}\n",
    "for split, dataset_sliced in sliced_ngram.items():\n",
    "    if split == 'train':\n",
    "        shuffle_ = True\n",
    "    else:\n",
    "        shuffle_ = False\n",
    "    dataset_ = NgramDataset(dataset_sliced)\n",
    "    ngram_datasets[split] = dataset_\n",
    "    ngram_loaders[split] = DataLoader(dataset_, batch_size=2048, shuffle=shuffle_, collate_fn=batchify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of original training dataset is:  78274\n",
      "The length of tokenzied training dataset is:  78274\n",
      "Slided_ngram for training dataset now has length of  2003028\n"
     ]
    }
   ],
   "source": [
    "print('The length of original training dataset is: ', len(datasets['train']))\n",
    "print('The length of tokenzied training dataset is: ', len(tokenized_ngram['train']))\n",
    "print('Slided_ngram for training dataset now has length of ', len(sliced_ngram['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 504, 1506, 7106, 741, 459, 20, 140, 98, 432, 19366, 10, 150, 15605, 13, 260, 3748, 955, 1826, 1384, 1145, 98, 1722, 4303, 39, 1]\n",
      "['<bos>', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ngram['valid'][10])\n",
    "print(train_dict.decode_idx_seq(tokenized_ngram['valid'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
    "    \n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 2\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0, 282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39]]),\n",
       " tensor([[282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39,   1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dataset = {}\n",
    "\n",
    "for split, listoflists in tokenized_ngram.items():\n",
    "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
    "    \n",
    "# check the first example\n",
    "tensor_dataset['train'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaders = {}\n",
    "batch_size = 32\n",
    "for split, dataset in tensor_dataset.items():\n",
    "    loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNNBase, RNN\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Linear, functional\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example embedding layer\n",
    "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
    "lookup.weight.size()\n",
    "# train_dict = vocab size + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "input_ = train_dict.encode_token_seq('hello world'.split(' '))\n",
    "print(f'discrete input: {[input_]}')\n",
    "\n",
    "input_continious = lookup(torch.tensor([input_], dtype=torch.long))\n",
    "print(f'continious input size: {input_continious.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "# before nn.RNN is hidden\n",
    "# Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
    "# RNN natrually takes multi sentence inputs and outputs hidden_size \n",
    "rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "projection = nn.Linear(options['hidden_size'], options['num_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        # before nn.RNN is hidden\n",
    "        # Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
    "        # RNN natrually takes multi sentence inputs and outputs hidden_size \n",
    "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        rnn_outputs = self.rnn(embeddings)\n",
    "        # project of outputs \n",
    "        # rnn_outputs: tupple with second element being last hidden state. \n",
    "        logits = self.projection(rnn_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 64\n",
    "    hidden_size = 128 # output of dimension \n",
    "    num_layers = 2\n",
    "    rnn_dropout = 0.1\n",
    "    input_size = lookup.weight.size(1)\n",
    "    vocab_size = lookup.weight.size(0)\n",
    "    \n",
    "    options = {\n",
    "        'num_embeddings': len(train_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': train_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }\n",
    "\n",
    "    \n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "\n",
    "# same as previous nn based \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cache = []\n",
    "\n",
    "for epoch_number in range(100):\n",
    "    avg_loss=0\n",
    "    if not load_pretrained:\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        for i, (inp, target) in enumerate(loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_log_cache.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "            \n",
    "    #do valid\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(loaders['valid']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_losses.append(loss.item())\n",
    "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "        \n",
    "    plot_cache.append((avg_loss, avg_val_loss))\n",
    "\n",
    "    if load_pretrained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves of RNN baseline model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 LSTM and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import LSTM\n",
    "# input_size, hidden_size, num_layers. \n",
    "# Optional: bias = False, dropout = 0 (probability of dropout)\n",
    "# bidirectional = False. \n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "\n",
    "# input: tensor containing feature of the input sequence \n",
    "# shape: seq_len, batch, input_size\n",
    "input = torch.randn(5,3,10)\n",
    "\n",
    "#h0: tensor contain hidden state for t = seq_len\n",
    "# shape: num_layers * num*directions, batch, hidden_size\n",
    "h0 = torch.randn(2,3,20)\n",
    "\n",
    "#c0: tensor contain cell state for t = seq_length\n",
    "# shape: num_layers * num_directions, batch, hidden_size \n",
    "c0 = torch.randn(2,3,20)\n",
    "\n",
    "# output: shape (seq_len, batch, num_direction * hidden size)\n",
    "output, (hn,cn) = rnn(input, (h0,c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33178, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example embedding layer\n",
    "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
    "lookup.weight.size()\n",
    "# train_dict = vocab size + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the baseline, we will stop the epoch around 20 \n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], \n",
    "                            dropout=options['lstm_dropout'], batch_first=True, bias = options['bias'],\n",
    "                           bidirectional = options['bid'])\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "        # project of outputs \n",
    "        # rnn_outputs: tupple with second element being last hidden state. \n",
    "        logits = self.projection(lstm_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = LSTMModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 64\n",
    "    hidden_size = 128 # output of dimension \n",
    "    num_layers = 2\n",
    "    lstm_dropout = 0.1\n",
    "#     input_size = lookup.weight.size(1)\n",
    "    vocab_size = len(train_dict)\n",
    "    \n",
    "    options = {\n",
    "        'num_embeddings': len(train_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': train_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'lstm_dropout': lstm_dropout,\n",
    "        'bias': True,\n",
    "        'bid': False \n",
    "    }\n",
    "\n",
    "    \n",
    "    model = LSTMModel(options).to(current_device)\n",
    "\n",
    "# same as previous nn based \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 avg train loss = 10.4141\n",
      "Step 100 avg train loss = 7.8508\n",
      "Step 200 avg train loss = 7.1631\n",
      "Step 300 avg train loss = 7.0615\n",
      "Step 400 avg train loss = 6.9351\n",
      "Step 500 avg train loss = 6.8432\n",
      "Step 600 avg train loss = 6.7137\n",
      "Step 700 avg train loss = 6.6335\n",
      "Step 800 avg train loss = 6.5849\n",
      "Step 900 avg train loss = 6.4968\n",
      "Step 1000 avg train loss = 6.4613\n",
      "Step 1100 avg train loss = 6.4289\n",
      "Step 1200 avg train loss = 6.3595\n",
      "Step 1300 avg train loss = 6.3364\n",
      "Step 1400 avg train loss = 6.2780\n",
      "Step 1500 avg train loss = 6.2327\n",
      "Step 1600 avg train loss = 6.2201\n",
      "Step 1700 avg train loss = 6.2095\n",
      "Step 1800 avg train loss = 6.1694\n",
      "Step 1900 avg train loss = 6.1184\n",
      "Step 2000 avg train loss = 6.1127\n",
      "Step 2100 avg train loss = 6.0936\n",
      "Step 2200 avg train loss = 6.0831\n",
      "Step 2300 avg train loss = 6.0651\n",
      "Step 2400 avg train loss = 6.0419\n",
      "Validation loss after 0 epoch = 5.8386\n",
      "Saving best model at epoch 0\n",
      "Step 0 avg train loss = 5.9000\n",
      "Step 100 avg train loss = 5.9375\n",
      "Step 200 avg train loss = 5.9147\n",
      "Step 300 avg train loss = 5.9001\n",
      "Step 400 avg train loss = 5.8872\n",
      "Step 500 avg train loss = 5.8874\n",
      "Step 600 avg train loss = 5.8633\n",
      "Step 700 avg train loss = 5.8651\n",
      "Step 800 avg train loss = 5.8344\n",
      "Step 900 avg train loss = 5.8261\n",
      "Step 1000 avg train loss = 5.8202\n",
      "Step 1100 avg train loss = 5.7904\n",
      "Step 1200 avg train loss = 5.7700\n",
      "Step 1300 avg train loss = 5.7769\n",
      "Step 1400 avg train loss = 5.7582\n",
      "Step 1500 avg train loss = 5.7533\n",
      "Step 1600 avg train loss = 5.7442\n",
      "Step 1700 avg train loss = 5.7391\n",
      "Step 1800 avg train loss = 5.7037\n",
      "Step 1900 avg train loss = 5.6794\n",
      "Step 2000 avg train loss = 5.7077\n",
      "Step 2100 avg train loss = 5.6745\n",
      "Step 2200 avg train loss = 5.6902\n",
      "Step 2300 avg train loss = 5.6784\n",
      "Step 2400 avg train loss = 5.6717\n",
      "Validation loss after 1 epoch = 5.5503\n",
      "Saving best model at epoch 1\n",
      "Step 0 avg train loss = 5.6099\n",
      "Step 100 avg train loss = 5.5613\n",
      "Step 200 avg train loss = 5.5608\n",
      "Step 300 avg train loss = 5.5605\n",
      "Step 400 avg train loss = 5.5377\n",
      "Step 500 avg train loss = 5.5282\n",
      "Step 600 avg train loss = 5.5270\n",
      "Step 700 avg train loss = 5.5491\n",
      "Step 800 avg train loss = 5.5005\n",
      "Step 900 avg train loss = 5.5005\n",
      "Step 1000 avg train loss = 5.5137\n",
      "Step 1100 avg train loss = 5.5084\n",
      "Step 1200 avg train loss = 5.5035\n",
      "Step 1300 avg train loss = 5.4919\n",
      "Step 1400 avg train loss = 5.4756\n",
      "Step 1500 avg train loss = 5.5118\n",
      "Step 1600 avg train loss = 5.4895\n",
      "Step 1700 avg train loss = 5.4571\n",
      "Step 1800 avg train loss = 5.4632\n",
      "Step 1900 avg train loss = 5.4740\n",
      "Step 2000 avg train loss = 5.4506\n",
      "Step 2100 avg train loss = 5.4680\n",
      "Step 2200 avg train loss = 5.4469\n",
      "Step 2300 avg train loss = 5.4544\n",
      "Step 2400 avg train loss = 5.4340\n",
      "Validation loss after 2 epoch = 5.4199\n",
      "Saving best model at epoch 2\n",
      "Step 0 avg train loss = 5.3272\n",
      "Step 100 avg train loss = 5.3144\n",
      "Step 200 avg train loss = 5.3276\n",
      "Step 300 avg train loss = 5.3158\n",
      "Step 400 avg train loss = 5.3240\n",
      "Step 500 avg train loss = 5.3284\n"
     ]
    }
   ],
   "source": [
    "#model = LSTMModel(options).to(current_device)\n",
    "plot_cache = []\n",
    "min_val_loss = 20 \n",
    "for epoch_number in range(20):\n",
    "    avg_loss=0\n",
    "    if not load_pretrained:\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        for i, (inp, target) in enumerate(loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_log_cache.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "            \n",
    "    #do valid\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(loaders['valid']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_losses.append(loss.item())\n",
    "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "        best = avg_val_loss < min_val_loss\n",
    "        if best:\n",
    "            min_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_LSTM.pt')\n",
    "            print('Saving best model at epoch', epoch_number)\n",
    "    if load_pretrained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves of LSTM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(2**(i[1]/np.log(2)) for i in plot_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (LSTM vs. Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Variation Based on Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Learned Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
    "\n",
    "Use `!pip install umap-learn` to install UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def umap_plot(weight_matrix, word_ids, words):\n",
    "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
    "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
    "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy()) \n",
    "    # transfer tensor back to cpu\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    to_plot = reduced[word_ids, :]\n",
    "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        current_point = to_plot[i]\n",
    "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vsize = 100                                 # e.g. len(dictionary)\n",
    "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
    "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
    "\n",
    "words = ['the', 'dog', 'ran']\n",
    "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
    "\n",
    "umap_plot(fake_weight_matrix, word_ids, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2 Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3 Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.2 Highest and Lowest scoring sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.3 Modified sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4.3 Number of unique tokens and sequence length \n",
    "\n",
    "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4.4 Example Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
