{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgZz1d0gYKxn",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUpYySfTYKxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpnVCIhVYKxq",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igUUQhkFYKxr",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDg01M5jYKxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf9t1dDuYKxu",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dns7tEEjYKxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNx_IrG5YKxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "993b342c-292a-42b2-af3b-a0beb2ae1675"
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-30 18:16:11--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.25.197, 107.152.24.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.25.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 18:16:11--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 18:16:11--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.26.199, 107.152.27.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.26.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!WuFYrV43bM6uOxuN9YhmJu2n_4O4uIs9L2GViWXzcSP64zHAR8bIVO5-8iZqk4yO2SfVOhL3UMFxqoHAJoD5eltM4urnvLChnQ2nrxjfjGgwjYvR3bhOSqeFU9m92u3CxdlbZwshbYGycreg04Ds0LWd5B0iA43EUoXubKm5I8VMJru-meQ7bGA8tq9Ajcetd_nQzmgDZ-JLfuYPTyPz4QirfmuBoCvBurb8cA8_8PbP2NDtHfKmo5NVWBx1gZJPISJE_O9vRgdrRY1mEXTUJEt-h-pfnAqepNwkBFxgwJfqkWDfDkiLroPzMxCD9rC0eksGTlziJzlwg_LSD41nGq-mJ38WyzGgufJJ8BiEE8lwf8pWP1cSbKhQTMoI2Pa6u6ZH510ypo5-mMSHdveZAsLZnEC1epDJ2QkVBascIUr6zb_-fWX5DRvNruMxuFfK4q_foOyrt6tmSPVLJEQSKt-GWRamBbbwStUGzD6YLPn8iY5VD6Hb8x8t9EgTqFIxqA9WSYdQDSrJlPNPT3K4JnQ3v2EqBBCDsu3Xq6wneYMS3j1WBVUlMI1e1kfPp3MfDRolUuwMWpd2rFDJCHZV56nHNY0dDyx5jYgmUFAvVCaoEksdDxzEpU7e8Ne4CCi9Z_yE4fM-vZJFqHi1UOf5AdN45bsuHXUnHArTsFYUEI3S7oKOEfFS9y9LNThkW6XoTZO7XyZPm6g473CkYjWGvw7oF_z41h7FDAKoenPD3kfSIp8Da8KW5jmZEHBpvzJgDTWDgwFr6nhDrBBOO8x5qy64LR5JdZXuIx6TdUiL_7Zc_xW3LK1e5R6TifwXXqElmz5LaEkD71brgCpP1gLKSqYRf5rUL-bs2aMmIhRRUvKnARCSBAcJtRg1whpIn6Sd-SE5K5IJ3PnijEgNeUAJO2mW_C1C-da25iR0cqp19YM7oEumfVhYzGJ5AcreWPL19aiE4Jt4i2hEENiIqVP0R68dqlpR4YiwUdchF044woGx4UVeUYMMEUb2nQ4MuIVx4AcDxGwMgnOM0Cu9BhrYeMdySZxcbpKaPfvSI-7pzvgIuJLQu95Ki73fpSxY8lDa4JpeVKw2E2Z2RWuLcwAUctOQf0E3GVQnZGMhUQIGZm9QMJ-Gkfq7o13LdzcfAvFjXNJgMxLI57Ex7Zh_RqbbPFn6oEnILVixSSTVdfEOlQJ_5d3R-uFsus0Rgkw8EyYNKfF6eLRrEP3CCXv6W6CBUrxgXAUMtSGfIbB-C9_YEZO0m-8ooeIPuP0R5iWeFtIrS3omXv5XC1zcGSlhydHa3FzmrscdSmo0UDVZ8HU6Y7XwqvZhGg7HoVRdu8gW3crJ7y3u4RRIQ1lLM73HkR2qKo7-hRy4BTrGg8Tpyy05GsMLS4MZjNT_jbWLaDNRSGXgyYnw-c6QccVlPg../download [following]\n",
            "--2019-09-30 18:16:12--  https://public.boxcloud.com/d/1/b1!WuFYrV43bM6uOxuN9YhmJu2n_4O4uIs9L2GViWXzcSP64zHAR8bIVO5-8iZqk4yO2SfVOhL3UMFxqoHAJoD5eltM4urnvLChnQ2nrxjfjGgwjYvR3bhOSqeFU9m92u3CxdlbZwshbYGycreg04Ds0LWd5B0iA43EUoXubKm5I8VMJru-meQ7bGA8tq9Ajcetd_nQzmgDZ-JLfuYPTyPz4QirfmuBoCvBurb8cA8_8PbP2NDtHfKmo5NVWBx1gZJPISJE_O9vRgdrRY1mEXTUJEt-h-pfnAqepNwkBFxgwJfqkWDfDkiLroPzMxCD9rC0eksGTlziJzlwg_LSD41nGq-mJ38WyzGgufJJ8BiEE8lwf8pWP1cSbKhQTMoI2Pa6u6ZH510ypo5-mMSHdveZAsLZnEC1epDJ2QkVBascIUr6zb_-fWX5DRvNruMxuFfK4q_foOyrt6tmSPVLJEQSKt-GWRamBbbwStUGzD6YLPn8iY5VD6Hb8x8t9EgTqFIxqA9WSYdQDSrJlPNPT3K4JnQ3v2EqBBCDsu3Xq6wneYMS3j1WBVUlMI1e1kfPp3MfDRolUuwMWpd2rFDJCHZV56nHNY0dDyx5jYgmUFAvVCaoEksdDxzEpU7e8Ne4CCi9Z_yE4fM-vZJFqHi1UOf5AdN45bsuHXUnHArTsFYUEI3S7oKOEfFS9y9LNThkW6XoTZO7XyZPm6g473CkYjWGvw7oF_z41h7FDAKoenPD3kfSIp8Da8KW5jmZEHBpvzJgDTWDgwFr6nhDrBBOO8x5qy64LR5JdZXuIx6TdUiL_7Zc_xW3LK1e5R6TifwXXqElmz5LaEkD71brgCpP1gLKSqYRf5rUL-bs2aMmIhRRUvKnARCSBAcJtRg1whpIn6Sd-SE5K5IJ3PnijEgNeUAJO2mW_C1C-da25iR0cqp19YM7oEumfVhYzGJ5AcreWPL19aiE4Jt4i2hEENiIqVP0R68dqlpR4YiwUdchF044woGx4UVeUYMMEUb2nQ4MuIVx4AcDxGwMgnOM0Cu9BhrYeMdySZxcbpKaPfvSI-7pzvgIuJLQu95Ki73fpSxY8lDa4JpeVKw2E2Z2RWuLcwAUctOQf0E3GVQnZGMhUQIGZm9QMJ-Gkfq7o13LdzcfAvFjXNJgMxLI57Ex7Zh_RqbbPFn6oEnILVixSSTVdfEOlQJ_5d3R-uFsus0Rgkw8EyYNKfF6eLRrEP3CCXv6W6CBUrxgXAUMtSGfIbB-C9_YEZO0m-8ooeIPuP0R5iWeFtIrS3omXv5XC1zcGSlhydHa3FzmrscdSmo0UDVZ8HU6Y7XwqvZhGg7HoVRdu8gW3crJ7y3u4RRIQ1lLM73HkR2qKo7-hRy4BTrGg8Tpyy05GsMLS4MZjNT_jbWLaDNRSGXgyYnw-c6QccVlPg../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.25.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.25.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  18.3MB/s    in 0.7s    \n",
            "\n",
            "2019-09-30 18:16:13 (18.3 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9enysqSzYKx0",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn6s2MMgYKx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, n, lamb_list, vsize):\n",
        "        # lamb_list should be a list containing all lambda values for interpolation\n",
        "        self.n = n\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "        self.word_count = defaultdict(float) # Count of every word\n",
        "        self.length = 0 # The total length of the dataset\n",
        "        \n",
        "        # Check if lambda fulfill our conditions\n",
        "        if len(lamb_list) != n + 1:\n",
        "            raise ValueError(\"Lambda list must have length n + 1\")\n",
        "        elif np.abs(np.sum(lamb_list) - 1) > 0.00001:\n",
        "            raise ValueError(\"Sum of lambdas must be 1\")\n",
        "        self.lamb_list = lamb_list\n",
        "\n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            for w in sequence:\n",
        "                self.word_count[w] += 1\n",
        "                self.length += 1\n",
        "            for current_n in range(2, self.n + 1):\n",
        "                padded_sequence = ['<bos>']*(current_n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - current_n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+current_n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "\n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(lamb_list[-1], ngram, self.n))\n",
        "        return total_logp\n",
        "    \n",
        "    def ngram_prob(self, lamb, ngram, order):\n",
        "        \"\"\"\n",
        "        lamb: a single value from the lambda list\n",
        "        order: means which index of lambda we are talking about, max and initial\n",
        "            order should be n\n",
        "        \"\"\"\n",
        "        \n",
        "        if order > n:\n",
        "            raise ValueError(\"Order can't exceed n\")\n",
        "        elif order == 0:\n",
        "            return lamb * (1 / self.vsize) # Since p_0 = 1 / |vocab|\n",
        "        elif order >= 1:\n",
        "            prefix = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "            if prefix: # Check if prefix is empty\n",
        "                if self.total[prefix] == 0:\n",
        "                    prob = 0\n",
        "                else:\n",
        "                    prob = self.count[prefix][word] / self.total[prefix]\n",
        "            else: # If prefix is empty, we are in lambda_1\n",
        "                prob = self.word_count[word] / self.length\n",
        "            \n",
        "            return lamb * prob + self.ngram_prob(self.lamb_list[order], \n",
        "                                                 ngram[1:], order - 1)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC0aMK3JiBUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a7369764-b906-47ff-a5d7-6453df3e7d22"
      },
      "source": [
        "for n in [2, 3, 4]:\n",
        "    if n == 2:\n",
        "        lamb_list = [0.2, 0.4, 0.4]\n",
        "    elif n == 3:\n",
        "        lamb_list = [0.2, 0.2, 0.3, 0.3]\n",
        "    else:\n",
        "        lamb_list = [0.1, 0.1, 0.2, 0.2, 0.4]\n",
        "    interp = NGramInterpolation(n=n, lamb_list = lamb_list, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    interp.estimate(datasets['train'])\n",
        "\n",
        "    print(f\"Interpolation model, n = {n}, lambda = {lamb_list}, Train Perplexity = {perplexity(interp, datasets['train'])}\")\n",
        "    print(f\"Interpolation model, n = {n}, lambda = {lamb_list}, Validation Perplexity = {perplexity(interp, datasets['valid'])}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation model, n = 2, lambda = [0.2, 0.4, 0.4], Train Perplexity = 149.14058601187182\n",
            "Interpolation model, n = 2, lambda = [0.2, 0.4, 0.4], Validation Perplexity = 330.56932143293216\n",
            "Interpolation model, n = 3, lambda = [0.2, 0.2, 0.3, 0.3], Train Perplexity = 19.476557684727467\n",
            "Interpolation model, n = 3, lambda = [0.2, 0.2, 0.3, 0.3], Validation Perplexity = 283.3942638303449\n",
            "Interpolation model, n = 4, lambda = [0.1, 0.1, 0.2, 0.2, 0.4], Train Perplexity = 3.71422334002876\n",
            "Interpolation model, n = 4, lambda = [0.1, 0.1, 0.2, 0.2, 0.4], Validation Perplexity = 310.2176484896974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKanZw6bYKx3",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFg5I5d9YKx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a6dfdb74-5625-4df3-c18c-a69305bef3df"
      },
      "source": [
        "for n in [4]:\n",
        "    if n == 2:\n",
        "        lamb_list = [0.2, 0.4, 0.4]\n",
        "    elif n == 3:\n",
        "        lamb_list = [0.2, 0.2, 0.3, 0.3]\n",
        "    else:\n",
        "        lamb_list = [0.05, 0.15, 0.2, 0.3, 0.3]\n",
        "    interp = NGramInterpolation(n=n, lamb_list = lamb_list, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    interp.estimate(datasets['train'])\n",
        "\n",
        "    print(f\"Interpolation model, n = {n}, lambda = {lamb_list}, Train Perplexity = {perplexity(interp, datasets['train'])}\")\n",
        "    print(f\"Interpolation model, n = {n}, lambda = {lamb_list}, Validation Perplexity = {perplexity(interp, datasets['valid'])}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation model, n = 4, lambda = [0.05, 0.15, 0.2, 0.3, 0.3], Train Perplexity = 4.706373565940121\n",
            "Interpolation model, n = 4, lambda = [0.05, 0.15, 0.2, 0.3, 0.3], Validation Perplexity = 281.6049590165684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM9cl4s5YKx6",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBVcVIZLYKx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LUz5-TSYKx9",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDCByCSKYKx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhjKFn8iYKyB",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L19_F7ibYKyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crDwk505YKyD",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irhwrFc5YKyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLkRk9p4YKyG",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmG6gceQYKyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoZhUcWIYKyJ",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhdqpkWEYKyJ",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUTbfJJpYKyK",
        "colab_type": "code",
        "colab": {},
        "outputId": "8468d5dd-6116-4bdb-d7fa-71c9f31902c9"
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TUSyD5jYKyM",
        "colab_type": "code",
        "colab": {},
        "outputId": "511f9fb8-63ac-4fea-b122-b94e1e418287"
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flD8nPbMYKyO",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-W_mKt_YKyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVE3m1vYKyR",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNvovKiYYKyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDbOskHAYKyV",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gh6k_mfYKyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTY54t9TYKya",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwv_ZngvYKyb",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWLzt9RMYKyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnynuwkYKye",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0DT0Nz4YKyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgDR3Vb7YKyh",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lZnRuY3YKyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK87HIhUYKyl",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWAqFbF4YKyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lMTWi_IYKyn",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCJqPo1GYKyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6yiuQCYKyq",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ASKYldYKyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}