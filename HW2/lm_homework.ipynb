{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1011 Homework 2\n",
    "## N-Gram and Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
    "    \n",
    "    datasets = json.load(open(filename, 'r'))\n",
    "    for name in datasets:\n",
    "        datasets[name] = [x.split() for x in datasets[name]]\n",
    "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return datasets, vocab\n",
    "\n",
    "def perplexity(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        logp_total += model.sequence_logp(sequence)\n",
    "        n_total += len(sequence) + 1  \n",
    "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramAdditive(object):\n",
    "    def __init__(self, n, delta, vsize):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "        self.vsize = vsize\n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "            for i in range(len(padded_sequence) - self.n+1):\n",
    "                ngram = tuple(padded_sequence[i:i+self.n])\n",
    "                prefix, word = ngram[:-1], ngram[-1]\n",
    "                self.count[prefix][word] += 1\n",
    "                self.total[prefix] += 1\n",
    "                \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_prob(ngram))\n",
    "        return total_logp\n",
    "\n",
    "    def ngram_prob(self, ngram):\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        prob = ((self.delta + self.count[prefix][word]) / \n",
    "                (self.total[prefix] + self.delta*self.vsize))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-29 21:24:46--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
      "Resolving nyu.box.com (nyu.box.com)... 107.152.26.197, 107.152.27.197\n",
      "Connecting to nyu.box.com (nyu.box.com)|107.152.26.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
      "--2019-09-29 21:24:46--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
      "Reusing existing connection to nyu.box.com:443.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
      "--2019-09-29 21:24:47--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
      "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.26.199, 107.152.27.199\n",
      "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.26.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://public.boxcloud.com/d/1/b1!0mNN7Gp10l-uOxwPHnCrePCcI3yxW1TxRIcjviLh0Uf8v0gSuVnswt7VnSQiCfrxTjIAS7JYyNURzOMoDQJvktyPG2n1X6A0tsxLgrZE6taqB0YCuPXORQBlXsjUwzhnoQDa0OsnE5SDokfZgagFtwM4B0N_VdxSOW6dJpvOrHw9o3TZaVwRFqO2vFQ-IUiSVn3Qc9fLratFbljy1r9OqBzxgWlLXgzeXK5cNROMFBQT-CZXhoPaTOL5X6diYFjVq-pi-CqPGpeLD1nPMAgIhzq8cnb5EFrZJlWuUeF1oN8z6uhqPiQ_p0UXdBhVgBB0iz1Ub2nBny2x2ALogTwB3eeI2wDZ42ii7tlCfb42wKsABeNywkUZ1BoynvxfTGLrPUdUzA__nEt3QaEry_9ddAaoQ5KnABhWjdutoxt9nwcePBTZ-_zfqx1OTsXItE5-QjYzZU8KPiz7261Zzs-Fqqv5nyhsfKwwSgJJNwAlamXh1f7bsMEK6snNYEFiKfMEE5LJRyc53o1sbCgVqYtseUVZQdFZcgPWIO_uKEXpK_sYwxQ85I-ZNgEbqtfnKaxUEvP624m0bFoI6p8yTU5GxovNV9_ujoU-I7jrXz3vKDCg8SSgZBE_RE6ZGFSeYWaFqfRq63gicCZulZZiNaqX8_4BpkC-Thv3ioFzmAVjyx5SubFF49qgnVxkLA7_Jae9pYuSmiFf72Um7g3o1OVOIcayMFei7SuXlERip1e1gwfH00gSQIyiQzSTWmGbZ5ZlI9YNoHyii77jT7UN2w_k28KgKqqY6HkSqDNnc2NjkH_2xhNAyYvd8RbW-ezAjySqWuhm9jlR3Kzn4yG-22s2ZDpsUEQOamOvkKoByTdYFWlYsqo4tGUy6K5dwLui5VYNEFFBQjhwIFA_ZtuNtqOcfSBM19YI3P3n-SE19L8xHsrQEqrnDRzvtnZdBvdOhtD4g0U_L3NpOcQoMoOgInY-yTw0nmABjXLzOwJTCqJPQBMjoQLS7secuPx-0_f9a5tRQCUaUm9qi5YZCR4EdINdPMEEOFfaOraMKFlPMHbK-GqNnuAQsHdwvr1NFP1_9Cq-fOf3i0ovOIlkPsgBgDYzQh08MBPMvR4rVpUAy5-aTupUaRvSQdUnSSAKzm7efGzIo079hPlYq1G99uDr0ZY72gSCcgyrQcKw-8jsHRE_dNG1NASp_VlcROz8YdafujjXkvUDY245zPkfaiwuE8Uk_iB773BsWKXHdfpw-Uvq7XPLe-yh1qpqWwfoAYl2UdUjSX1h1kcpQDFiwCTzqx49Pchsmf5BlI4N9FubDdrybs8IjVOit6bFNjIosmcUoR8lCF7mjVRY_ilCb8MLr-ayAwOHJJOE8cDbJEgsA6j9A8YWtaQZq3X5BDWNVjcI7j6thQmIYO_GGlXaDPE./download [following]\n",
      "--2019-09-29 21:24:47--  https://public.boxcloud.com/d/1/b1!0mNN7Gp10l-uOxwPHnCrePCcI3yxW1TxRIcjviLh0Uf8v0gSuVnswt7VnSQiCfrxTjIAS7JYyNURzOMoDQJvktyPG2n1X6A0tsxLgrZE6taqB0YCuPXORQBlXsjUwzhnoQDa0OsnE5SDokfZgagFtwM4B0N_VdxSOW6dJpvOrHw9o3TZaVwRFqO2vFQ-IUiSVn3Qc9fLratFbljy1r9OqBzxgWlLXgzeXK5cNROMFBQT-CZXhoPaTOL5X6diYFjVq-pi-CqPGpeLD1nPMAgIhzq8cnb5EFrZJlWuUeF1oN8z6uhqPiQ_p0UXdBhVgBB0iz1Ub2nBny2x2ALogTwB3eeI2wDZ42ii7tlCfb42wKsABeNywkUZ1BoynvxfTGLrPUdUzA__nEt3QaEry_9ddAaoQ5KnABhWjdutoxt9nwcePBTZ-_zfqx1OTsXItE5-QjYzZU8KPiz7261Zzs-Fqqv5nyhsfKwwSgJJNwAlamXh1f7bsMEK6snNYEFiKfMEE5LJRyc53o1sbCgVqYtseUVZQdFZcgPWIO_uKEXpK_sYwxQ85I-ZNgEbqtfnKaxUEvP624m0bFoI6p8yTU5GxovNV9_ujoU-I7jrXz3vKDCg8SSgZBE_RE6ZGFSeYWaFqfRq63gicCZulZZiNaqX8_4BpkC-Thv3ioFzmAVjyx5SubFF49qgnVxkLA7_Jae9pYuSmiFf72Um7g3o1OVOIcayMFei7SuXlERip1e1gwfH00gSQIyiQzSTWmGbZ5ZlI9YNoHyii77jT7UN2w_k28KgKqqY6HkSqDNnc2NjkH_2xhNAyYvd8RbW-ezAjySqWuhm9jlR3Kzn4yG-22s2ZDpsUEQOamOvkKoByTdYFWlYsqo4tGUy6K5dwLui5VYNEFFBQjhwIFA_ZtuNtqOcfSBM19YI3P3n-SE19L8xHsrQEqrnDRzvtnZdBvdOhtD4g0U_L3NpOcQoMoOgInY-yTw0nmABjXLzOwJTCqJPQBMjoQLS7secuPx-0_f9a5tRQCUaUm9qi5YZCR4EdINdPMEEOFfaOraMKFlPMHbK-GqNnuAQsHdwvr1NFP1_9Cq-fOf3i0ovOIlkPsgBgDYzQh08MBPMvR4rVpUAy5-aTupUaRvSQdUnSSAKzm7efGzIo079hPlYq1G99uDr0ZY72gSCcgyrQcKw-8jsHRE_dNG1NASp_VlcROz8YdafujjXkvUDY245zPkfaiwuE8Uk_iB773BsWKXHdfpw-Uvq7XPLe-yh1qpqWwfoAYl2UdUjSX1h1kcpQDFiwCTzqx49Pchsmf5BlI4N9FubDdrybs8IjVOit6bFNjIosmcUoR8lCF7mjVRY_ilCb8MLr-ayAwOHJJOE8cDbJEgsA6j9A8YWtaQZq3X5BDWNVjcI7j6thQmIYO_GGlXaDPE./download\n",
      "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.27.200\n",
      "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.27.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12714601 (12M) [application/octet-stream]\n",
      "Saving to: ‘wikitext2-sentencized.json’\n",
      "\n",
      "100%[======================================>] 12,714,601  16.9MB/s   in 0.7s   \n",
      "\n",
      "2019-09-29 21:24:49 (16.9 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
      "\n",
      "Vocab size: 33175\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
     ]
    }
   ],
   "source": [
    "datasets, vocab = load_wikitext()\n",
    "\n",
    "delta = 0.0005\n",
    "for n in [2, 3, 4]:\n",
    "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "    lm.estimate(datasets['train'])\n",
    "\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramInterpolation(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Neural Language Modeling with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler,DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>') # validation token is not seen in the training dataset\n",
    "        \n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [02:25<00:00, 538.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to federal prosecutors Matthews obtained a college <unk> of Turner 's , and learned his car 's license plate number and his parking location .\n",
      "\n",
      " encoded - [811, 13, 1146, 29352, 19606, 1529, 20, 4413, 3, 5, 9442, 115, 10, 30, 9368, 345, 4399, 115, 10650, 3265, 650, 30, 345, 6414, 201, 39]\n",
      "\n",
      " decoded - ['According', 'to', 'federal', 'prosecutors', 'Matthews', 'obtained', 'a', 'college', '<unk>', 'of', 'Turner', \"'s\", ',', 'and', 'learned', 'his', 'car', \"'s\", 'license', 'plate', 'number', 'and', 'his', 'parking', 'location', '.']\n",
      "length of train_dict is  33178\n",
      "unique words in training dataset is  33175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dict=Dictionary(datasets) # excluding validation dataset #109\n",
    "#all_dict=Dictionary(datasets, include_valid = True)\n",
    "\n",
    "# example\n",
    "rand_int = np.random.randint(1, len(datasets['valid']))\n",
    "print(' '.join(datasets['valid'][rand_int]))\n",
    "encoded = train_dict.encode_token_seq(datasets['valid'][rand_int])\n",
    "print(f'\\n encoded - {encoded}')\n",
    "decoded = train_dict.decode_idx_seq(encoded)\n",
    "print(f'\\n decoded - {decoded}')\n",
    "\n",
    "# checking\n",
    "print('length of train_dict is ', len(train_dict))\n",
    "print('unique words in training dataset is ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given the dictionary from above, now write a function that tokenize the all datasets into id's\n",
    "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "        \n",
    "    return tokenized_datasets\n",
    "\n",
    "# Given a tokenzied dataset with ngram defined, slice the input sequences into n-grams \n",
    "# [0,1,2,3,4,5], 2 -> [0,1], [1,2], [2,3], [3,4], [4,5]\n",
    "def slice_sequences_given_order(tokenized_dataset_with_spec, ngram_order=2):\n",
    "    sliced_datasets = {}\n",
    "    for split, dataset in tokenized_dataset_with_spec.items():\n",
    "        _list_of_sliced_ngrams = []\n",
    "        for seq in tqdm(dataset):\n",
    "            ngrams = [seq[i:i+ngram_order] for i in range(len(seq)-ngram_order+1)]\n",
    "            _list_of_sliced_ngrams.extend(ngrams)\n",
    "        sliced_datasets[split] = _list_of_sliced_ngrams\n",
    "\n",
    "    return sliced_datasets\n",
    "\n",
    "# # Now we create a dataset\n",
    "class NgramDataset(Dataset):\n",
    "    def __init__(self, sliced_dataset_split):\n",
    "        super().__init__()\n",
    "\n",
    "        # for each sample: [:-1] is input, [-1] is target\n",
    "        self.sequences = [torch.tensor(i, dtype=torch.long) for i in sliced_dataset_split]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.sequences[i]\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "def batchify(list_minibatch):\n",
    "    inp_list = [i[:-1] for i in list_minibatch]\n",
    "    tar_list = [i[-1] for i in list_minibatch]\n",
    "\n",
    "    inp_tensor = torch.stack(inp_list, dim=0) # list of tensors and create a new tensor a u-dimension specified by dim\n",
    "    tar_tensor = torch.stack(tar_list, dim=0)\n",
    "    # cat: take a list of tensors and use existing dimension to concatent. Cannot create a new u-dimension speicfied. \n",
    "\n",
    "    return inp_tensor, tar_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:00<00:00, 106197.21it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 120650.17it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 112944.20it/s]\n",
      "100%|██████████| 78274/78274 [00:02<00:00, 30650.57it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 130345.79it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 134435.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " decoded with spec - ['<bos>', 'The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "#  returns dictionary of three items with 'train', 'valid' and 'test' with lists of token ids \n",
    "tokenized_ngram = tokenize_dataset(datasets, train_dict, ngram_order=2)\n",
    "\n",
    "# returns dictionary of three and lists of sliced n-grams\n",
    "sliced_ngram = slice_sequences_given_order(tokenized_ngram, ngram_order=2)\n",
    "\n",
    "# check that the sentence is encoded with (n-1)<bos> and can be decoded back to tokens \n",
    "decoded_with_spec = train_dict.decode_idx_seq(tokenized_ngram['train'][3010])\n",
    "print(f'\\n decoded with spec - {decoded_with_spec}')\n",
    "\n",
    "ngram_datasets = {}\n",
    "ngram_loaders = {}\n",
    "for split, dataset_sliced in sliced_ngram.items():\n",
    "    if split == 'train':\n",
    "        shuffle_ = True\n",
    "    else:\n",
    "        shuffle_ = False\n",
    "    dataset_ = NgramDataset(dataset_sliced)\n",
    "    ngram_datasets[split] = dataset_\n",
    "    ngram_loaders[split] = DataLoader(dataset_, batch_size=2048, shuffle=shuffle_, collate_fn=batchify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of original training dataset is:  78274\n",
      "The length of tokenzied training dataset is:  78274\n",
      "Slided_ngram for training dataset now has length of  2003028\n"
     ]
    }
   ],
   "source": [
    "print('The length of original training dataset is: ', len(datasets['train']))\n",
    "print('The length of tokenzied training dataset is: ', len(tokenized_ngram['train']))\n",
    "print('Slided_ngram for training dataset now has length of ', len(sliced_ngram['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 504, 1506, 7106, 741, 459, 20, 140, 98, 432, 19366, 10, 150, 15605, 13, 260, 3748, 955, 1826, 1384, 1145, 98, 1722, 4303, 39, 1]\n",
      "['<bos>', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ngram['valid'][10])\n",
    "print(train_dict.decode_idx_seq(tokenized_ngram['valid'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
    "    \n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    #pad_token = persona_dict.get_id('<pad>')\n",
    "    pad_token = 2\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_dataset = {}\n",
    "\n",
    "for split, listoflists in tokenized_ngram.items():\n",
    "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
    "    \n",
    "# check the first example\n",
    "tensor_dataset['train'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaders = {}\n",
    "batch_size = 32\n",
    "for split, dataset in tensor_dataset.items():\n",
    "    loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNNBase, RNN\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Linear, functional\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33178, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example embedding layer\n",
    "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
    "lookup.weight.size()\n",
    "# train_dict = vocab size + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete input: [[3, 693]]\n",
      "continious input size: torch.Size([1, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "input_ = train_dict.encode_token_seq('hello world'.split(' '))\n",
    "print(f'discrete input: {[input_]}')\n",
    "\n",
    "input_continious = lookup(torch.tensor([input_], dtype=torch.long))\n",
    "print(f'continious input size: {input_continious.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        # before nn.RNN is hidden\n",
    "        # Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
    "        # RNN natrually takes multi sentence inputs and outputs hidden_size \n",
    "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        rnn_outputs = self.rnn(embeddings)\n",
    "        # project of outputs \n",
    "        # rnn_outputs: tupple with second element being last hidden state. \n",
    "        logits = self.projection(rnn_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 64\n",
    "    hidden_size = 128 # output of dimension \n",
    "    num_layers = 2\n",
    "    rnn_dropout = 0.1\n",
    "    input_size = lookup.weight.size(1)\n",
    "    vocab_size = lookup.weight.size(0)\n",
    "    \n",
    "    options = {\n",
    "        'num_embeddings': len(train_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': train_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }\n",
    "\n",
    "    \n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "\n",
    "# same as previous nn based \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLanguageModel(\n",
       "  (lookup): Embedding(33178, 64, padding_idx=2)\n",
       "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 avg train loss = 10.4261\n",
      "Step 100 avg train loss = 7.7394\n",
      "Step 200 avg train loss = 6.9135\n",
      "Step 300 avg train loss = 6.7328\n",
      "Step 400 avg train loss = 6.5821\n",
      "Step 500 avg train loss = 6.4996\n",
      "Step 600 avg train loss = 6.4202\n",
      "Step 700 avg train loss = 6.3538\n",
      "Step 800 avg train loss = 6.2909\n",
      "Step 900 avg train loss = 6.2615\n",
      "Step 1000 avg train loss = 6.1872\n",
      "Step 1100 avg train loss = 6.1498\n",
      "Step 1200 avg train loss = 6.1237\n",
      "Step 1300 avg train loss = 6.0962\n",
      "Step 1400 avg train loss = 6.0534\n",
      "Step 1500 avg train loss = 6.0385\n",
      "Step 1600 avg train loss = 6.0171\n",
      "Step 1700 avg train loss = 5.9814\n",
      "Step 1800 avg train loss = 5.9500\n",
      "Step 1900 avg train loss = 5.9562\n",
      "Step 2000 avg train loss = 5.9307\n",
      "Step 2100 avg train loss = 5.8974\n",
      "Step 2200 avg train loss = 5.8863\n",
      "Step 2300 avg train loss = 5.8849\n",
      "Step 2400 avg train loss = 5.8922\n",
      "Validation loss after 0 epoch = 5.6953\n",
      "Step 0 avg train loss = 5.8538\n",
      "Step 100 avg train loss = 5.7598\n",
      "Step 200 avg train loss = 5.7068\n",
      "Step 300 avg train loss = 5.7236\n",
      "Step 400 avg train loss = 5.7152\n",
      "Step 500 avg train loss = 5.7137\n",
      "Step 600 avg train loss = 5.6853\n",
      "Step 700 avg train loss = 5.7068\n",
      "Step 800 avg train loss = 5.6854\n",
      "Step 900 avg train loss = 5.6750\n",
      "Step 1000 avg train loss = 5.6740\n",
      "Step 1100 avg train loss = 5.6588\n",
      "Step 1200 avg train loss = 5.6423\n",
      "Step 1300 avg train loss = 5.6357\n",
      "Step 1400 avg train loss = 5.6234\n",
      "Step 1500 avg train loss = 5.6255\n",
      "Step 1600 avg train loss = 5.6423\n",
      "Step 1700 avg train loss = 5.6170\n",
      "Step 1800 avg train loss = 5.6262\n",
      "Step 1900 avg train loss = 5.6134\n",
      "Step 2000 avg train loss = 5.5860\n",
      "Step 2100 avg train loss = 5.5948\n",
      "Step 2200 avg train loss = 5.5892\n",
      "Step 2300 avg train loss = 5.5986\n",
      "Step 2400 avg train loss = 5.5750\n",
      "Validation loss after 1 epoch = 5.5241\n",
      "Step 0 avg train loss = 5.5480\n",
      "Step 100 avg train loss = 5.4182\n",
      "Step 200 avg train loss = 5.4236\n",
      "Step 300 avg train loss = 5.4281\n",
      "Step 400 avg train loss = 5.3995\n",
      "Step 500 avg train loss = 5.4402\n",
      "Step 600 avg train loss = 5.4274\n",
      "Step 700 avg train loss = 5.4167\n",
      "Step 800 avg train loss = 5.4232\n",
      "Step 900 avg train loss = 5.4297\n",
      "Step 1000 avg train loss = 5.4221\n",
      "Step 1100 avg train loss = 5.4084\n",
      "Step 1200 avg train loss = 5.4091\n",
      "Step 1300 avg train loss = 5.4060\n",
      "Step 1400 avg train loss = 5.4171\n",
      "Step 1500 avg train loss = 5.4002\n",
      "Step 1600 avg train loss = 5.4056\n",
      "Step 1700 avg train loss = 5.4018\n",
      "Step 1800 avg train loss = 5.3980\n",
      "Step 1900 avg train loss = 5.3830\n",
      "Step 2000 avg train loss = 5.4016\n",
      "Step 2100 avg train loss = 5.4058\n",
      "Step 2200 avg train loss = 5.3888\n",
      "Step 2300 avg train loss = 5.3800\n",
      "Step 2400 avg train loss = 5.4074\n",
      "Validation loss after 2 epoch = 5.4301\n",
      "Step 0 avg train loss = 5.2530\n",
      "Step 100 avg train loss = 5.2438\n",
      "Step 200 avg train loss = 5.2402\n",
      "Step 300 avg train loss = 5.2424\n",
      "Step 400 avg train loss = 5.2296\n",
      "Step 500 avg train loss = 5.2301\n",
      "Step 600 avg train loss = 5.2467\n",
      "Step 700 avg train loss = 5.2501\n",
      "Step 800 avg train loss = 5.2434\n",
      "Step 900 avg train loss = 5.2392\n",
      "Step 1000 avg train loss = 5.2666\n",
      "Step 1100 avg train loss = 5.2673\n",
      "Step 1200 avg train loss = 5.2583\n",
      "Step 1300 avg train loss = 5.2529\n",
      "Step 1400 avg train loss = 5.2742\n",
      "Step 1500 avg train loss = 5.2387\n",
      "Step 1600 avg train loss = 5.2545\n",
      "Step 1700 avg train loss = 5.2418\n",
      "Step 1800 avg train loss = 5.2737\n",
      "Step 1900 avg train loss = 5.2722\n",
      "Step 2000 avg train loss = 5.2494\n",
      "Step 2100 avg train loss = 5.2381\n",
      "Step 2200 avg train loss = 5.2553\n",
      "Step 2300 avg train loss = 5.2355\n",
      "Step 2400 avg train loss = 5.2311\n",
      "Validation loss after 3 epoch = 5.3850\n",
      "Step 0 avg train loss = 5.1814\n",
      "Step 100 avg train loss = 5.0947\n",
      "Step 200 avg train loss = 5.0881\n",
      "Step 300 avg train loss = 5.0852\n",
      "Step 400 avg train loss = 5.1016\n",
      "Step 500 avg train loss = 5.1072\n",
      "Step 600 avg train loss = 5.1172\n",
      "Step 700 avg train loss = 5.1150\n",
      "Step 800 avg train loss = 5.1190\n",
      "Step 900 avg train loss = 5.1087\n",
      "Step 1000 avg train loss = 5.1376\n",
      "Step 1100 avg train loss = 5.1421\n",
      "Step 1200 avg train loss = 5.1379\n",
      "Step 1300 avg train loss = 5.1189\n",
      "Step 1400 avg train loss = 5.1382\n",
      "Step 1500 avg train loss = 5.1415\n",
      "Step 1600 avg train loss = 5.1543\n",
      "Step 1700 avg train loss = 5.1359\n",
      "Step 1800 avg train loss = 5.1448\n",
      "Step 1900 avg train loss = 5.1285\n",
      "Step 2000 avg train loss = 5.1373\n",
      "Step 2100 avg train loss = 5.1395\n",
      "Step 2200 avg train loss = 5.1387\n",
      "Step 2300 avg train loss = 5.1493\n",
      "Step 2400 avg train loss = 5.1370\n",
      "Validation loss after 4 epoch = 5.3533\n",
      "Step 0 avg train loss = 4.9614\n",
      "Step 100 avg train loss = 4.9881\n",
      "Step 200 avg train loss = 4.9930\n",
      "Step 300 avg train loss = 4.9957\n",
      "Step 400 avg train loss = 4.9981\n",
      "Step 500 avg train loss = 5.0335\n",
      "Step 600 avg train loss = 5.0058\n",
      "Step 700 avg train loss = 5.0163\n",
      "Step 800 avg train loss = 5.0262\n",
      "Step 900 avg train loss = 5.0070\n",
      "Step 1000 avg train loss = 5.0162\n",
      "Step 1100 avg train loss = 5.0254\n",
      "Step 1200 avg train loss = 5.0215\n",
      "Step 1300 avg train loss = 5.0481\n",
      "Step 1400 avg train loss = 5.0537\n",
      "Step 1500 avg train loss = 5.0157\n",
      "Step 1600 avg train loss = 5.0271\n",
      "Step 1700 avg train loss = 5.0452\n",
      "Step 1800 avg train loss = 5.0482\n",
      "Step 1900 avg train loss = 5.0571\n",
      "Step 2000 avg train loss = 5.0585\n",
      "Step 2100 avg train loss = 5.0548\n",
      "Step 2200 avg train loss = 5.0718\n",
      "Step 2300 avg train loss = 5.0502\n",
      "Step 2400 avg train loss = 5.0668\n",
      "Validation loss after 5 epoch = 5.3472\n",
      "Step 0 avg train loss = 4.6264\n",
      "Step 100 avg train loss = 4.8768\n",
      "Step 200 avg train loss = 4.8906\n",
      "Step 300 avg train loss = 4.9250\n",
      "Step 400 avg train loss = 4.9417\n",
      "Step 500 avg train loss = 4.9256\n",
      "Step 600 avg train loss = 4.9113\n",
      "Step 700 avg train loss = 4.9395\n",
      "Step 800 avg train loss = 4.9448\n",
      "Step 900 avg train loss = 4.9667\n",
      "Step 1000 avg train loss = 4.9601\n",
      "Step 1100 avg train loss = 4.9478\n",
      "Step 1200 avg train loss = 4.9584\n",
      "Step 1300 avg train loss = 4.9506\n",
      "Step 1400 avg train loss = 4.9579\n",
      "Step 1500 avg train loss = 4.9472\n",
      "Step 1600 avg train loss = 4.9463\n",
      "Step 1700 avg train loss = 4.9658\n",
      "Step 1800 avg train loss = 4.9846\n",
      "Step 1900 avg train loss = 4.9655\n",
      "Step 2000 avg train loss = 4.9623\n",
      "Step 2100 avg train loss = 4.9612\n",
      "Step 2200 avg train loss = 4.9866\n",
      "Step 2300 avg train loss = 4.9885\n",
      "Step 2400 avg train loss = 4.9851\n",
      "Validation loss after 6 epoch = 5.3364\n",
      "Step 0 avg train loss = 4.6999\n",
      "Step 100 avg train loss = 4.8034\n",
      "Step 200 avg train loss = 4.8232\n",
      "Step 300 avg train loss = 4.8265\n",
      "Step 400 avg train loss = 4.8425\n",
      "Step 500 avg train loss = 4.8447\n",
      "Step 600 avg train loss = 4.8616\n",
      "Step 700 avg train loss = 4.8481\n",
      "Step 800 avg train loss = 4.8731\n",
      "Step 900 avg train loss = 4.8717\n",
      "Step 1000 avg train loss = 4.8781\n",
      "Step 1100 avg train loss = 4.8740\n",
      "Step 1200 avg train loss = 4.8982\n",
      "Step 1300 avg train loss = 4.8931\n",
      "Step 1400 avg train loss = 4.8746\n",
      "Step 1500 avg train loss = 4.9195\n",
      "Step 1600 avg train loss = 4.9044\n",
      "Step 1700 avg train loss = 4.9141\n",
      "Step 1800 avg train loss = 4.9002\n",
      "Step 1900 avg train loss = 4.9196\n",
      "Step 2000 avg train loss = 4.9474\n",
      "Step 2100 avg train loss = 4.9398\n",
      "Step 2200 avg train loss = 4.9307\n",
      "Step 2300 avg train loss = 4.9061\n",
      "Step 2400 avg train loss = 4.9255\n",
      "Validation loss after 7 epoch = 5.3327\n",
      "Step 0 avg train loss = 4.8402\n",
      "Step 100 avg train loss = 4.7445\n",
      "Step 200 avg train loss = 4.7859\n",
      "Step 300 avg train loss = 4.7803\n",
      "Step 400 avg train loss = 4.7722\n",
      "Step 500 avg train loss = 4.7946\n",
      "Step 600 avg train loss = 4.8095\n",
      "Step 700 avg train loss = 4.8091\n",
      "Step 800 avg train loss = 4.7761\n",
      "Step 900 avg train loss = 4.7978\n",
      "Step 1000 avg train loss = 4.8107\n",
      "Step 1100 avg train loss = 4.8230\n",
      "Step 1200 avg train loss = 4.8098\n",
      "Step 1300 avg train loss = 4.8364\n",
      "Step 1400 avg train loss = 4.8364\n",
      "Step 1500 avg train loss = 4.8636\n",
      "Step 1600 avg train loss = 4.8358\n",
      "Step 1700 avg train loss = 4.8498\n",
      "Step 1800 avg train loss = 4.8783\n",
      "Step 1900 avg train loss = 4.8310\n",
      "Step 2000 avg train loss = 4.8547\n",
      "Step 2100 avg train loss = 4.8483\n",
      "Step 2200 avg train loss = 4.8747\n",
      "Step 2300 avg train loss = 4.8591\n",
      "Step 2400 avg train loss = 4.8645\n",
      "Validation loss after 8 epoch = 5.3978\n",
      "Step 0 avg train loss = 4.6977\n",
      "Step 100 avg train loss = 4.7431\n",
      "Step 200 avg train loss = 4.7601\n",
      "Step 300 avg train loss = 4.7529\n",
      "Step 400 avg train loss = 4.7423\n",
      "Step 500 avg train loss = 4.7585\n",
      "Step 600 avg train loss = 4.7371\n",
      "Step 700 avg train loss = 4.7473\n",
      "Step 800 avg train loss = 4.7847\n",
      "Step 900 avg train loss = 4.7827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 avg train loss = 4.7657\n",
      "Step 1100 avg train loss = 4.7588\n",
      "Step 1200 avg train loss = 4.7826\n",
      "Step 1300 avg train loss = 4.7706\n",
      "Step 1400 avg train loss = 4.7808\n",
      "Step 1500 avg train loss = 4.7988\n",
      "Step 1600 avg train loss = 4.7761\n",
      "Step 1700 avg train loss = 4.7935\n",
      "Step 1800 avg train loss = 4.7860\n",
      "Step 1900 avg train loss = 4.8002\n",
      "Step 2000 avg train loss = 4.7845\n",
      "Step 2100 avg train loss = 4.8196\n",
      "Step 2200 avg train loss = 4.8286\n",
      "Step 2300 avg train loss = 4.8175\n",
      "Step 2400 avg train loss = 4.7874\n",
      "Validation loss after 9 epoch = 5.3382\n",
      "Step 0 avg train loss = 4.5673\n",
      "Step 100 avg train loss = 4.6618\n",
      "Step 200 avg train loss = 4.6595\n",
      "Step 300 avg train loss = 4.6603\n",
      "Step 400 avg train loss = 4.6979\n",
      "Step 500 avg train loss = 4.6858\n",
      "Step 600 avg train loss = 4.6957\n",
      "Step 700 avg train loss = 4.7160\n",
      "Step 800 avg train loss = 4.7336\n",
      "Step 900 avg train loss = 4.7253\n",
      "Step 1000 avg train loss = 4.7179\n",
      "Step 1100 avg train loss = 4.7218\n",
      "Step 1200 avg train loss = 4.7345\n",
      "Step 1300 avg train loss = 4.7303\n",
      "Step 1400 avg train loss = 4.7420\n",
      "Step 1500 avg train loss = 4.7372\n",
      "Step 1600 avg train loss = 4.7471\n",
      "Step 1700 avg train loss = 4.7465\n",
      "Step 1800 avg train loss = 4.7840\n",
      "Step 1900 avg train loss = 4.7734\n",
      "Step 2000 avg train loss = 4.7525\n",
      "Step 2100 avg train loss = 4.7478\n",
      "Step 2200 avg train loss = 4.7748\n",
      "Step 2300 avg train loss = 4.7715\n",
      "Step 2400 avg train loss = 4.7947\n",
      "Validation loss after 10 epoch = 5.3426\n",
      "Step 0 avg train loss = 4.6072\n",
      "Step 100 avg train loss = 4.6680\n",
      "Step 200 avg train loss = 4.6250\n",
      "Step 300 avg train loss = 4.6387\n",
      "Step 400 avg train loss = 4.6669\n",
      "Step 500 avg train loss = 4.6573\n",
      "Step 600 avg train loss = 4.6721\n",
      "Step 700 avg train loss = 4.6652\n",
      "Step 800 avg train loss = 4.6909\n",
      "Step 900 avg train loss = 4.6871\n",
      "Step 1000 avg train loss = 4.6705\n",
      "Step 1100 avg train loss = 4.6824\n",
      "Step 1200 avg train loss = 4.6949\n",
      "Step 1300 avg train loss = 4.7013\n",
      "Step 1400 avg train loss = 4.6873\n",
      "Step 1500 avg train loss = 4.7170\n",
      "Step 1600 avg train loss = 4.7154\n",
      "Step 1700 avg train loss = 4.7234\n",
      "Step 1800 avg train loss = 4.7262\n",
      "Step 1900 avg train loss = 4.6801\n",
      "Step 2000 avg train loss = 4.7380\n",
      "Step 2100 avg train loss = 4.7335\n",
      "Step 2200 avg train loss = 4.7533\n",
      "Step 2300 avg train loss = 4.7408\n",
      "Step 2400 avg train loss = 4.7463\n",
      "Validation loss after 11 epoch = 5.3482\n",
      "Step 0 avg train loss = 4.3401\n",
      "Step 100 avg train loss = 4.5772\n",
      "Step 200 avg train loss = 4.6036\n",
      "Step 300 avg train loss = 4.6005\n",
      "Step 400 avg train loss = 4.6271\n",
      "Step 500 avg train loss = 4.6071\n",
      "Step 600 avg train loss = 4.6242\n",
      "Step 700 avg train loss = 4.6333\n",
      "Step 800 avg train loss = 4.6224\n",
      "Step 900 avg train loss = 4.6269\n",
      "Step 1000 avg train loss = 4.6283\n",
      "Step 1100 avg train loss = 4.6600\n",
      "Step 1200 avg train loss = 4.6543\n",
      "Step 1300 avg train loss = 4.6545\n",
      "Step 1400 avg train loss = 4.6833\n",
      "Step 1500 avg train loss = 4.6645\n",
      "Step 1600 avg train loss = 4.6867\n",
      "Step 1700 avg train loss = 4.6859\n",
      "Step 1800 avg train loss = 4.6760\n",
      "Step 1900 avg train loss = 4.6830\n",
      "Step 2000 avg train loss = 4.6628\n",
      "Step 2100 avg train loss = 4.6994\n",
      "Step 2200 avg train loss = 4.7000\n",
      "Step 2300 avg train loss = 4.7075\n",
      "Step 2400 avg train loss = 4.7066\n",
      "Validation loss after 12 epoch = 5.3542\n",
      "Step 0 avg train loss = 4.4476\n",
      "Step 100 avg train loss = 4.5505\n",
      "Step 200 avg train loss = 4.5859\n",
      "Step 300 avg train loss = 4.5835\n",
      "Step 400 avg train loss = 4.5703\n",
      "Step 500 avg train loss = 4.5980\n",
      "Step 600 avg train loss = 4.5854\n",
      "Step 700 avg train loss = 4.6147\n",
      "Step 800 avg train loss = 4.6097\n",
      "Step 900 avg train loss = 4.6062\n",
      "Step 1000 avg train loss = 4.6072\n",
      "Step 1100 avg train loss = 4.6208\n",
      "Step 1200 avg train loss = 4.6287\n",
      "Step 1300 avg train loss = 4.6230\n",
      "Step 1400 avg train loss = 4.6341\n",
      "Step 1500 avg train loss = 4.6358\n",
      "Step 1600 avg train loss = 4.6244\n",
      "Step 1700 avg train loss = 4.6627\n",
      "Step 1800 avg train loss = 4.6392\n",
      "Step 1900 avg train loss = 4.6543\n",
      "Step 2000 avg train loss = 4.6597\n",
      "Step 2100 avg train loss = 4.6624\n",
      "Step 2200 avg train loss = 4.6572\n",
      "Step 2300 avg train loss = 4.6790\n",
      "Step 2400 avg train loss = 4.6650\n",
      "Validation loss after 13 epoch = 5.3613\n",
      "Step 0 avg train loss = 4.4110\n",
      "Step 100 avg train loss = 4.5094\n",
      "Step 200 avg train loss = 4.5172\n",
      "Step 300 avg train loss = 4.5507\n",
      "Step 400 avg train loss = 4.6086\n",
      "Step 500 avg train loss = 4.5688\n",
      "Step 600 avg train loss = 4.5726\n",
      "Step 700 avg train loss = 4.5672\n",
      "Step 800 avg train loss = 4.5873\n",
      "Step 900 avg train loss = 4.5811\n",
      "Step 1000 avg train loss = 4.6270\n",
      "Step 1100 avg train loss = 4.5987\n",
      "Step 1200 avg train loss = 4.6025\n",
      "Step 1300 avg train loss = 4.5862\n",
      "Step 1400 avg train loss = 4.5897\n",
      "Step 1500 avg train loss = 4.6336\n",
      "Step 1600 avg train loss = 4.6214\n",
      "Step 1700 avg train loss = 4.5961\n",
      "Step 1800 avg train loss = 4.6274\n",
      "Step 1900 avg train loss = 4.6338\n",
      "Step 2000 avg train loss = 4.6575\n",
      "Step 2100 avg train loss = 4.6322\n",
      "Step 2200 avg train loss = 4.6471\n",
      "Step 2300 avg train loss = 4.6034\n",
      "Step 2400 avg train loss = 4.6357\n",
      "Validation loss after 14 epoch = 5.3731\n",
      "Step 0 avg train loss = 4.3555\n",
      "Step 100 avg train loss = 4.4774\n",
      "Step 200 avg train loss = 4.4874\n",
      "Step 300 avg train loss = 4.4968\n",
      "Step 400 avg train loss = 4.5179\n",
      "Step 500 avg train loss = 4.5392\n",
      "Step 600 avg train loss = 4.5149\n",
      "Step 700 avg train loss = 4.5417\n",
      "Step 800 avg train loss = 4.5515\n",
      "Step 900 avg train loss = 4.5486\n",
      "Step 1000 avg train loss = 4.5413\n",
      "Step 1100 avg train loss = 4.5592\n",
      "Step 1200 avg train loss = 4.5669\n",
      "Step 1300 avg train loss = 4.5594\n",
      "Step 1400 avg train loss = 4.5562\n",
      "Step 1500 avg train loss = 4.5653\n",
      "Step 1600 avg train loss = 4.5736\n",
      "Step 1700 avg train loss = 4.5965\n",
      "Step 1800 avg train loss = 4.5960\n",
      "Step 1900 avg train loss = 4.6075\n",
      "Step 2000 avg train loss = 4.6112\n",
      "Step 2100 avg train loss = 4.5897\n",
      "Step 2200 avg train loss = 4.5954\n",
      "Step 2300 avg train loss = 4.5999\n",
      "Step 2400 avg train loss = 4.6213\n",
      "Validation loss after 15 epoch = 5.3743\n",
      "Step 0 avg train loss = 4.5588\n",
      "Step 100 avg train loss = 4.4453\n",
      "Step 200 avg train loss = 4.4811\n",
      "Step 300 avg train loss = 4.4844\n",
      "Step 400 avg train loss = 4.4531\n",
      "Step 500 avg train loss = 4.4730\n",
      "Step 600 avg train loss = 4.5017\n",
      "Step 700 avg train loss = 4.5078\n",
      "Step 800 avg train loss = 4.5217\n",
      "Step 900 avg train loss = 4.5204\n",
      "Step 1000 avg train loss = 4.5183\n",
      "Step 1100 avg train loss = 4.5365\n",
      "Step 1200 avg train loss = 4.5293\n",
      "Step 1300 avg train loss = 4.5448\n",
      "Step 1400 avg train loss = 4.5426\n",
      "Step 1500 avg train loss = 4.5473\n",
      "Step 1600 avg train loss = 4.5563\n",
      "Step 1700 avg train loss = 4.5598\n",
      "Step 1800 avg train loss = 4.5511\n",
      "Step 1900 avg train loss = 4.5793\n",
      "Step 2000 avg train loss = 4.5925\n",
      "Step 2100 avg train loss = 4.5964\n",
      "Step 2200 avg train loss = 4.5759\n",
      "Step 2300 avg train loss = 4.5801\n",
      "Step 2400 avg train loss = 4.5921\n",
      "Validation loss after 16 epoch = 5.3823\n",
      "Step 0 avg train loss = 4.3229\n",
      "Step 100 avg train loss = 4.4073\n",
      "Step 200 avg train loss = 4.4447\n",
      "Step 300 avg train loss = 4.4326\n",
      "Step 400 avg train loss = 4.4637\n",
      "Step 500 avg train loss = 4.4715\n",
      "Step 600 avg train loss = 4.4713\n",
      "Step 700 avg train loss = 4.4837\n",
      "Step 800 avg train loss = 4.4945\n",
      "Step 900 avg train loss = 4.4909\n",
      "Step 1000 avg train loss = 4.4992\n",
      "Step 1100 avg train loss = 4.5172\n",
      "Step 1200 avg train loss = 4.5113\n",
      "Step 1300 avg train loss = 4.5480\n",
      "Step 1400 avg train loss = 4.5424\n",
      "Step 1500 avg train loss = 4.5117\n",
      "Step 1600 avg train loss = 4.5472\n",
      "Step 1700 avg train loss = 4.5396\n",
      "Step 1800 avg train loss = 4.5385\n",
      "Step 1900 avg train loss = 4.5448\n",
      "Step 2000 avg train loss = 4.6011\n",
      "Step 2100 avg train loss = 4.6925\n",
      "Step 2200 avg train loss = 4.6362\n",
      "Step 2300 avg train loss = 4.5940\n",
      "Step 2400 avg train loss = 4.6070\n",
      "Validation loss after 17 epoch = 5.3981\n",
      "Step 0 avg train loss = 4.4178\n",
      "Step 100 avg train loss = 4.4342\n",
      "Step 200 avg train loss = 4.4463\n",
      "Step 300 avg train loss = 4.4395\n",
      "Step 400 avg train loss = 4.4743\n",
      "Step 500 avg train loss = 4.4982\n",
      "Step 600 avg train loss = 4.4881\n",
      "Step 700 avg train loss = 4.4714\n",
      "Step 800 avg train loss = 4.4924\n",
      "Step 900 avg train loss = 4.4864\n",
      "Step 1000 avg train loss = 4.5029\n",
      "Step 1100 avg train loss = 4.5075\n",
      "Step 1200 avg train loss = 4.4609\n",
      "Step 1300 avg train loss = 4.5053\n",
      "Step 1400 avg train loss = 4.5071\n",
      "Step 1500 avg train loss = 4.5504\n",
      "Step 1600 avg train loss = 4.5450\n",
      "Step 1700 avg train loss = 4.5323\n",
      "Step 1800 avg train loss = 4.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900 avg train loss = 4.5133\n",
      "Step 2000 avg train loss = 4.5262\n",
      "Step 2100 avg train loss = 4.5452\n",
      "Step 2200 avg train loss = 4.5498\n",
      "Step 2300 avg train loss = 4.5443\n",
      "Step 2400 avg train loss = 4.5089\n",
      "Validation loss after 18 epoch = 5.4030\n",
      "Step 0 avg train loss = 4.3478\n",
      "Step 100 avg train loss = 4.3634\n",
      "Step 200 avg train loss = 4.4092\n",
      "Step 300 avg train loss = 4.3912\n",
      "Step 400 avg train loss = 4.4102\n",
      "Step 500 avg train loss = 4.4815\n",
      "Step 600 avg train loss = 4.4947\n",
      "Step 700 avg train loss = 4.4555\n",
      "Step 800 avg train loss = 4.4605\n",
      "Step 900 avg train loss = 4.4632\n",
      "Step 1000 avg train loss = 4.5010\n",
      "Step 1100 avg train loss = 4.4885\n",
      "Step 1200 avg train loss = 4.4630\n",
      "Step 1300 avg train loss = 4.4777\n",
      "Step 1400 avg train loss = 4.4776\n",
      "Step 1500 avg train loss = 4.4847\n",
      "Step 1600 avg train loss = 4.4683\n",
      "Step 1700 avg train loss = 4.4854\n",
      "Step 1800 avg train loss = 4.5125\n",
      "Step 1900 avg train loss = 4.5027\n",
      "Step 2000 avg train loss = 4.4803\n",
      "Step 2100 avg train loss = 4.5116\n",
      "Step 2200 avg train loss = 4.5167\n",
      "Step 2300 avg train loss = 4.5267\n",
      "Step 2400 avg train loss = 4.5102\n",
      "Validation loss after 19 epoch = 5.4346\n",
      "Step 0 avg train loss = 4.3895\n",
      "Step 100 avg train loss = 4.3711\n",
      "Step 200 avg train loss = 4.3840\n",
      "Step 300 avg train loss = 4.3866\n",
      "Step 400 avg train loss = 4.4210\n",
      "Step 500 avg train loss = 4.4129\n",
      "Step 600 avg train loss = 4.3946\n",
      "Step 700 avg train loss = 4.4175\n",
      "Step 800 avg train loss = 4.4296\n",
      "Step 900 avg train loss = 4.4744\n",
      "Step 1000 avg train loss = 4.4235\n",
      "Step 1100 avg train loss = 4.4541\n",
      "Step 1200 avg train loss = 4.4559\n",
      "Step 1300 avg train loss = 4.4547\n",
      "Step 1400 avg train loss = 4.4618\n",
      "Step 1500 avg train loss = 4.4653\n",
      "Step 1600 avg train loss = 4.4581\n",
      "Step 1700 avg train loss = 4.4777\n",
      "Step 1800 avg train loss = 4.4725\n",
      "Step 1900 avg train loss = 4.5064\n",
      "Step 2000 avg train loss = 4.5059\n",
      "Step 2100 avg train loss = 4.4921\n",
      "Step 2200 avg train loss = 4.4870\n",
      "Step 2300 avg train loss = 4.4904\n",
      "Step 2400 avg train loss = 4.4884\n",
      "Validation loss after 20 epoch = 5.4142\n",
      "Step 0 avg train loss = 4.2535\n",
      "Step 100 avg train loss = 4.3259\n",
      "Step 200 avg train loss = 4.3570\n",
      "Step 300 avg train loss = 4.3373\n",
      "Step 400 avg train loss = 4.3636\n",
      "Step 500 avg train loss = 4.3776\n",
      "Step 600 avg train loss = 4.3856\n",
      "Step 700 avg train loss = 4.3815\n",
      "Step 800 avg train loss = 4.3974\n",
      "Step 900 avg train loss = 4.4371\n",
      "Step 1000 avg train loss = 4.4134\n",
      "Step 1100 avg train loss = 4.4046\n",
      "Step 1200 avg train loss = 4.4430\n",
      "Step 1300 avg train loss = 4.4385\n",
      "Step 1400 avg train loss = 4.4508\n",
      "Step 1500 avg train loss = 4.4443\n",
      "Step 1600 avg train loss = 4.4650\n",
      "Step 1700 avg train loss = 4.4646\n",
      "Step 1800 avg train loss = 4.4907\n",
      "Step 1900 avg train loss = 4.4653\n",
      "Step 2000 avg train loss = 4.4842\n",
      "Step 2100 avg train loss = 4.4734\n",
      "Step 2200 avg train loss = 4.4548\n",
      "Step 2300 avg train loss = 4.4866\n",
      "Step 2400 avg train loss = 4.4748\n",
      "Validation loss after 21 epoch = 5.4432\n",
      "Step 0 avg train loss = 4.3027\n",
      "Step 100 avg train loss = 4.3303\n",
      "Step 200 avg train loss = 4.3423\n",
      "Step 300 avg train loss = 4.3346\n",
      "Step 400 avg train loss = 4.3596\n",
      "Step 500 avg train loss = 4.3523\n",
      "Step 600 avg train loss = 4.3849\n",
      "Step 700 avg train loss = 4.3805\n",
      "Step 800 avg train loss = 4.3680\n",
      "Step 900 avg train loss = 4.3929\n",
      "Step 1000 avg train loss = 4.4134\n",
      "Step 1100 avg train loss = 4.4115\n",
      "Step 1200 avg train loss = 4.4110\n",
      "Step 1300 avg train loss = 4.3986\n",
      "Step 1400 avg train loss = 4.4241\n",
      "Step 1500 avg train loss = 4.4343\n",
      "Step 1600 avg train loss = 4.4346\n",
      "Step 1700 avg train loss = 4.4426\n",
      "Step 1800 avg train loss = 4.4437\n",
      "Step 1900 avg train loss = 4.4479\n",
      "Step 2000 avg train loss = 4.4830\n",
      "Step 2100 avg train loss = 4.4782\n",
      "Step 2200 avg train loss = 4.4822\n",
      "Step 2300 avg train loss = 4.4912\n",
      "Step 2400 avg train loss = 4.4621\n",
      "Validation loss after 22 epoch = 5.4369\n",
      "Step 0 avg train loss = 4.1254\n",
      "Step 100 avg train loss = 4.2974\n",
      "Step 200 avg train loss = 4.3326\n",
      "Step 300 avg train loss = 4.3116\n",
      "Step 400 avg train loss = 4.3595\n",
      "Step 500 avg train loss = 4.3665\n",
      "Step 600 avg train loss = 4.3553\n",
      "Step 700 avg train loss = 4.3518\n",
      "Step 800 avg train loss = 4.3939\n",
      "Step 900 avg train loss = 4.3634\n",
      "Step 1000 avg train loss = 4.3926\n",
      "Step 1100 avg train loss = 4.4093\n",
      "Step 1200 avg train loss = 4.4039\n",
      "Step 1300 avg train loss = 4.4208\n",
      "Step 1400 avg train loss = 4.4049\n",
      "Step 1500 avg train loss = 4.3911\n",
      "Step 1600 avg train loss = 4.4068\n",
      "Step 1700 avg train loss = 4.4124\n",
      "Step 1800 avg train loss = 4.4114\n",
      "Step 1900 avg train loss = 4.4111\n",
      "Step 2000 avg train loss = 4.4233\n",
      "Step 2100 avg train loss = 4.4287\n",
      "Step 2200 avg train loss = 4.4472\n",
      "Step 2300 avg train loss = 4.4519\n",
      "Step 2400 avg train loss = 4.4614\n",
      "Validation loss after 23 epoch = 5.4436\n",
      "Step 0 avg train loss = 4.2136\n",
      "Step 100 avg train loss = 4.2992\n",
      "Step 200 avg train loss = 4.3149\n",
      "Step 300 avg train loss = 4.2963\n",
      "Step 400 avg train loss = 4.3295\n",
      "Step 500 avg train loss = 4.3214\n",
      "Step 600 avg train loss = 4.3428\n",
      "Step 700 avg train loss = 4.3432\n",
      "Step 800 avg train loss = 4.3601\n",
      "Step 900 avg train loss = 4.3757\n",
      "Step 1000 avg train loss = 4.3775\n",
      "Step 1100 avg train loss = 4.3733\n",
      "Step 1200 avg train loss = 4.3829\n",
      "Step 1300 avg train loss = 4.3712\n",
      "Step 1400 avg train loss = 4.3806\n",
      "Step 1500 avg train loss = 4.3997\n",
      "Step 1600 avg train loss = 4.4112\n",
      "Step 1700 avg train loss = 4.4084\n",
      "Step 1800 avg train loss = 4.4229\n",
      "Step 1900 avg train loss = 4.4252\n",
      "Step 2000 avg train loss = 4.4332\n",
      "Step 2100 avg train loss = 4.4233\n",
      "Step 2200 avg train loss = 4.4334\n",
      "Step 2300 avg train loss = 4.4004\n",
      "Step 2400 avg train loss = 4.4222\n",
      "Validation loss after 24 epoch = 5.4557\n",
      "Step 0 avg train loss = 4.3158\n",
      "Step 100 avg train loss = 4.2718\n",
      "Step 200 avg train loss = 4.2885\n",
      "Step 300 avg train loss = 4.2770\n",
      "Step 400 avg train loss = 4.3331\n",
      "Step 500 avg train loss = 4.3351\n",
      "Step 600 avg train loss = 4.3345\n",
      "Step 700 avg train loss = 4.3369\n",
      "Step 800 avg train loss = 4.3339\n",
      "Step 900 avg train loss = 4.3502\n",
      "Step 1000 avg train loss = 4.3545\n",
      "Step 1100 avg train loss = 4.3863\n",
      "Step 1200 avg train loss = 4.3832\n",
      "Step 1300 avg train loss = 4.3732\n",
      "Step 1400 avg train loss = 4.3680\n",
      "Step 1500 avg train loss = 4.3814\n",
      "Step 1600 avg train loss = 4.3878\n",
      "Step 1700 avg train loss = 4.3783\n",
      "Step 1800 avg train loss = 4.3961\n",
      "Step 1900 avg train loss = 4.3910\n",
      "Step 2000 avg train loss = 4.4136\n",
      "Step 2100 avg train loss = 4.3960\n",
      "Step 2200 avg train loss = 4.4209\n",
      "Step 2300 avg train loss = 4.4280\n",
      "Step 2400 avg train loss = 4.4152\n",
      "Validation loss after 25 epoch = 5.4579\n",
      "Step 0 avg train loss = 4.5130\n",
      "Step 100 avg train loss = 4.2619\n",
      "Step 200 avg train loss = 4.2720\n",
      "Step 300 avg train loss = 4.2844\n",
      "Step 400 avg train loss = 4.2985\n",
      "Step 500 avg train loss = 4.2966\n",
      "Step 600 avg train loss = 4.3071\n",
      "Step 700 avg train loss = 4.3358\n",
      "Step 800 avg train loss = 4.3231\n",
      "Step 900 avg train loss = 4.3308\n",
      "Step 1000 avg train loss = 4.3373\n",
      "Step 1100 avg train loss = 4.3313\n",
      "Step 1200 avg train loss = 4.3276\n",
      "Step 1300 avg train loss = 4.3456\n",
      "Step 1400 avg train loss = 4.3467\n",
      "Step 1500 avg train loss = 4.3753\n",
      "Step 1600 avg train loss = 4.3927\n",
      "Step 1700 avg train loss = 4.3806\n",
      "Step 1800 avg train loss = 4.3757\n",
      "Step 1900 avg train loss = 4.3889\n",
      "Step 2000 avg train loss = 4.3751\n",
      "Step 2100 avg train loss = 4.3959\n",
      "Step 2200 avg train loss = 4.3940\n",
      "Step 2300 avg train loss = 4.3954\n",
      "Step 2400 avg train loss = 4.4047\n",
      "Validation loss after 26 epoch = 5.4624\n",
      "Step 0 avg train loss = 3.9940\n",
      "Step 100 avg train loss = 4.2313\n",
      "Step 200 avg train loss = 4.2837\n",
      "Step 300 avg train loss = 4.2869\n",
      "Step 400 avg train loss = 4.2780\n",
      "Step 500 avg train loss = 4.2756\n",
      "Step 600 avg train loss = 4.2930\n",
      "Step 700 avg train loss = 4.3119\n",
      "Step 800 avg train loss = 4.3297\n",
      "Step 900 avg train loss = 4.3348\n",
      "Step 1000 avg train loss = 4.3594\n",
      "Step 1100 avg train loss = 4.3272\n",
      "Step 1200 avg train loss = 4.3222\n",
      "Step 1300 avg train loss = 4.3400\n",
      "Step 1400 avg train loss = 4.3330\n",
      "Step 1500 avg train loss = 4.3470\n",
      "Step 1600 avg train loss = 4.3445\n",
      "Step 1700 avg train loss = 4.3559\n",
      "Step 1800 avg train loss = 4.3889\n",
      "Step 1900 avg train loss = 4.3614\n",
      "Step 2000 avg train loss = 4.3903\n",
      "Step 2100 avg train loss = 4.3876\n",
      "Step 2200 avg train loss = 4.3791\n",
      "Step 2300 avg train loss = 4.3775\n",
      "Step 2400 avg train loss = 4.3820\n",
      "Validation loss after 27 epoch = 5.4728\n",
      "Step 0 avg train loss = 4.3110\n",
      "Step 100 avg train loss = 4.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 avg train loss = 4.2323\n",
      "Step 300 avg train loss = 4.2624\n",
      "Step 400 avg train loss = 4.2691\n",
      "Step 500 avg train loss = 4.2689\n",
      "Step 600 avg train loss = 4.2838\n",
      "Step 700 avg train loss = 4.2992\n",
      "Step 800 avg train loss = 4.3568\n",
      "Step 900 avg train loss = 4.3213\n",
      "Step 1000 avg train loss = 4.3344\n",
      "Step 1100 avg train loss = 4.3254\n",
      "Step 1200 avg train loss = 4.3510\n",
      "Step 1300 avg train loss = 4.3461\n",
      "Step 1400 avg train loss = 4.3610\n",
      "Step 1500 avg train loss = 4.3429\n",
      "Step 1600 avg train loss = 4.3647\n",
      "Step 1700 avg train loss = 4.3467\n",
      "Step 1800 avg train loss = 4.3616\n",
      "Step 1900 avg train loss = 4.3593\n",
      "Step 2000 avg train loss = 4.3872\n",
      "Step 2100 avg train loss = 4.4999\n",
      "Step 2200 avg train loss = 4.3964\n",
      "Step 2300 avg train loss = 4.4047\n",
      "Step 2400 avg train loss = 4.4325\n",
      "Validation loss after 28 epoch = 5.4940\n",
      "Step 0 avg train loss = 4.2147\n",
      "Step 100 avg train loss = 4.2228\n",
      "Step 200 avg train loss = 4.2549\n",
      "Step 300 avg train loss = 4.2149\n",
      "Step 400 avg train loss = 4.2651\n",
      "Step 500 avg train loss = 4.2745\n",
      "Step 600 avg train loss = 4.2766\n",
      "Step 700 avg train loss = 4.2745\n",
      "Step 800 avg train loss = 4.2874\n",
      "Step 900 avg train loss = 4.2989\n",
      "Step 1000 avg train loss = 4.2812\n",
      "Step 1100 avg train loss = 4.3230\n",
      "Step 1200 avg train loss = 4.2995\n",
      "Step 1300 avg train loss = 4.3044\n",
      "Step 1400 avg train loss = 4.3312\n",
      "Step 1500 avg train loss = 4.3484\n",
      "Step 1600 avg train loss = 4.3788\n",
      "Step 1700 avg train loss = 4.3473\n",
      "Step 1800 avg train loss = 4.3583\n",
      "Step 1900 avg train loss = 4.3292\n",
      "Step 2000 avg train loss = 4.3651\n",
      "Step 2100 avg train loss = 4.3409\n",
      "Step 2200 avg train loss = 4.3575\n",
      "Step 2300 avg train loss = 4.3597\n",
      "Step 2400 avg train loss = 4.3583\n",
      "Validation loss after 29 epoch = 5.4936\n",
      "Step 0 avg train loss = 3.9491\n",
      "Step 100 avg train loss = 4.1880\n",
      "Step 200 avg train loss = 4.2111\n",
      "Step 300 avg train loss = 4.2847\n",
      "Step 400 avg train loss = 4.2460\n",
      "Step 500 avg train loss = 4.2658\n",
      "Step 600 avg train loss = 4.2555\n",
      "Step 700 avg train loss = 4.2663\n",
      "Step 800 avg train loss = 4.2586\n",
      "Step 900 avg train loss = 4.2736\n",
      "Step 1000 avg train loss = 4.2764\n",
      "Step 1100 avg train loss = 4.2843\n",
      "Step 1200 avg train loss = 4.2988\n",
      "Step 1300 avg train loss = 4.3143\n",
      "Step 1400 avg train loss = 4.3068\n",
      "Step 1500 avg train loss = 4.3258\n",
      "Step 1600 avg train loss = 4.3406\n",
      "Step 1700 avg train loss = 4.3155\n",
      "Step 1800 avg train loss = 4.3316\n",
      "Step 1900 avg train loss = 4.3426\n",
      "Step 2000 avg train loss = 4.3377\n",
      "Step 2100 avg train loss = 4.3452\n",
      "Step 2200 avg train loss = 4.3285\n",
      "Step 2300 avg train loss = 4.3597\n",
      "Step 2400 avg train loss = 4.3440\n",
      "Validation loss after 30 epoch = 5.4984\n",
      "Step 0 avg train loss = 3.8698\n",
      "Step 100 avg train loss = 4.2031\n",
      "Step 200 avg train loss = 4.1966\n",
      "Step 300 avg train loss = 4.2202\n",
      "Step 400 avg train loss = 4.2410\n",
      "Step 500 avg train loss = 4.2438\n",
      "Step 600 avg train loss = 4.2502\n",
      "Step 700 avg train loss = 4.2606\n",
      "Step 800 avg train loss = 4.2538\n",
      "Step 900 avg train loss = 4.2525\n",
      "Step 1000 avg train loss = 4.2810\n",
      "Step 1100 avg train loss = 4.2935\n",
      "Step 1200 avg train loss = 4.2660\n",
      "Step 1300 avg train loss = 4.3022\n",
      "Step 1400 avg train loss = 4.2997\n",
      "Step 1500 avg train loss = 4.2902\n",
      "Step 1600 avg train loss = 4.3131\n",
      "Step 1700 avg train loss = 4.3230\n",
      "Step 1800 avg train loss = 4.3157\n",
      "Step 1900 avg train loss = 4.3130\n",
      "Step 2000 avg train loss = 4.2915\n",
      "Step 2100 avg train loss = 4.3204\n",
      "Step 2200 avg train loss = 4.3346\n",
      "Step 2300 avg train loss = 4.3474\n",
      "Step 2400 avg train loss = 4.3216\n",
      "Validation loss after 31 epoch = 5.5129\n",
      "Step 0 avg train loss = 4.0918\n",
      "Step 100 avg train loss = 4.1804\n",
      "Step 200 avg train loss = 4.1966\n",
      "Step 300 avg train loss = 4.2073\n",
      "Step 400 avg train loss = 4.2361\n",
      "Step 500 avg train loss = 4.2336\n",
      "Step 600 avg train loss = 4.2471\n",
      "Step 700 avg train loss = 4.2360\n",
      "Step 800 avg train loss = 4.2483\n",
      "Step 900 avg train loss = 4.2381\n",
      "Step 1000 avg train loss = 4.2790\n",
      "Step 1100 avg train loss = 4.2602\n",
      "Step 1200 avg train loss = 4.2561\n",
      "Step 1300 avg train loss = 4.2836\n",
      "Step 1400 avg train loss = 4.2733\n",
      "Step 1500 avg train loss = 4.2962\n",
      "Step 1600 avg train loss = 4.2890\n",
      "Step 1700 avg train loss = 4.3080\n",
      "Step 1800 avg train loss = 4.2996\n",
      "Step 1900 avg train loss = 4.3062\n",
      "Step 2000 avg train loss = 4.3069\n",
      "Step 2100 avg train loss = 4.3282\n",
      "Step 2200 avg train loss = 4.3567\n",
      "Step 2300 avg train loss = 4.3358\n",
      "Step 2400 avg train loss = 4.3516\n",
      "Validation loss after 32 epoch = 5.5191\n",
      "Step 0 avg train loss = 4.1664\n",
      "Step 100 avg train loss = 4.1685\n",
      "Step 200 avg train loss = 4.2003\n",
      "Step 300 avg train loss = 4.1961\n",
      "Step 400 avg train loss = 4.2142\n",
      "Step 500 avg train loss = 4.2062\n",
      "Step 600 avg train loss = 4.1885\n",
      "Step 700 avg train loss = 4.2301\n",
      "Step 800 avg train loss = 4.3268\n",
      "Step 900 avg train loss = 4.3386\n",
      "Step 1000 avg train loss = 4.2625\n",
      "Step 1100 avg train loss = 4.2708\n",
      "Step 1200 avg train loss = 4.2448\n",
      "Step 1300 avg train loss = 4.2664\n",
      "Step 1400 avg train loss = 4.2854\n",
      "Step 1500 avg train loss = 4.2717\n",
      "Step 1600 avg train loss = 4.2631\n",
      "Step 1700 avg train loss = 4.2901\n",
      "Step 1800 avg train loss = 4.2639\n",
      "Step 1900 avg train loss = 4.2999\n",
      "Step 2000 avg train loss = 4.3377\n",
      "Step 2100 avg train loss = 4.3404\n",
      "Step 2200 avg train loss = 4.3338\n",
      "Step 2300 avg train loss = 4.3173\n",
      "Step 2400 avg train loss = 4.3060\n",
      "Validation loss after 33 epoch = 5.5259\n",
      "Step 0 avg train loss = 4.0655\n",
      "Step 100 avg train loss = 4.1594\n",
      "Step 200 avg train loss = 4.1740\n",
      "Step 300 avg train loss = 4.1841\n",
      "Step 400 avg train loss = 4.1967\n",
      "Step 500 avg train loss = 4.2121\n",
      "Step 600 avg train loss = 4.2084\n",
      "Step 700 avg train loss = 4.2185\n",
      "Step 800 avg train loss = 4.2082\n",
      "Step 900 avg train loss = 4.2909\n",
      "Step 1000 avg train loss = 4.2411\n",
      "Step 1100 avg train loss = 4.2422\n",
      "Step 1200 avg train loss = 4.2451\n",
      "Step 1300 avg train loss = 4.2837\n",
      "Step 1400 avg train loss = 4.3077\n",
      "Step 1500 avg train loss = 4.2817\n",
      "Step 1600 avg train loss = 4.2934\n",
      "Step 1700 avg train loss = 4.3077\n",
      "Step 1800 avg train loss = 4.3526\n",
      "Step 1900 avg train loss = 4.3201\n",
      "Step 2000 avg train loss = 4.3193\n",
      "Step 2100 avg train loss = 4.2836\n",
      "Step 2200 avg train loss = 4.3104\n",
      "Step 2300 avg train loss = 4.3198\n",
      "Step 2400 avg train loss = 4.3101\n",
      "Validation loss after 34 epoch = 5.5343\n",
      "Step 0 avg train loss = 4.3189\n",
      "Step 100 avg train loss = 4.1558\n",
      "Step 200 avg train loss = 4.1624\n",
      "Step 300 avg train loss = 4.1615\n",
      "Step 400 avg train loss = 4.1943\n",
      "Step 500 avg train loss = 4.2008\n",
      "Step 600 avg train loss = 4.2543\n",
      "Step 700 avg train loss = 4.4443\n",
      "Step 800 avg train loss = 4.3913\n",
      "Step 900 avg train loss = 4.3660\n",
      "Step 1000 avg train loss = 4.3642\n",
      "Step 1100 avg train loss = 4.3445\n",
      "Step 1200 avg train loss = 4.3449\n",
      "Step 1300 avg train loss = 4.3292\n",
      "Step 1400 avg train loss = 4.3322\n",
      "Step 1500 avg train loss = 4.3250\n",
      "Step 1600 avg train loss = 4.3184\n",
      "Step 1700 avg train loss = 4.3305\n",
      "Step 1800 avg train loss = 4.3256\n",
      "Step 1900 avg train loss = 4.3075\n",
      "Step 2000 avg train loss = 4.3361\n",
      "Step 2100 avg train loss = 4.3271\n",
      "Step 2200 avg train loss = 4.3290\n",
      "Step 2300 avg train loss = 4.3444\n",
      "Step 2400 avg train loss = 4.3309\n",
      "Validation loss after 35 epoch = 5.5313\n",
      "Step 0 avg train loss = 4.3559\n",
      "Step 100 avg train loss = 4.1513\n",
      "Step 200 avg train loss = 4.1743\n",
      "Step 300 avg train loss = 4.2047\n",
      "Step 400 avg train loss = 4.1914\n",
      "Step 500 avg train loss = 4.1903\n",
      "Step 600 avg train loss = 4.2396\n",
      "Step 700 avg train loss = 4.2278\n",
      "Step 800 avg train loss = 4.2487\n",
      "Step 900 avg train loss = 4.2380\n",
      "Step 1000 avg train loss = 4.2389\n",
      "Step 1100 avg train loss = 4.2506\n",
      "Step 1200 avg train loss = 4.2447\n",
      "Step 1300 avg train loss = 4.2681\n",
      "Step 1400 avg train loss = 4.2259\n",
      "Step 1500 avg train loss = 4.2456\n",
      "Step 1600 avg train loss = 4.3096\n",
      "Step 1700 avg train loss = 4.2816\n",
      "Step 1800 avg train loss = 4.2787\n",
      "Step 1900 avg train loss = 4.3016\n",
      "Step 2000 avg train loss = 4.2739\n",
      "Step 2100 avg train loss = 4.2934\n",
      "Step 2200 avg train loss = 4.2801\n",
      "Step 2300 avg train loss = 4.2933\n",
      "Step 2400 avg train loss = 4.2948\n",
      "Validation loss after 36 epoch = 5.5523\n",
      "Step 0 avg train loss = 4.0570\n",
      "Step 100 avg train loss = 4.1586\n",
      "Step 200 avg train loss = 4.1391\n",
      "Step 300 avg train loss = 4.1511\n",
      "Step 400 avg train loss = 4.1551\n",
      "Step 500 avg train loss = 4.1702\n",
      "Step 600 avg train loss = 4.1868\n",
      "Step 700 avg train loss = 4.1846\n",
      "Step 800 avg train loss = 4.2009\n",
      "Step 900 avg train loss = 4.1936\n",
      "Step 1000 avg train loss = 4.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100 avg train loss = 4.1971\n",
      "Step 1200 avg train loss = 4.2423\n",
      "Step 1300 avg train loss = 4.2572\n",
      "Step 1400 avg train loss = 4.2484\n",
      "Step 1500 avg train loss = 4.2294\n",
      "Step 1600 avg train loss = 4.2473\n",
      "Step 1700 avg train loss = 4.2432\n",
      "Step 1800 avg train loss = 4.2279\n",
      "Step 1900 avg train loss = 4.2680\n",
      "Step 2000 avg train loss = 4.3068\n",
      "Step 2100 avg train loss = 4.2813\n",
      "Step 2200 avg train loss = 4.2765\n",
      "Step 2300 avg train loss = 4.2889\n",
      "Step 2400 avg train loss = 4.2702\n",
      "Validation loss after 37 epoch = 5.5486\n",
      "Step 0 avg train loss = 3.9832\n",
      "Step 100 avg train loss = 4.1192\n",
      "Step 200 avg train loss = 4.1160\n",
      "Step 300 avg train loss = 4.1290\n",
      "Step 400 avg train loss = 4.1297\n",
      "Step 500 avg train loss = 4.1726\n",
      "Step 600 avg train loss = 4.1801\n",
      "Step 700 avg train loss = 4.2943\n",
      "Step 800 avg train loss = 4.2565\n",
      "Step 900 avg train loss = 4.2276\n",
      "Step 1000 avg train loss = 4.2098\n",
      "Step 1100 avg train loss = 4.2011\n",
      "Step 1200 avg train loss = 4.2353\n",
      "Step 1300 avg train loss = 4.2109\n",
      "Step 1400 avg train loss = 4.4141\n",
      "Step 1500 avg train loss = 4.3120\n",
      "Step 1600 avg train loss = 4.2877\n",
      "Step 1700 avg train loss = 4.2661\n",
      "Step 1800 avg train loss = 4.2622\n",
      "Step 1900 avg train loss = 4.2450\n",
      "Step 2000 avg train loss = 4.2453\n",
      "Step 2100 avg train loss = 4.2643\n",
      "Step 2200 avg train loss = 4.2786\n",
      "Step 2300 avg train loss = 4.2633\n",
      "Step 2400 avg train loss = 4.2817\n",
      "Validation loss after 38 epoch = 5.5611\n",
      "Step 0 avg train loss = 4.0594\n",
      "Step 100 avg train loss = 4.1202\n",
      "Step 200 avg train loss = 4.1353\n",
      "Step 300 avg train loss = 4.2294\n",
      "Step 400 avg train loss = 4.2362\n",
      "Step 500 avg train loss = 4.1699\n",
      "Step 600 avg train loss = 4.1789\n",
      "Step 700 avg train loss = 4.2058\n",
      "Step 800 avg train loss = 4.2008\n",
      "Step 900 avg train loss = 4.2108\n",
      "Step 1000 avg train loss = 4.1982\n",
      "Step 1100 avg train loss = 4.2215\n",
      "Step 1200 avg train loss = 4.4374\n",
      "Step 1300 avg train loss = 4.4691\n",
      "Step 1400 avg train loss = 4.3498\n",
      "Step 1500 avg train loss = 4.2843\n",
      "Step 1600 avg train loss = 4.2855\n",
      "Step 1700 avg train loss = 4.2765\n",
      "Step 1800 avg train loss = 4.2533\n",
      "Step 1900 avg train loss = 4.2608\n",
      "Step 2000 avg train loss = 4.2546\n",
      "Step 2100 avg train loss = 4.2617\n",
      "Step 2200 avg train loss = 4.2895\n",
      "Step 2300 avg train loss = 4.2694\n",
      "Step 2400 avg train loss = 4.2748\n",
      "Validation loss after 39 epoch = 5.5647\n",
      "Step 0 avg train loss = 4.2461\n",
      "Step 100 avg train loss = 4.1365\n",
      "Step 200 avg train loss = 4.1560\n",
      "Step 300 avg train loss = 4.1309\n",
      "Step 400 avg train loss = 4.1464\n",
      "Step 500 avg train loss = 4.1482\n",
      "Step 600 avg train loss = 4.2973\n",
      "Step 700 avg train loss = 4.3550\n",
      "Step 800 avg train loss = 4.2320\n",
      "Step 900 avg train loss = 4.2479\n",
      "Step 1000 avg train loss = 4.2084\n",
      "Step 1100 avg train loss = 4.2235\n",
      "Step 1200 avg train loss = 4.2062\n",
      "Step 1300 avg train loss = 4.2002\n",
      "Step 1400 avg train loss = 4.2386\n",
      "Step 1500 avg train loss = 4.2033\n",
      "Step 1600 avg train loss = 4.2148\n",
      "Step 1700 avg train loss = 4.2166\n",
      "Step 1800 avg train loss = 4.2269\n",
      "Step 1900 avg train loss = 4.2339\n",
      "Step 2000 avg train loss = 4.2354\n",
      "Step 2100 avg train loss = 4.2593\n",
      "Step 2200 avg train loss = 4.2446\n",
      "Step 2300 avg train loss = 4.2611\n",
      "Step 2400 avg train loss = 4.2574\n",
      "Validation loss after 40 epoch = 5.5747\n",
      "Step 0 avg train loss = 3.9828\n",
      "Step 100 avg train loss = 4.1056\n",
      "Step 200 avg train loss = 4.1155\n",
      "Step 300 avg train loss = 4.1228\n",
      "Step 400 avg train loss = 4.1354\n",
      "Step 500 avg train loss = 4.1342\n",
      "Step 600 avg train loss = 4.1465\n",
      "Step 700 avg train loss = 4.1364\n",
      "Step 800 avg train loss = 4.1556\n",
      "Step 900 avg train loss = 4.1447\n",
      "Step 1000 avg train loss = 4.1687\n",
      "Step 1100 avg train loss = 4.1921\n",
      "Step 1200 avg train loss = 4.1773\n",
      "Step 1300 avg train loss = 4.1866\n",
      "Step 1400 avg train loss = 4.2566\n",
      "Step 1500 avg train loss = 4.2367\n",
      "Step 1600 avg train loss = 4.2463\n",
      "Step 1700 avg train loss = 4.2223\n",
      "Step 1800 avg train loss = 4.2473\n",
      "Step 1900 avg train loss = 4.2414\n",
      "Step 2000 avg train loss = 4.2436\n",
      "Step 2100 avg train loss = 4.2271\n",
      "Step 2200 avg train loss = 4.2350\n",
      "Step 2300 avg train loss = 4.2350\n",
      "Step 2400 avg train loss = 4.2854\n",
      "Validation loss after 41 epoch = 5.5857\n",
      "Step 0 avg train loss = 4.0012\n",
      "Step 100 avg train loss = 4.0727\n",
      "Step 200 avg train loss = 4.0820\n",
      "Step 300 avg train loss = 4.1392\n",
      "Step 400 avg train loss = 4.1259\n",
      "Step 500 avg train loss = 4.1139\n",
      "Step 600 avg train loss = 4.1402\n",
      "Step 700 avg train loss = 4.1485\n",
      "Step 800 avg train loss = 4.1655\n",
      "Step 900 avg train loss = 4.2726\n",
      "Step 1000 avg train loss = 4.2502\n",
      "Step 1100 avg train loss = 4.2062\n",
      "Step 1200 avg train loss = 4.1835\n",
      "Step 1300 avg train loss = 4.1871\n",
      "Step 1400 avg train loss = 4.2178\n",
      "Step 1500 avg train loss = 4.2395\n",
      "Step 1600 avg train loss = 4.2309\n",
      "Step 1700 avg train loss = 4.2366\n",
      "Step 1800 avg train loss = 4.2238\n",
      "Step 1900 avg train loss = 4.2108\n",
      "Step 2000 avg train loss = 4.2012\n",
      "Step 2100 avg train loss = 4.2437\n",
      "Step 2200 avg train loss = 4.2377\n",
      "Step 2300 avg train loss = 4.2378\n",
      "Step 2400 avg train loss = 4.2577\n",
      "Validation loss after 42 epoch = 5.5793\n",
      "Step 0 avg train loss = 3.7721\n",
      "Step 100 avg train loss = 4.0666\n",
      "Step 200 avg train loss = 4.0852\n",
      "Step 300 avg train loss = 4.1091\n",
      "Step 400 avg train loss = 4.1190\n",
      "Step 500 avg train loss = 4.1293\n",
      "Step 600 avg train loss = 4.1176\n",
      "Step 700 avg train loss = 4.1420\n",
      "Step 800 avg train loss = 4.1610\n",
      "Step 900 avg train loss = 4.1531\n",
      "Step 1000 avg train loss = 4.1654\n",
      "Step 1100 avg train loss = 4.1857\n",
      "Step 1200 avg train loss = 4.2218\n",
      "Step 1300 avg train loss = 4.1988\n",
      "Step 1400 avg train loss = 4.2243\n",
      "Step 1500 avg train loss = 4.2242\n",
      "Step 1600 avg train loss = 4.2652\n",
      "Step 1700 avg train loss = 4.2434\n",
      "Step 1800 avg train loss = 4.2135\n",
      "Step 1900 avg train loss = 4.2154\n",
      "Step 2000 avg train loss = 4.2142\n",
      "Step 2100 avg train loss = 4.2007\n",
      "Step 2200 avg train loss = 4.2017\n",
      "Step 2300 avg train loss = 4.2196\n",
      "Step 2400 avg train loss = 4.2257\n",
      "Validation loss after 43 epoch = 5.6040\n",
      "Step 0 avg train loss = 4.1507\n",
      "Step 100 avg train loss = 4.0498\n",
      "Step 200 avg train loss = 4.1095\n",
      "Step 300 avg train loss = 4.1049\n",
      "Step 400 avg train loss = 4.1175\n",
      "Step 500 avg train loss = 4.1031\n",
      "Step 600 avg train loss = 4.1106\n",
      "Step 700 avg train loss = 4.1437\n",
      "Step 800 avg train loss = 4.1525\n",
      "Step 900 avg train loss = 4.1450\n",
      "Step 1000 avg train loss = 4.1323\n",
      "Step 1100 avg train loss = 4.1514\n",
      "Step 1200 avg train loss = 4.1537\n",
      "Step 1300 avg train loss = 4.1881\n",
      "Step 1400 avg train loss = 4.1740\n",
      "Step 1500 avg train loss = 4.1381\n",
      "Step 1600 avg train loss = 4.2016\n",
      "Step 1700 avg train loss = 4.1978\n",
      "Step 1800 avg train loss = 4.2000\n",
      "Step 1900 avg train loss = 4.2040\n",
      "Step 2000 avg train loss = 4.2162\n",
      "Step 2100 avg train loss = 4.2696\n",
      "Step 2200 avg train loss = 4.2626\n",
      "Step 2300 avg train loss = 4.2252\n",
      "Step 2400 avg train loss = 4.2267\n",
      "Validation loss after 44 epoch = 5.6065\n",
      "Step 0 avg train loss = 3.9369\n",
      "Step 100 avg train loss = 4.0728\n",
      "Step 200 avg train loss = 4.0796\n",
      "Step 300 avg train loss = 4.0979\n",
      "Step 400 avg train loss = 4.1276\n",
      "Step 500 avg train loss = 4.0997\n",
      "Step 600 avg train loss = 4.1369\n",
      "Step 700 avg train loss = 4.1349\n",
      "Step 800 avg train loss = 4.1240\n",
      "Step 900 avg train loss = 4.1527\n",
      "Step 1000 avg train loss = 4.1460\n",
      "Step 1100 avg train loss = 4.1390\n",
      "Step 1200 avg train loss = 4.1377\n",
      "Step 1300 avg train loss = 4.1601\n",
      "Step 1400 avg train loss = 4.1782\n",
      "Step 1500 avg train loss = 4.1794\n",
      "Step 1600 avg train loss = 4.1791\n",
      "Step 1700 avg train loss = 4.1889\n",
      "Step 1800 avg train loss = 4.1791\n",
      "Step 1900 avg train loss = 4.2814\n",
      "Step 2000 avg train loss = 4.2505\n",
      "Step 2100 avg train loss = 4.2313\n",
      "Step 2200 avg train loss = 4.2013\n",
      "Step 2300 avg train loss = 4.1946\n",
      "Step 2400 avg train loss = 4.2604\n",
      "Validation loss after 45 epoch = 5.6039\n",
      "Step 0 avg train loss = 4.2256\n",
      "Step 100 avg train loss = 4.0706\n",
      "Step 200 avg train loss = 4.0761\n",
      "Step 300 avg train loss = 4.2655\n",
      "Step 400 avg train loss = 4.1771\n",
      "Step 500 avg train loss = 4.1441\n",
      "Step 600 avg train loss = 4.1375\n",
      "Step 700 avg train loss = 4.2009\n",
      "Step 800 avg train loss = 4.1725\n",
      "Step 900 avg train loss = 4.1661\n",
      "Step 1000 avg train loss = 4.1550\n",
      "Step 1100 avg train loss = 4.1466\n",
      "Step 1200 avg train loss = 4.1441\n",
      "Step 1300 avg train loss = 4.3394\n",
      "Step 1400 avg train loss = 4.2287\n",
      "Step 1500 avg train loss = 4.2134\n",
      "Step 1600 avg train loss = 4.1845\n",
      "Step 1700 avg train loss = 4.2234\n",
      "Step 1800 avg train loss = 4.2028\n",
      "Step 1900 avg train loss = 4.2169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000 avg train loss = 4.1954\n",
      "Step 2100 avg train loss = 4.1880\n",
      "Step 2200 avg train loss = 4.1936\n",
      "Step 2300 avg train loss = 4.2093\n",
      "Step 2400 avg train loss = 4.2125\n",
      "Validation loss after 46 epoch = 5.6037\n",
      "Step 0 avg train loss = 4.2805\n",
      "Step 100 avg train loss = 4.0499\n",
      "Step 200 avg train loss = 4.0676\n",
      "Step 300 avg train loss = 4.0736\n",
      "Step 400 avg train loss = 4.0734\n",
      "Step 500 avg train loss = 4.0755\n",
      "Step 600 avg train loss = 4.0905\n",
      "Step 700 avg train loss = 4.1096\n",
      "Step 800 avg train loss = 4.1103\n",
      "Step 900 avg train loss = 4.1181\n",
      "Step 1000 avg train loss = 4.1226\n",
      "Step 1100 avg train loss = 4.2436\n",
      "Step 1200 avg train loss = 4.1984\n",
      "Step 1300 avg train loss = 4.1584\n",
      "Step 1400 avg train loss = 4.1682\n",
      "Step 1500 avg train loss = 4.1482\n",
      "Step 1600 avg train loss = 4.1395\n",
      "Step 1700 avg train loss = 4.1717\n",
      "Step 1800 avg train loss = 4.1559\n",
      "Step 1900 avg train loss = 4.1953\n",
      "Step 2000 avg train loss = 4.1837\n",
      "Step 2100 avg train loss = 4.1836\n",
      "Step 2200 avg train loss = 4.1905\n",
      "Step 2300 avg train loss = 4.1783\n",
      "Step 2400 avg train loss = 4.1968\n",
      "Validation loss after 47 epoch = 5.6166\n",
      "Step 0 avg train loss = 3.9987\n",
      "Step 100 avg train loss = 4.0327\n",
      "Step 200 avg train loss = 4.0620\n",
      "Step 300 avg train loss = 4.0559\n",
      "Step 400 avg train loss = 4.0893\n",
      "Step 500 avg train loss = 4.0694\n",
      "Step 600 avg train loss = 4.0964\n",
      "Step 700 avg train loss = 4.1490\n",
      "Step 800 avg train loss = 4.1266\n",
      "Step 900 avg train loss = 4.1249\n",
      "Step 1000 avg train loss = 4.1398\n",
      "Step 1100 avg train loss = 4.1394\n",
      "Step 1200 avg train loss = 4.1374\n",
      "Step 1300 avg train loss = 4.1604\n",
      "Step 1400 avg train loss = 4.1467\n",
      "Step 1500 avg train loss = 4.1788\n",
      "Step 1600 avg train loss = 4.2093\n",
      "Step 1700 avg train loss = 4.1752\n",
      "Step 1800 avg train loss = 4.1797\n",
      "Step 1900 avg train loss = 4.1742\n",
      "Step 2000 avg train loss = 4.1761\n",
      "Step 2100 avg train loss = 4.1782\n",
      "Step 2200 avg train loss = 4.1746\n",
      "Step 2300 avg train loss = 4.2165\n",
      "Step 2400 avg train loss = 4.2518\n",
      "Validation loss after 48 epoch = 5.6227\n",
      "Step 0 avg train loss = 4.1255\n",
      "Step 100 avg train loss = 4.0472\n",
      "Step 200 avg train loss = 4.0445\n",
      "Step 300 avg train loss = 4.0661\n",
      "Step 400 avg train loss = 4.0862\n",
      "Step 500 avg train loss = 4.0925\n",
      "Step 600 avg train loss = 4.0954\n",
      "Step 700 avg train loss = 4.1136\n",
      "Step 800 avg train loss = 4.1055\n",
      "Step 900 avg train loss = 4.1855\n",
      "Step 1000 avg train loss = 4.1249\n",
      "Step 1100 avg train loss = 4.1391\n",
      "Step 1200 avg train loss = 4.1674\n",
      "Step 1300 avg train loss = 4.1988\n",
      "Step 1400 avg train loss = 4.1705\n",
      "Step 1500 avg train loss = 4.1745\n",
      "Step 1600 avg train loss = 4.1583\n",
      "Step 1700 avg train loss = 4.1786\n",
      "Step 1800 avg train loss = 4.1358\n",
      "Step 1900 avg train loss = 4.1764\n",
      "Step 2000 avg train loss = 4.1679\n",
      "Step 2100 avg train loss = 4.1593\n",
      "Step 2200 avg train loss = 4.1894\n",
      "Step 2300 avg train loss = 4.1816\n",
      "Step 2400 avg train loss = 4.1734\n",
      "Validation loss after 49 epoch = 5.6218\n",
      "Step 0 avg train loss = 3.8288\n",
      "Step 100 avg train loss = 4.0454\n",
      "Step 200 avg train loss = 4.0451\n",
      "Step 300 avg train loss = 4.0683\n",
      "Step 400 avg train loss = 4.0572\n",
      "Step 500 avg train loss = 4.0924\n",
      "Step 600 avg train loss = 4.1327\n",
      "Step 700 avg train loss = 4.1027\n",
      "Step 800 avg train loss = 4.0971\n",
      "Step 900 avg train loss = 4.1026\n",
      "Step 1000 avg train loss = 4.0884\n",
      "Step 1100 avg train loss = 4.1537\n",
      "Step 1200 avg train loss = 4.1431\n",
      "Step 1300 avg train loss = 4.1452\n",
      "Step 1400 avg train loss = 4.1295\n",
      "Step 1500 avg train loss = 4.1502\n",
      "Step 1600 avg train loss = 4.1543\n",
      "Step 1700 avg train loss = 4.1512\n",
      "Step 1800 avg train loss = 4.1528\n",
      "Step 1900 avg train loss = 4.1405\n",
      "Step 2000 avg train loss = 4.1556\n",
      "Step 2100 avg train loss = 4.1565\n",
      "Step 2200 avg train loss = 4.1725\n",
      "Step 2300 avg train loss = 4.1601\n",
      "Step 2400 avg train loss = 4.2268\n",
      "Validation loss after 50 epoch = 5.6434\n",
      "Step 0 avg train loss = 3.9041\n",
      "Step 100 avg train loss = 4.0430\n",
      "Step 200 avg train loss = 4.0500\n",
      "Step 300 avg train loss = 4.0366\n",
      "Step 400 avg train loss = 4.0389\n",
      "Step 500 avg train loss = 4.0440\n",
      "Step 600 avg train loss = 4.0719\n",
      "Step 700 avg train loss = 4.0944\n",
      "Step 800 avg train loss = 4.1007\n",
      "Step 900 avg train loss = 4.0873\n",
      "Step 1000 avg train loss = 4.0798\n",
      "Step 1100 avg train loss = 4.0982\n",
      "Step 1200 avg train loss = 4.1060\n",
      "Step 1300 avg train loss = 4.1236\n",
      "Step 1400 avg train loss = 4.1325\n",
      "Step 1500 avg train loss = 4.1513\n",
      "Step 1600 avg train loss = 4.1453\n",
      "Step 1700 avg train loss = 4.1606\n",
      "Step 1800 avg train loss = 4.1952\n",
      "Step 1900 avg train loss = 4.1476\n",
      "Step 2000 avg train loss = 4.1447\n",
      "Step 2100 avg train loss = 4.1756\n",
      "Step 2200 avg train loss = 4.1710\n",
      "Step 2300 avg train loss = 4.1524\n",
      "Step 2400 avg train loss = 4.1670\n",
      "Validation loss after 51 epoch = 5.6363\n",
      "Step 0 avg train loss = 3.9695\n",
      "Step 100 avg train loss = 4.1182\n",
      "Step 200 avg train loss = 4.0721\n",
      "Step 300 avg train loss = 4.0784\n",
      "Step 400 avg train loss = 4.0575\n",
      "Step 500 avg train loss = 4.0803\n",
      "Step 600 avg train loss = 4.1683\n",
      "Step 700 avg train loss = 4.1625\n",
      "Step 800 avg train loss = 4.1284\n",
      "Step 900 avg train loss = 4.1244\n",
      "Step 1000 avg train loss = 4.1096\n",
      "Step 1100 avg train loss = 4.1263\n",
      "Step 1200 avg train loss = 4.1136\n",
      "Step 1300 avg train loss = 4.1280\n",
      "Step 1400 avg train loss = 4.1290\n",
      "Step 1500 avg train loss = 4.1336\n",
      "Step 1600 avg train loss = 4.1333\n",
      "Step 1700 avg train loss = 4.1052\n",
      "Step 1800 avg train loss = 4.1217\n",
      "Step 1900 avg train loss = 4.1457\n",
      "Step 2000 avg train loss = 4.1502\n",
      "Step 2100 avg train loss = 4.1297\n",
      "Step 2200 avg train loss = 4.1571\n",
      "Step 2300 avg train loss = 4.3943\n",
      "Step 2400 avg train loss = 4.2660\n",
      "Validation loss after 52 epoch = 5.6632\n",
      "Step 0 avg train loss = 3.9835\n",
      "Step 100 avg train loss = 4.0601\n",
      "Step 200 avg train loss = 4.0194\n",
      "Step 300 avg train loss = 4.0583\n",
      "Step 400 avg train loss = 4.0388\n",
      "Step 500 avg train loss = 4.0384\n",
      "Step 600 avg train loss = 4.0690\n",
      "Step 700 avg train loss = 4.0689\n",
      "Step 800 avg train loss = 4.0706\n",
      "Step 900 avg train loss = 4.0853\n",
      "Step 1000 avg train loss = 4.0926\n",
      "Step 1100 avg train loss = 4.0926\n",
      "Step 1200 avg train loss = 4.0690\n",
      "Step 1300 avg train loss = 4.1039\n",
      "Step 1400 avg train loss = 4.1071\n",
      "Step 1500 avg train loss = 4.1461\n",
      "Step 1600 avg train loss = 4.1177\n",
      "Step 1700 avg train loss = 4.1097\n",
      "Step 1800 avg train loss = 4.1600\n",
      "Step 1900 avg train loss = 4.1675\n",
      "Step 2000 avg train loss = 4.1804\n",
      "Step 2100 avg train loss = 4.2164\n",
      "Step 2200 avg train loss = 4.1771\n",
      "Step 2300 avg train loss = 4.1517\n",
      "Step 2400 avg train loss = 4.2064\n",
      "Validation loss after 53 epoch = 5.6517\n",
      "Step 0 avg train loss = 4.1124\n",
      "Step 100 avg train loss = 4.0169\n",
      "Step 200 avg train loss = 4.0211\n",
      "Step 300 avg train loss = 4.0358\n",
      "Step 400 avg train loss = 4.0371\n",
      "Step 500 avg train loss = 4.0570\n",
      "Step 600 avg train loss = 4.0515\n",
      "Step 700 avg train loss = 4.0636\n",
      "Step 800 avg train loss = 4.0831\n",
      "Step 900 avg train loss = 4.0755\n",
      "Step 1000 avg train loss = 4.0911\n",
      "Step 1100 avg train loss = 4.0818\n",
      "Step 1200 avg train loss = 4.1017\n",
      "Step 1300 avg train loss = 4.1701\n",
      "Step 1400 avg train loss = 4.1167\n",
      "Step 1500 avg train loss = 4.2407\n",
      "Step 1600 avg train loss = 4.1674\n",
      "Step 1700 avg train loss = 4.1594\n",
      "Step 1800 avg train loss = 4.1550\n",
      "Step 1900 avg train loss = 4.1352\n",
      "Step 2000 avg train loss = 4.1473\n",
      "Step 2100 avg train loss = 4.1404\n",
      "Step 2200 avg train loss = 4.1740\n",
      "Step 2300 avg train loss = 4.1728\n",
      "Step 2400 avg train loss = 4.1502\n",
      "Validation loss after 54 epoch = 5.6553\n",
      "Step 0 avg train loss = 4.0538\n",
      "Step 100 avg train loss = 4.0121\n",
      "Step 200 avg train loss = 4.0239\n",
      "Step 300 avg train loss = 4.0171\n",
      "Step 400 avg train loss = 4.0315\n",
      "Step 500 avg train loss = 4.0563\n",
      "Step 600 avg train loss = 4.0604\n",
      "Step 700 avg train loss = 4.0310\n",
      "Step 800 avg train loss = 4.0656\n",
      "Step 900 avg train loss = 4.0909\n",
      "Step 1000 avg train loss = 4.0790\n",
      "Step 1100 avg train loss = 4.1744\n",
      "Step 1200 avg train loss = 4.1031\n",
      "Step 1300 avg train loss = 4.1200\n",
      "Step 1400 avg train loss = 4.0973\n",
      "Step 1500 avg train loss = 4.1145\n",
      "Step 1600 avg train loss = 4.1334\n",
      "Step 1700 avg train loss = 4.5456\n",
      "Step 1800 avg train loss = 4.3755\n",
      "Step 1900 avg train loss = 4.2770\n",
      "Step 2000 avg train loss = 4.2914\n",
      "Step 2100 avg train loss = 4.2542\n",
      "Step 2200 avg train loss = 4.2514\n",
      "Step 2300 avg train loss = 4.2787\n",
      "Step 2400 avg train loss = 4.2455\n",
      "Validation loss after 55 epoch = 5.6744\n",
      "Step 0 avg train loss = 4.1163\n",
      "Step 100 avg train loss = 4.0614\n",
      "Step 200 avg train loss = 4.0863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300 avg train loss = 4.0819\n",
      "Step 400 avg train loss = 4.0739\n",
      "Step 500 avg train loss = 4.0925\n",
      "Step 600 avg train loss = 4.1024\n",
      "Step 700 avg train loss = 4.0981\n",
      "Step 800 avg train loss = 4.1063\n",
      "Step 900 avg train loss = 4.1223\n",
      "Step 1000 avg train loss = 4.0924\n",
      "Step 1100 avg train loss = 4.1061\n",
      "Step 1200 avg train loss = 4.0932\n",
      "Step 1300 avg train loss = 4.1067\n",
      "Step 1400 avg train loss = 4.1081\n",
      "Step 1500 avg train loss = 4.1048\n",
      "Step 1600 avg train loss = 4.1305\n",
      "Step 1700 avg train loss = 4.1291\n",
      "Step 1800 avg train loss = 4.1144\n",
      "Step 1900 avg train loss = 4.1464\n",
      "Step 2000 avg train loss = 4.1371\n",
      "Step 2100 avg train loss = 4.1371\n",
      "Step 2200 avg train loss = 4.1576\n",
      "Step 2300 avg train loss = 4.1456\n",
      "Step 2400 avg train loss = 4.1417\n",
      "Validation loss after 56 epoch = 5.6678\n",
      "Step 0 avg train loss = 4.0340\n",
      "Step 100 avg train loss = 4.0217\n",
      "Step 200 avg train loss = 4.0008\n",
      "Step 300 avg train loss = 4.0553\n",
      "Step 400 avg train loss = 4.0562\n",
      "Step 500 avg train loss = 4.0253\n",
      "Step 600 avg train loss = 4.0550\n",
      "Step 700 avg train loss = 4.0613\n",
      "Step 800 avg train loss = 4.0437\n",
      "Step 900 avg train loss = 4.0649\n",
      "Step 1000 avg train loss = 4.0497\n",
      "Step 1100 avg train loss = 4.1200\n",
      "Step 1200 avg train loss = 4.4433\n",
      "Step 1300 avg train loss = 4.2589\n",
      "Step 1400 avg train loss = 4.2031\n",
      "Step 1500 avg train loss = 4.1594\n",
      "Step 1600 avg train loss = 4.1424\n",
      "Step 1700 avg train loss = 4.2530\n",
      "Step 1800 avg train loss = 4.1924\n",
      "Step 1900 avg train loss = 4.1349\n",
      "Step 2000 avg train loss = 4.1313\n",
      "Step 2100 avg train loss = 4.1555\n",
      "Step 2200 avg train loss = 4.1277\n",
      "Step 2300 avg train loss = 4.1228\n",
      "Step 2400 avg train loss = 4.1255\n",
      "Validation loss after 57 epoch = 5.6659\n",
      "Step 0 avg train loss = 3.9647\n",
      "Step 100 avg train loss = 3.9956\n",
      "Step 200 avg train loss = 4.0074\n",
      "Step 300 avg train loss = 4.0592\n",
      "Step 400 avg train loss = 4.1290\n",
      "Step 500 avg train loss = 4.0616\n",
      "Step 600 avg train loss = 4.0492\n",
      "Step 700 avg train loss = 4.0514\n",
      "Step 800 avg train loss = 4.0696\n",
      "Step 900 avg train loss = 4.0431\n",
      "Step 1000 avg train loss = 4.0580\n",
      "Step 1100 avg train loss = 4.0775\n",
      "Step 1200 avg train loss = 4.0713\n",
      "Step 1300 avg train loss = 4.0723\n",
      "Step 1400 avg train loss = 4.1088\n",
      "Step 1500 avg train loss = 4.0751\n",
      "Step 1600 avg train loss = 4.1173\n",
      "Step 1700 avg train loss = 4.0997\n",
      "Step 1800 avg train loss = 4.0927\n",
      "Step 1900 avg train loss = 4.1153\n",
      "Step 2000 avg train loss = 4.1053\n",
      "Step 2100 avg train loss = 4.1136\n",
      "Step 2200 avg train loss = 4.1025\n",
      "Step 2300 avg train loss = 4.1093\n",
      "Step 2400 avg train loss = 4.1028\n",
      "Validation loss after 58 epoch = 5.6755\n",
      "Step 0 avg train loss = 3.8863\n",
      "Step 100 avg train loss = 3.9778\n",
      "Step 200 avg train loss = 4.0237\n",
      "Step 300 avg train loss = 4.0788\n",
      "Step 400 avg train loss = 4.0577\n",
      "Step 500 avg train loss = 4.0361\n",
      "Step 600 avg train loss = 4.0252\n",
      "Step 700 avg train loss = 4.0368\n",
      "Step 800 avg train loss = 4.0445\n",
      "Step 900 avg train loss = 4.0491\n",
      "Step 1000 avg train loss = 4.0493\n",
      "Step 1100 avg train loss = 4.0756\n",
      "Step 1200 avg train loss = 4.0603\n",
      "Step 1300 avg train loss = 4.0597\n",
      "Step 1400 avg train loss = 4.0773\n",
      "Step 1500 avg train loss = 4.0982\n",
      "Step 1600 avg train loss = 4.0861\n",
      "Step 1700 avg train loss = 4.1678\n",
      "Step 1800 avg train loss = 4.1505\n",
      "Step 1900 avg train loss = 4.2385\n",
      "Step 2000 avg train loss = 4.2494\n",
      "Step 2100 avg train loss = 4.1984\n",
      "Step 2200 avg train loss = 4.1539\n",
      "Step 2300 avg train loss = 4.1488\n",
      "Step 2400 avg train loss = 4.1756\n",
      "Validation loss after 59 epoch = 5.6844\n",
      "Step 0 avg train loss = 3.9038\n",
      "Step 100 avg train loss = 4.0070\n",
      "Step 200 avg train loss = 4.0090\n",
      "Step 300 avg train loss = 4.0456\n",
      "Step 400 avg train loss = 4.0794\n",
      "Step 500 avg train loss = 4.0472\n",
      "Step 600 avg train loss = 4.0364\n",
      "Step 700 avg train loss = 4.0526\n",
      "Step 800 avg train loss = 4.0492\n",
      "Step 900 avg train loss = 4.0512\n",
      "Step 1000 avg train loss = 4.0506\n",
      "Step 1100 avg train loss = 4.0753\n",
      "Step 1200 avg train loss = 4.0605\n",
      "Step 1300 avg train loss = 4.0685\n",
      "Step 1400 avg train loss = 4.0664\n",
      "Step 1500 avg train loss = 4.1158\n",
      "Step 1600 avg train loss = 4.1045\n",
      "Step 1700 avg train loss = 4.1189\n",
      "Step 1800 avg train loss = 4.0853\n",
      "Step 1900 avg train loss = 4.0988\n",
      "Step 2000 avg train loss = 4.0988\n",
      "Step 2100 avg train loss = 4.0873\n",
      "Step 2200 avg train loss = 4.1110\n",
      "Step 2300 avg train loss = 4.1235\n",
      "Step 2400 avg train loss = 4.1437\n",
      "Validation loss after 60 epoch = 5.6926\n",
      "Step 0 avg train loss = 3.8356\n",
      "Step 100 avg train loss = 4.0029\n",
      "Step 200 avg train loss = 3.9990\n",
      "Step 300 avg train loss = 3.9864\n",
      "Step 400 avg train loss = 4.0118\n",
      "Step 500 avg train loss = 3.9956\n",
      "Step 600 avg train loss = 4.0426\n",
      "Step 700 avg train loss = 3.9837\n",
      "Step 800 avg train loss = 4.0383\n",
      "Step 900 avg train loss = 4.0337\n",
      "Step 1000 avg train loss = 4.0681\n",
      "Step 1100 avg train loss = 4.0937\n",
      "Step 1200 avg train loss = 4.1633\n",
      "Step 1300 avg train loss = 4.0711\n",
      "Step 1400 avg train loss = 4.0762\n",
      "Step 1500 avg train loss = 4.0720\n",
      "Step 1600 avg train loss = 4.0858\n",
      "Step 1700 avg train loss = 4.0855\n",
      "Step 1800 avg train loss = 4.0649\n",
      "Step 1900 avg train loss = 4.0808\n",
      "Step 2000 avg train loss = 4.1533\n",
      "Step 2100 avg train loss = 4.1432\n",
      "Step 2200 avg train loss = 4.1407\n",
      "Step 2300 avg train loss = 4.1430\n",
      "Step 2400 avg train loss = 4.1797\n",
      "Validation loss after 61 epoch = 5.6945\n",
      "Step 0 avg train loss = 3.9774\n",
      "Step 100 avg train loss = 3.9954\n",
      "Step 200 avg train loss = 4.0349\n",
      "Step 300 avg train loss = 4.0057\n",
      "Step 400 avg train loss = 4.0035\n",
      "Step 500 avg train loss = 4.0057\n",
      "Step 600 avg train loss = 4.0419\n",
      "Step 700 avg train loss = 4.0329\n",
      "Step 800 avg train loss = 4.0364\n",
      "Step 900 avg train loss = 4.2051\n",
      "Step 1000 avg train loss = 4.1805\n",
      "Step 1100 avg train loss = 4.1443\n",
      "Step 1200 avg train loss = 4.1475\n",
      "Step 1300 avg train loss = 4.1141\n",
      "Step 1400 avg train loss = 4.1274\n",
      "Step 1500 avg train loss = 4.1064\n",
      "Step 1600 avg train loss = 4.1021\n",
      "Step 1700 avg train loss = 4.1210\n",
      "Step 1800 avg train loss = 4.1249\n",
      "Step 1900 avg train loss = 4.1333\n",
      "Step 2000 avg train loss = 4.1350\n",
      "Step 2100 avg train loss = 4.1195\n",
      "Step 2200 avg train loss = 4.1952\n",
      "Step 2300 avg train loss = 4.1379\n",
      "Step 2400 avg train loss = 4.1299\n",
      "Validation loss after 62 epoch = 5.7014\n",
      "Step 0 avg train loss = 4.2533\n",
      "Step 100 avg train loss = 3.9799\n",
      "Step 200 avg train loss = 3.9781\n",
      "Step 300 avg train loss = 3.9905\n",
      "Step 400 avg train loss = 3.9978\n",
      "Step 500 avg train loss = 4.0139\n",
      "Step 600 avg train loss = 4.0299\n",
      "Step 700 avg train loss = 4.1133\n",
      "Step 800 avg train loss = 4.0514\n",
      "Step 900 avg train loss = 4.0492\n",
      "Step 1000 avg train loss = 4.0269\n",
      "Step 1100 avg train loss = 4.0546\n",
      "Step 1200 avg train loss = 4.1696\n",
      "Step 1300 avg train loss = 4.1912\n",
      "Step 1400 avg train loss = 4.1377\n",
      "Step 1500 avg train loss = 4.1107\n",
      "Step 1600 avg train loss = 4.0819\n",
      "Step 1700 avg train loss = 4.0924\n",
      "Step 1800 avg train loss = 4.1071\n",
      "Step 1900 avg train loss = 4.0794\n",
      "Step 2000 avg train loss = 4.1014\n",
      "Step 2100 avg train loss = 4.1169\n",
      "Step 2200 avg train loss = 4.0951\n",
      "Step 2300 avg train loss = 4.1035\n",
      "Step 2400 avg train loss = 4.0991\n",
      "Validation loss after 63 epoch = 5.6959\n",
      "Step 0 avg train loss = 3.8503\n",
      "Step 100 avg train loss = 3.9505\n",
      "Step 200 avg train loss = 3.9521\n",
      "Step 300 avg train loss = 3.9832\n",
      "Step 400 avg train loss = 3.9795\n",
      "Step 500 avg train loss = 3.9896\n",
      "Step 600 avg train loss = 4.3465\n",
      "Step 700 avg train loss = 4.2419\n",
      "Step 800 avg train loss = 4.1658\n",
      "Step 900 avg train loss = 4.1442\n",
      "Step 1000 avg train loss = 4.1195\n",
      "Step 1100 avg train loss = 4.1168\n",
      "Step 1200 avg train loss = 4.1039\n",
      "Step 1300 avg train loss = 4.1007\n",
      "Step 1400 avg train loss = 4.0902\n",
      "Step 1500 avg train loss = 4.1093\n",
      "Step 1600 avg train loss = 4.0848\n",
      "Step 1700 avg train loss = 4.1109\n",
      "Step 1800 avg train loss = 4.1070\n",
      "Step 1900 avg train loss = 4.0979\n",
      "Step 2000 avg train loss = 4.0933\n",
      "Step 2100 avg train loss = 4.0954\n",
      "Step 2200 avg train loss = 4.1249\n",
      "Step 2300 avg train loss = 4.1201\n",
      "Step 2400 avg train loss = 4.1263\n",
      "Validation loss after 64 epoch = 5.6995\n",
      "Step 0 avg train loss = 4.0349\n",
      "Step 100 avg train loss = 3.9736\n",
      "Step 200 avg train loss = 3.9676\n",
      "Step 300 avg train loss = 3.9865\n",
      "Step 400 avg train loss = 4.0045\n",
      "Step 500 avg train loss = 4.0074\n",
      "Step 600 avg train loss = 3.9859\n",
      "Step 700 avg train loss = 4.0430\n",
      "Step 800 avg train loss = 4.0416\n",
      "Step 900 avg train loss = 4.0194\n",
      "Step 1000 avg train loss = 4.0214\n",
      "Step 1100 avg train loss = 4.0546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200 avg train loss = 4.1148\n",
      "Step 1300 avg train loss = 4.0604\n",
      "Step 1400 avg train loss = 4.0999\n",
      "Step 1500 avg train loss = 4.0673\n",
      "Step 1600 avg train loss = 4.0741\n",
      "Step 1700 avg train loss = 4.0605\n",
      "Step 1800 avg train loss = 4.0891\n",
      "Step 1900 avg train loss = 4.0928\n",
      "Step 2000 avg train loss = 4.1012\n",
      "Step 2100 avg train loss = 4.1231\n",
      "Step 2200 avg train loss = 4.1037\n",
      "Step 2300 avg train loss = 4.0858\n",
      "Step 2400 avg train loss = 4.0984\n",
      "Validation loss after 65 epoch = 5.7062\n",
      "Step 0 avg train loss = 3.9618\n",
      "Step 100 avg train loss = 4.4189\n",
      "Step 200 avg train loss = 4.3646\n",
      "Step 300 avg train loss = 4.2775\n",
      "Step 400 avg train loss = 4.2026\n",
      "Step 500 avg train loss = 4.1992\n",
      "Step 600 avg train loss = 4.1649\n",
      "Step 700 avg train loss = 4.1380\n",
      "Step 800 avg train loss = 4.1359\n",
      "Step 900 avg train loss = 4.1362\n",
      "Step 1000 avg train loss = 4.1286\n",
      "Step 1100 avg train loss = 4.1394\n",
      "Step 1200 avg train loss = 4.0872\n",
      "Step 1300 avg train loss = 4.1157\n",
      "Step 1400 avg train loss = 4.1359\n",
      "Step 1500 avg train loss = 4.1126\n",
      "Step 1600 avg train loss = 4.0935\n",
      "Step 1700 avg train loss = 4.0983\n",
      "Step 1800 avg train loss = 4.1028\n",
      "Step 1900 avg train loss = 4.0964\n",
      "Step 2000 avg train loss = 4.1237\n",
      "Step 2100 avg train loss = 4.1071\n",
      "Step 2200 avg train loss = 4.1037\n",
      "Step 2300 avg train loss = 4.0835\n",
      "Step 2400 avg train loss = 4.1130\n",
      "Validation loss after 66 epoch = 5.7087\n",
      "Step 0 avg train loss = 3.9775\n",
      "Step 100 avg train loss = 3.9521\n",
      "Step 200 avg train loss = 3.9624\n",
      "Step 300 avg train loss = 3.9688\n",
      "Step 400 avg train loss = 3.9789\n",
      "Step 500 avg train loss = 3.9971\n",
      "Step 600 avg train loss = 4.0026\n",
      "Step 700 avg train loss = 3.9914\n",
      "Step 800 avg train loss = 3.9925\n",
      "Step 900 avg train loss = 4.0241\n",
      "Step 1000 avg train loss = 4.0408\n",
      "Step 1100 avg train loss = 4.0469\n",
      "Step 1200 avg train loss = 4.0299\n",
      "Step 1300 avg train loss = 4.0351\n",
      "Step 1400 avg train loss = 4.0333\n",
      "Step 1500 avg train loss = 4.0342\n",
      "Step 1600 avg train loss = 4.0565\n",
      "Step 1700 avg train loss = 4.0701\n",
      "Step 1800 avg train loss = 4.0459\n",
      "Step 1900 avg train loss = 4.0922\n",
      "Step 2000 avg train loss = 4.0824\n",
      "Step 2100 avg train loss = 4.0968\n",
      "Step 2200 avg train loss = 4.0543\n",
      "Step 2300 avg train loss = 4.0844\n",
      "Step 2400 avg train loss = 4.0828\n",
      "Validation loss after 67 epoch = 5.7176\n",
      "Step 0 avg train loss = 3.8647\n",
      "Step 100 avg train loss = 3.9231\n",
      "Step 200 avg train loss = 3.9711\n",
      "Step 300 avg train loss = 3.9748\n",
      "Step 400 avg train loss = 3.9480\n",
      "Step 500 avg train loss = 3.9852\n",
      "Step 600 avg train loss = 3.9817\n",
      "Step 700 avg train loss = 3.9706\n",
      "Step 800 avg train loss = 4.0049\n",
      "Step 900 avg train loss = 3.9866\n",
      "Step 1000 avg train loss = 4.0247\n",
      "Step 1100 avg train loss = 4.1093\n",
      "Step 1200 avg train loss = 4.0704\n",
      "Step 1300 avg train loss = 4.0584\n",
      "Step 1400 avg train loss = 4.0420\n",
      "Step 1500 avg train loss = 4.0424\n",
      "Step 1600 avg train loss = 4.0580\n",
      "Step 1700 avg train loss = 4.0595\n",
      "Step 1800 avg train loss = 4.0865\n",
      "Step 1900 avg train loss = 4.0708\n",
      "Step 2000 avg train loss = 4.0758\n",
      "Step 2100 avg train loss = 4.2985\n",
      "Step 2200 avg train loss = 4.1734\n",
      "Step 2300 avg train loss = 4.1249\n",
      "Step 2400 avg train loss = 4.1011\n",
      "Validation loss after 68 epoch = 5.7266\n",
      "Step 0 avg train loss = 3.8029\n",
      "Step 100 avg train loss = 3.9393\n",
      "Step 200 avg train loss = 3.9252\n",
      "Step 300 avg train loss = 3.9685\n",
      "Step 400 avg train loss = 3.9667\n",
      "Step 500 avg train loss = 3.9783\n",
      "Step 600 avg train loss = 4.0194\n",
      "Step 700 avg train loss = 3.9696\n",
      "Step 800 avg train loss = 4.0095\n",
      "Step 900 avg train loss = 4.0101\n",
      "Step 1000 avg train loss = 3.9969\n",
      "Step 1100 avg train loss = 4.0308\n",
      "Step 1200 avg train loss = 4.0174\n",
      "Step 1300 avg train loss = 4.0452\n",
      "Step 1400 avg train loss = 4.0248\n",
      "Step 1500 avg train loss = 4.0389\n",
      "Step 1600 avg train loss = 4.0353\n",
      "Step 1700 avg train loss = 4.0497\n",
      "Step 1800 avg train loss = 4.0649\n",
      "Step 1900 avg train loss = 4.1309\n",
      "Step 2000 avg train loss = 4.0966\n",
      "Step 2100 avg train loss = 4.0625\n",
      "Step 2200 avg train loss = 4.0428\n",
      "Step 2300 avg train loss = 4.0805\n",
      "Step 2400 avg train loss = 4.0715\n",
      "Validation loss after 69 epoch = 5.7220\n",
      "Step 0 avg train loss = 3.9382\n",
      "Step 100 avg train loss = 3.9178\n",
      "Step 200 avg train loss = 3.9537\n",
      "Step 300 avg train loss = 3.9709\n",
      "Step 400 avg train loss = 3.9706\n",
      "Step 500 avg train loss = 3.9725\n",
      "Step 600 avg train loss = 3.9867\n",
      "Step 700 avg train loss = 3.9826\n",
      "Step 800 avg train loss = 4.0295\n",
      "Step 900 avg train loss = 4.0143\n",
      "Step 1000 avg train loss = 3.9924\n",
      "Step 1100 avg train loss = 4.0238\n",
      "Step 1200 avg train loss = 4.0112\n",
      "Step 1300 avg train loss = 4.0204\n",
      "Step 1400 avg train loss = 4.0144\n",
      "Step 1500 avg train loss = 4.0401\n",
      "Step 1600 avg train loss = 4.0499\n",
      "Step 1700 avg train loss = 4.0862\n",
      "Step 1800 avg train loss = 4.0396\n",
      "Step 1900 avg train loss = 4.0679\n",
      "Step 2000 avg train loss = 4.0434\n",
      "Step 2100 avg train loss = 4.0613\n",
      "Step 2200 avg train loss = 4.0477\n",
      "Step 2300 avg train loss = 4.0856\n",
      "Step 2400 avg train loss = 4.0585\n",
      "Validation loss after 70 epoch = 5.7715\n",
      "Step 0 avg train loss = 4.0239\n",
      "Step 100 avg train loss = 3.9811\n",
      "Step 200 avg train loss = 3.9500\n",
      "Step 300 avg train loss = 3.9413\n",
      "Step 400 avg train loss = 3.9551\n",
      "Step 500 avg train loss = 3.9582\n",
      "Step 600 avg train loss = 3.9649\n",
      "Step 700 avg train loss = 3.9669\n",
      "Step 800 avg train loss = 3.9702\n",
      "Step 900 avg train loss = 3.9969\n",
      "Step 1000 avg train loss = 4.0232\n",
      "Step 1100 avg train loss = 4.0801\n",
      "Step 1200 avg train loss = 3.9977\n",
      "Step 1300 avg train loss = 3.9971\n",
      "Step 1400 avg train loss = 4.0077\n",
      "Step 1500 avg train loss = 4.0140\n",
      "Step 1600 avg train loss = 4.0933\n",
      "Step 1700 avg train loss = 4.1873\n",
      "Step 1800 avg train loss = 4.0815\n",
      "Step 1900 avg train loss = 4.0711\n",
      "Step 2000 avg train loss = 4.0874\n",
      "Step 2100 avg train loss = 4.0839\n",
      "Step 2200 avg train loss = 4.0658\n",
      "Step 2300 avg train loss = 4.0818\n",
      "Step 2400 avg train loss = 4.0791\n",
      "Validation loss after 71 epoch = 5.7340\n",
      "Step 0 avg train loss = 4.0196\n",
      "Step 100 avg train loss = 3.9229\n",
      "Step 200 avg train loss = 3.9635\n",
      "Step 300 avg train loss = 3.9406\n",
      "Step 400 avg train loss = 3.9481\n",
      "Step 500 avg train loss = 3.9656\n",
      "Step 600 avg train loss = 3.9795\n",
      "Step 700 avg train loss = 3.9852\n",
      "Step 800 avg train loss = 3.9922\n",
      "Step 900 avg train loss = 4.0395\n",
      "Step 1000 avg train loss = 4.0962\n",
      "Step 1100 avg train loss = 4.0549\n",
      "Step 1200 avg train loss = 4.0284\n",
      "Step 1300 avg train loss = 4.0146\n",
      "Step 1400 avg train loss = 4.0163\n",
      "Step 1500 avg train loss = 4.0405\n",
      "Step 1600 avg train loss = 4.0450\n",
      "Step 1700 avg train loss = 4.0206\n",
      "Step 1800 avg train loss = 4.0343\n",
      "Step 1900 avg train loss = 4.0681\n",
      "Step 2000 avg train loss = 4.0632\n",
      "Step 2100 avg train loss = 4.0635\n",
      "Step 2200 avg train loss = 4.0253\n",
      "Step 2300 avg train loss = 4.0419\n",
      "Step 2400 avg train loss = 4.0429\n",
      "Validation loss after 72 epoch = 5.7381\n",
      "Step 0 avg train loss = 3.8126\n",
      "Step 100 avg train loss = 3.9073\n",
      "Step 200 avg train loss = 3.9218\n",
      "Step 300 avg train loss = 3.9257\n",
      "Step 400 avg train loss = 3.9657\n",
      "Step 500 avg train loss = 3.9635\n",
      "Step 600 avg train loss = 3.9667\n",
      "Step 700 avg train loss = 3.9639\n",
      "Step 800 avg train loss = 3.9866\n",
      "Step 900 avg train loss = 3.9922\n",
      "Step 1000 avg train loss = 3.9849\n",
      "Step 1100 avg train loss = 4.0000\n",
      "Step 1200 avg train loss = 4.0074\n",
      "Step 1300 avg train loss = 4.0110\n",
      "Step 1400 avg train loss = 4.0511\n",
      "Step 1500 avg train loss = 4.0522\n",
      "Step 1600 avg train loss = 4.0017\n",
      "Step 1700 avg train loss = 4.0327\n",
      "Step 1800 avg train loss = 4.0551\n",
      "Step 1900 avg train loss = 4.0463\n",
      "Step 2000 avg train loss = 4.1376\n",
      "Step 2100 avg train loss = 4.0874\n",
      "Step 2200 avg train loss = 4.2982\n",
      "Step 2300 avg train loss = 4.3099\n",
      "Step 2400 avg train loss = 4.2707\n",
      "Validation loss after 73 epoch = 5.7681\n",
      "Step 0 avg train loss = 4.2644\n",
      "Step 100 avg train loss = 4.1039\n",
      "Step 200 avg train loss = 4.0939\n",
      "Step 300 avg train loss = 4.0838\n",
      "Step 400 avg train loss = 4.0802\n",
      "Step 500 avg train loss = 4.1109\n",
      "Step 600 avg train loss = 4.1181\n",
      "Step 700 avg train loss = 4.0773\n",
      "Step 800 avg train loss = 4.0992\n",
      "Step 900 avg train loss = 4.1124\n",
      "Step 1000 avg train loss = 4.1130\n",
      "Step 1100 avg train loss = 4.1071\n",
      "Step 1200 avg train loss = 4.0788\n",
      "Step 1300 avg train loss = 4.0848\n",
      "Step 1400 avg train loss = 4.0901\n",
      "Step 1500 avg train loss = 4.1143\n",
      "Step 1600 avg train loss = 4.1103\n",
      "Step 1700 avg train loss = 4.0955\n",
      "Step 1800 avg train loss = 4.1349\n",
      "Step 1900 avg train loss = 4.1049\n",
      "Step 2000 avg train loss = 4.1015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100 avg train loss = 4.1385\n",
      "Step 2200 avg train loss = 4.1485\n",
      "Step 2300 avg train loss = 4.1271\n",
      "Step 2400 avg train loss = 4.1160\n",
      "Validation loss after 74 epoch = 5.7366\n",
      "Step 0 avg train loss = 4.1497\n",
      "Step 100 avg train loss = 3.9577\n",
      "Step 200 avg train loss = 3.9740\n",
      "Step 300 avg train loss = 3.9561\n",
      "Step 400 avg train loss = 3.9715\n",
      "Step 500 avg train loss = 3.9826\n",
      "Step 600 avg train loss = 3.9868\n",
      "Step 700 avg train loss = 3.9898\n",
      "Step 800 avg train loss = 3.9782\n",
      "Step 900 avg train loss = 4.0025\n",
      "Step 1000 avg train loss = 4.0150\n",
      "Step 1100 avg train loss = 4.0044\n",
      "Step 1200 avg train loss = 4.0095\n",
      "Step 1300 avg train loss = 4.0107\n",
      "Step 1400 avg train loss = 4.0138\n",
      "Step 1500 avg train loss = 4.0434\n",
      "Step 1600 avg train loss = 4.0319\n",
      "Step 1700 avg train loss = 4.0335\n",
      "Step 1800 avg train loss = 4.0689\n",
      "Step 1900 avg train loss = 4.1043\n",
      "Step 2000 avg train loss = 4.0698\n",
      "Step 2100 avg train loss = 4.0642\n",
      "Step 2200 avg train loss = 4.0735\n",
      "Step 2300 avg train loss = 4.0669\n",
      "Step 2400 avg train loss = 4.0738\n",
      "Validation loss after 75 epoch = 5.7443\n",
      "Step 0 avg train loss = 4.0619\n",
      "Step 100 avg train loss = 3.9030\n",
      "Step 200 avg train loss = 3.9218\n",
      "Step 300 avg train loss = 3.9577\n",
      "Step 400 avg train loss = 3.9372\n",
      "Step 500 avg train loss = 3.9293\n",
      "Step 600 avg train loss = 3.9535\n",
      "Step 700 avg train loss = 3.9543\n",
      "Step 800 avg train loss = 3.9537\n",
      "Step 900 avg train loss = 3.9800\n",
      "Step 1000 avg train loss = 3.9781\n",
      "Step 1100 avg train loss = 4.0058\n",
      "Step 1200 avg train loss = 4.0107\n",
      "Step 1300 avg train loss = 4.0083\n",
      "Step 1400 avg train loss = 3.9981\n",
      "Step 1500 avg train loss = 3.9897\n",
      "Step 1600 avg train loss = 4.0215\n",
      "Step 1700 avg train loss = 4.0550\n",
      "Step 1800 avg train loss = 4.0352\n",
      "Step 1900 avg train loss = 4.0657\n",
      "Step 2000 avg train loss = 4.0158\n",
      "Step 2100 avg train loss = 4.0512\n",
      "Step 2200 avg train loss = 4.0993\n",
      "Step 2300 avg train loss = 4.0945\n",
      "Step 2400 avg train loss = 4.0800\n",
      "Validation loss after 76 epoch = 5.7568\n",
      "Step 0 avg train loss = 3.9164\n",
      "Step 100 avg train loss = 3.9063\n",
      "Step 200 avg train loss = 3.9198\n",
      "Step 300 avg train loss = 3.9263\n",
      "Step 400 avg train loss = 3.9216\n",
      "Step 500 avg train loss = 3.9401\n",
      "Step 600 avg train loss = 3.9397\n",
      "Step 700 avg train loss = 3.9583\n",
      "Step 800 avg train loss = 3.9787\n",
      "Step 900 avg train loss = 3.9624\n",
      "Step 1000 avg train loss = 4.0031\n",
      "Step 1100 avg train loss = 4.0126\n",
      "Step 1200 avg train loss = 4.0413\n",
      "Step 1300 avg train loss = 4.1194\n",
      "Step 1400 avg train loss = 4.1237\n",
      "Step 1500 avg train loss = 4.0789\n",
      "Step 1600 avg train loss = 4.0196\n",
      "Step 1700 avg train loss = 4.0244\n",
      "Step 1800 avg train loss = 4.0234\n",
      "Step 1900 avg train loss = 4.0365\n",
      "Step 2000 avg train loss = 4.0601\n",
      "Step 2100 avg train loss = 4.0650\n",
      "Step 2200 avg train loss = 4.0565\n",
      "Step 2300 avg train loss = 4.0351\n",
      "Step 2400 avg train loss = 4.0991\n",
      "Validation loss after 77 epoch = 5.7565\n",
      "Step 0 avg train loss = 3.7978\n",
      "Step 100 avg train loss = 3.9287\n",
      "Step 200 avg train loss = 3.9076\n",
      "Step 300 avg train loss = 3.9279\n",
      "Step 400 avg train loss = 3.9281\n",
      "Step 500 avg train loss = 3.9298\n",
      "Step 600 avg train loss = 3.9586\n",
      "Step 700 avg train loss = 3.9677\n",
      "Step 800 avg train loss = 3.9729\n",
      "Step 900 avg train loss = 3.9692\n",
      "Step 1000 avg train loss = 3.9653\n",
      "Step 1100 avg train loss = 3.9831\n",
      "Step 1200 avg train loss = 4.0007\n",
      "Step 1300 avg train loss = 3.9837\n",
      "Step 1400 avg train loss = 3.9985\n",
      "Step 1500 avg train loss = 4.0042\n",
      "Step 1600 avg train loss = 4.0167\n",
      "Step 1700 avg train loss = 3.9977\n",
      "Step 1800 avg train loss = 4.0672\n",
      "Step 1900 avg train loss = 4.0499\n",
      "Step 2000 avg train loss = 4.0330\n",
      "Step 2100 avg train loss = 4.0422\n",
      "Step 2200 avg train loss = 4.0258\n",
      "Step 2300 avg train loss = 4.0031\n",
      "Step 2400 avg train loss = 4.0439\n",
      "Validation loss after 78 epoch = 5.7664\n",
      "Step 0 avg train loss = 3.8308\n",
      "Step 100 avg train loss = 3.8898\n",
      "Step 200 avg train loss = 3.9123\n",
      "Step 300 avg train loss = 3.8996\n",
      "Step 400 avg train loss = 3.9211\n",
      "Step 500 avg train loss = 3.9350\n",
      "Step 600 avg train loss = 3.9874\n",
      "Step 700 avg train loss = 3.9664\n",
      "Step 800 avg train loss = 3.9786\n",
      "Step 900 avg train loss = 3.9877\n",
      "Step 1000 avg train loss = 3.9589\n",
      "Step 1100 avg train loss = 3.9835\n",
      "Step 1200 avg train loss = 3.9848\n",
      "Step 1300 avg train loss = 3.9702\n",
      "Step 1400 avg train loss = 4.0240\n",
      "Step 1500 avg train loss = 4.0019\n",
      "Step 1600 avg train loss = 4.0390\n",
      "Step 1700 avg train loss = 4.0419\n",
      "Step 1800 avg train loss = 4.0242\n",
      "Step 1900 avg train loss = 4.0310\n",
      "Step 2000 avg train loss = 4.0203\n",
      "Step 2100 avg train loss = 4.0594\n",
      "Step 2200 avg train loss = 4.0477\n",
      "Step 2300 avg train loss = 4.0388\n",
      "Step 2400 avg train loss = 4.1867\n",
      "Validation loss after 79 epoch = 5.7898\n",
      "Step 0 avg train loss = 3.8662\n",
      "Step 100 avg train loss = 3.9586\n",
      "Step 200 avg train loss = 3.9321\n",
      "Step 300 avg train loss = 3.9565\n",
      "Step 400 avg train loss = 3.9498\n",
      "Step 500 avg train loss = 3.9447\n",
      "Step 600 avg train loss = 3.9595\n",
      "Step 700 avg train loss = 3.9527\n",
      "Step 800 avg train loss = 3.9767\n",
      "Step 900 avg train loss = 3.9661\n",
      "Step 1000 avg train loss = 3.9606\n",
      "Step 1100 avg train loss = 3.9727\n",
      "Step 1200 avg train loss = 3.9865\n",
      "Step 1300 avg train loss = 3.9972\n",
      "Step 1400 avg train loss = 4.0125\n",
      "Step 1500 avg train loss = 3.9852\n",
      "Step 1600 avg train loss = 3.9901\n",
      "Step 1700 avg train loss = 3.9772\n",
      "Step 1800 avg train loss = 3.9985\n",
      "Step 1900 avg train loss = 4.0043\n",
      "Step 2000 avg train loss = 4.0232\n",
      "Step 2100 avg train loss = 4.0124\n",
      "Step 2200 avg train loss = 4.0155\n",
      "Step 2300 avg train loss = 4.0215\n",
      "Step 2400 avg train loss = 4.0491\n",
      "Validation loss after 80 epoch = 5.7782\n",
      "Step 0 avg train loss = 3.8804\n",
      "Step 100 avg train loss = 3.8731\n",
      "Step 200 avg train loss = 3.8795\n",
      "Step 300 avg train loss = 3.8953\n",
      "Step 400 avg train loss = 3.9307\n",
      "Step 500 avg train loss = 3.9365\n",
      "Step 600 avg train loss = 3.9242\n",
      "Step 700 avg train loss = 3.9578\n",
      "Step 800 avg train loss = 3.9803\n",
      "Step 900 avg train loss = 4.0257\n",
      "Step 1000 avg train loss = 4.0587\n",
      "Step 1100 avg train loss = 3.9962\n",
      "Step 1200 avg train loss = 4.0004\n",
      "Step 1300 avg train loss = 4.0049\n",
      "Step 1400 avg train loss = 4.0031\n",
      "Step 1500 avg train loss = 3.9960\n",
      "Step 1600 avg train loss = 4.0041\n",
      "Step 1700 avg train loss = 4.0382\n",
      "Step 1800 avg train loss = 4.0946\n",
      "Step 1900 avg train loss = 4.0713\n",
      "Step 2000 avg train loss = 4.0494\n",
      "Step 2100 avg train loss = 4.0355\n",
      "Step 2200 avg train loss = 4.0420\n",
      "Step 2300 avg train loss = 4.0420\n",
      "Step 2400 avg train loss = 4.0546\n",
      "Validation loss after 81 epoch = 5.7845\n",
      "Step 0 avg train loss = 3.8793\n",
      "Step 100 avg train loss = 3.8983\n",
      "Step 200 avg train loss = 3.9011\n",
      "Step 300 avg train loss = 3.9472\n",
      "Step 400 avg train loss = 3.9118\n",
      "Step 500 avg train loss = 3.9095\n",
      "Step 600 avg train loss = 3.9293\n",
      "Step 700 avg train loss = 3.9386\n",
      "Step 800 avg train loss = 3.9409\n",
      "Step 900 avg train loss = 4.0219\n",
      "Step 1000 avg train loss = 4.0235\n",
      "Step 1100 avg train loss = 3.9809\n",
      "Step 1200 avg train loss = 3.9787\n",
      "Step 1300 avg train loss = 4.2319\n",
      "Step 1400 avg train loss = 4.2108\n",
      "Step 1500 avg train loss = 4.1458\n",
      "Step 1600 avg train loss = 4.0972\n",
      "Step 1700 avg train loss = 4.1064\n",
      "Step 1800 avg train loss = 4.0830\n",
      "Step 1900 avg train loss = 4.0942\n",
      "Step 2000 avg train loss = 4.0825\n",
      "Step 2100 avg train loss = 4.1856\n",
      "Step 2200 avg train loss = 4.1189\n",
      "Step 2300 avg train loss = 4.1051\n",
      "Step 2400 avg train loss = 4.1026\n",
      "Validation loss after 82 epoch = 5.7869\n",
      "Step 0 avg train loss = 3.7904\n",
      "Step 100 avg train loss = 3.9511\n",
      "Step 200 avg train loss = 3.9361\n",
      "Step 300 avg train loss = 3.9303\n",
      "Step 400 avg train loss = 3.9268\n",
      "Step 500 avg train loss = 3.9366\n",
      "Step 600 avg train loss = 3.9543\n",
      "Step 700 avg train loss = 3.9877\n",
      "Step 800 avg train loss = 3.9578\n",
      "Step 900 avg train loss = 3.9523\n",
      "Step 1000 avg train loss = 3.9655\n",
      "Step 1100 avg train loss = 3.9909\n",
      "Step 1200 avg train loss = 3.9871\n",
      "Step 1300 avg train loss = 3.9800\n",
      "Step 1400 avg train loss = 3.9778\n",
      "Step 1500 avg train loss = 4.0122\n",
      "Step 1600 avg train loss = 4.0252\n",
      "Step 1700 avg train loss = 4.0061\n",
      "Step 1800 avg train loss = 3.9956\n",
      "Step 1900 avg train loss = 3.9842\n",
      "Step 2000 avg train loss = 3.9934\n",
      "Step 2100 avg train loss = 4.1367\n",
      "Step 2200 avg train loss = 4.2002\n",
      "Step 2300 avg train loss = 4.1300\n",
      "Step 2400 avg train loss = 4.0885\n",
      "Validation loss after 83 epoch = 5.8014\n",
      "Step 0 avg train loss = 3.9360\n",
      "Step 100 avg train loss = 3.9099\n",
      "Step 200 avg train loss = 3.9309\n",
      "Step 300 avg train loss = 3.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 avg train loss = 3.9435\n",
      "Step 500 avg train loss = 3.9372\n",
      "Step 600 avg train loss = 3.9476\n",
      "Step 700 avg train loss = 3.9728\n",
      "Step 800 avg train loss = 4.0116\n",
      "Step 900 avg train loss = 3.9770\n",
      "Step 1000 avg train loss = 3.9781\n",
      "Step 1100 avg train loss = 4.0027\n",
      "Step 1200 avg train loss = 3.9939\n",
      "Step 1300 avg train loss = 3.9996\n",
      "Step 1400 avg train loss = 3.9796\n",
      "Step 1500 avg train loss = 3.9921\n",
      "Step 1600 avg train loss = 3.9936\n",
      "Step 1700 avg train loss = 4.0244\n",
      "Step 1800 avg train loss = 4.1160\n",
      "Step 1900 avg train loss = 4.0969\n",
      "Step 2000 avg train loss = 4.0622\n",
      "Step 2100 avg train loss = 4.0452\n",
      "Step 2200 avg train loss = 4.0494\n",
      "Step 2300 avg train loss = 4.0328\n",
      "Step 2400 avg train loss = 4.0572\n",
      "Validation loss after 84 epoch = 5.7912\n",
      "Step 0 avg train loss = 3.9705\n",
      "Step 100 avg train loss = 4.1065\n",
      "Step 200 avg train loss = 4.0305\n",
      "Step 300 avg train loss = 3.9746\n",
      "Step 400 avg train loss = 4.0404\n",
      "Step 500 avg train loss = 3.9767\n",
      "Step 600 avg train loss = 3.9731\n",
      "Step 700 avg train loss = 3.9574\n",
      "Step 800 avg train loss = 3.9934\n",
      "Step 900 avg train loss = 3.9741\n",
      "Step 1000 avg train loss = 3.9621\n",
      "Step 1100 avg train loss = 3.9667\n",
      "Step 1200 avg train loss = 3.9778\n",
      "Step 1300 avg train loss = 3.9716\n",
      "Step 1400 avg train loss = 3.9783\n",
      "Step 1500 avg train loss = 4.0137\n",
      "Step 1600 avg train loss = 4.0103\n",
      "Step 1700 avg train loss = 4.0005\n",
      "Step 1800 avg train loss = 4.1136\n",
      "Step 1900 avg train loss = 4.2470\n",
      "Step 2000 avg train loss = 4.1144\n",
      "Step 2100 avg train loss = 4.0768\n",
      "Step 2200 avg train loss = 4.0731\n",
      "Step 2300 avg train loss = 4.0716\n",
      "Step 2400 avg train loss = 4.0398\n",
      "Validation loss after 85 epoch = 5.8048\n",
      "Step 0 avg train loss = 3.9508\n",
      "Step 100 avg train loss = 3.8982\n",
      "Step 200 avg train loss = 3.9032\n",
      "Step 300 avg train loss = 3.9030\n",
      "Step 400 avg train loss = 3.9204\n",
      "Step 500 avg train loss = 3.9398\n",
      "Step 600 avg train loss = 3.9304\n",
      "Step 700 avg train loss = 3.9260\n",
      "Step 800 avg train loss = 3.9441\n",
      "Step 900 avg train loss = 3.9559\n",
      "Step 1000 avg train loss = 3.9792\n",
      "Step 1100 avg train loss = 3.9691\n",
      "Step 1200 avg train loss = 3.9875\n",
      "Step 1300 avg train loss = 3.9691\n",
      "Step 1400 avg train loss = 3.9748\n",
      "Step 1500 avg train loss = 3.9962\n",
      "Step 1600 avg train loss = 3.9799\n",
      "Step 1700 avg train loss = 3.9809\n",
      "Step 1800 avg train loss = 3.9978\n",
      "Step 1900 avg train loss = 3.9878\n",
      "Step 2000 avg train loss = 4.0156\n",
      "Step 2100 avg train loss = 3.9864\n",
      "Step 2200 avg train loss = 4.0119\n",
      "Step 2300 avg train loss = 3.9984\n",
      "Step 2400 avg train loss = 3.9938\n",
      "Validation loss after 86 epoch = 5.7990\n",
      "Step 0 avg train loss = 3.7480\n",
      "Step 100 avg train loss = 3.8742\n",
      "Step 200 avg train loss = 3.8964\n",
      "Step 300 avg train loss = 3.8991\n",
      "Step 400 avg train loss = 3.8947\n",
      "Step 500 avg train loss = 3.8895\n",
      "Step 600 avg train loss = 3.8973\n",
      "Step 700 avg train loss = 3.9176\n",
      "Step 800 avg train loss = 3.9467\n",
      "Step 900 avg train loss = 3.9618\n",
      "Step 1000 avg train loss = 3.9553\n",
      "Step 1100 avg train loss = 3.9529\n",
      "Step 1200 avg train loss = 3.9640\n",
      "Step 1300 avg train loss = 3.9478\n",
      "Step 1400 avg train loss = 3.9937\n",
      "Step 1500 avg train loss = 4.0847\n",
      "Step 1600 avg train loss = 4.1226\n",
      "Step 1700 avg train loss = 4.0839\n",
      "Step 1800 avg train loss = 4.0372\n",
      "Step 1900 avg train loss = 4.0083\n",
      "Step 2000 avg train loss = 4.1415\n",
      "Step 2100 avg train loss = 4.1850\n",
      "Step 2200 avg train loss = 4.1540\n",
      "Step 2300 avg train loss = 4.1395\n",
      "Step 2400 avg train loss = 4.1303\n",
      "Validation loss after 87 epoch = 5.7958\n",
      "Step 0 avg train loss = 3.9785\n",
      "Step 100 avg train loss = 3.9685\n",
      "Step 200 avg train loss = 3.9682\n",
      "Step 300 avg train loss = 4.4036\n",
      "Step 400 avg train loss = 4.8048\n",
      "Step 500 avg train loss = 4.5508\n",
      "Step 600 avg train loss = 4.4581\n",
      "Step 700 avg train loss = 4.3932\n",
      "Step 800 avg train loss = 4.3407\n",
      "Step 900 avg train loss = 4.3274\n",
      "Step 1000 avg train loss = 4.3098\n",
      "Step 1100 avg train loss = 4.2825\n",
      "Step 1200 avg train loss = 4.2538\n",
      "Step 1300 avg train loss = 4.2568\n",
      "Step 1400 avg train loss = 4.2321\n",
      "Step 1500 avg train loss = 4.2161\n",
      "Step 1600 avg train loss = 4.1989\n",
      "Step 1700 avg train loss = 4.1992\n",
      "Step 1800 avg train loss = 4.2014\n",
      "Step 1900 avg train loss = 4.1820\n",
      "Step 2000 avg train loss = 4.1561\n",
      "Step 2100 avg train loss = 4.1804\n",
      "Step 2200 avg train loss = 4.1756\n",
      "Step 2300 avg train loss = 4.1572\n",
      "Step 2400 avg train loss = 4.1693\n",
      "Validation loss after 88 epoch = 5.7929\n",
      "Step 0 avg train loss = 3.8177\n",
      "Step 100 avg train loss = 3.9801\n",
      "Step 200 avg train loss = 3.9759\n",
      "Step 300 avg train loss = 3.9870\n",
      "Step 400 avg train loss = 3.9777\n",
      "Step 500 avg train loss = 4.0056\n",
      "Step 600 avg train loss = 4.0116\n",
      "Step 700 avg train loss = 4.0084\n",
      "Step 800 avg train loss = 4.0221\n",
      "Step 900 avg train loss = 4.0037\n",
      "Step 1000 avg train loss = 4.0127\n",
      "Step 1100 avg train loss = 4.0181\n",
      "Step 1200 avg train loss = 4.0584\n",
      "Step 1300 avg train loss = 4.0371\n",
      "Step 1400 avg train loss = 4.0368\n",
      "Step 1500 avg train loss = 4.0331\n",
      "Step 1600 avg train loss = 4.0635\n",
      "Step 1700 avg train loss = 4.0274\n",
      "Step 1800 avg train loss = 4.0639\n",
      "Step 1900 avg train loss = 4.0385\n",
      "Step 2000 avg train loss = 4.0291\n",
      "Step 2100 avg train loss = 4.0580\n",
      "Step 2200 avg train loss = 4.1379\n",
      "Step 2300 avg train loss = 4.1103\n",
      "Step 2400 avg train loss = 4.0616\n",
      "Validation loss after 89 epoch = 5.7918\n",
      "Step 0 avg train loss = 3.9189\n",
      "Step 100 avg train loss = 3.9270\n",
      "Step 200 avg train loss = 3.9260\n",
      "Step 300 avg train loss = 3.9130\n",
      "Step 400 avg train loss = 3.9268\n",
      "Step 500 avg train loss = 3.9346\n",
      "Step 600 avg train loss = 3.9667\n",
      "Step 700 avg train loss = 3.9592\n",
      "Step 800 avg train loss = 3.9463\n",
      "Step 900 avg train loss = 3.9755\n",
      "Step 1000 avg train loss = 3.9824\n",
      "Step 1100 avg train loss = 3.9813\n",
      "Step 1200 avg train loss = 3.9733\n",
      "Step 1300 avg train loss = 3.9951\n",
      "Step 1400 avg train loss = 4.0018\n",
      "Step 1500 avg train loss = 4.0278\n",
      "Step 1600 avg train loss = 4.0244\n",
      "Step 1700 avg train loss = 4.0111\n",
      "Step 1800 avg train loss = 4.0032\n",
      "Step 1900 avg train loss = 4.0091\n",
      "Step 2000 avg train loss = 4.0227\n",
      "Step 2100 avg train loss = 3.9923\n",
      "Step 2200 avg train loss = 4.0045\n",
      "Step 2300 avg train loss = 4.0024\n",
      "Step 2400 avg train loss = 4.0005\n",
      "Validation loss after 90 epoch = 5.7911\n",
      "Step 0 avg train loss = 3.9911\n",
      "Step 100 avg train loss = 3.8651\n",
      "Step 200 avg train loss = 3.8795\n",
      "Step 300 avg train loss = 3.9043\n",
      "Step 400 avg train loss = 3.9375\n",
      "Step 500 avg train loss = 3.9250\n",
      "Step 600 avg train loss = 3.8886\n",
      "Step 700 avg train loss = 3.9428\n",
      "Step 800 avg train loss = 3.9374\n",
      "Step 900 avg train loss = 3.9385\n",
      "Step 1000 avg train loss = 3.9291\n",
      "Step 1100 avg train loss = 3.9732\n",
      "Step 1200 avg train loss = 3.9448\n",
      "Step 1300 avg train loss = 3.9535\n",
      "Step 1400 avg train loss = 4.0215\n",
      "Step 1500 avg train loss = 3.9895\n",
      "Step 1600 avg train loss = 4.0450\n",
      "Step 1700 avg train loss = 4.0309\n",
      "Step 1800 avg train loss = 4.0003\n",
      "Step 1900 avg train loss = 4.1201\n",
      "Step 2000 avg train loss = 4.0365\n",
      "Step 2100 avg train loss = 4.0309\n",
      "Step 2200 avg train loss = 4.0411\n",
      "Step 2300 avg train loss = 4.0823\n",
      "Step 2400 avg train loss = 4.6118\n",
      "Validation loss after 91 epoch = 5.8585\n",
      "Step 0 avg train loss = 4.0995\n",
      "Step 100 avg train loss = 4.0947\n",
      "Step 200 avg train loss = 4.0355\n",
      "Step 300 avg train loss = 4.0126\n",
      "Step 400 avg train loss = 3.9795\n",
      "Step 500 avg train loss = 3.9879\n",
      "Step 600 avg train loss = 3.9808\n",
      "Step 700 avg train loss = 3.9778\n",
      "Step 800 avg train loss = 3.9872\n",
      "Step 900 avg train loss = 3.9612\n",
      "Step 1000 avg train loss = 3.9599\n",
      "Step 1100 avg train loss = 3.9755\n",
      "Step 1200 avg train loss = 3.9302\n",
      "Step 1300 avg train loss = 3.9824\n",
      "Step 1400 avg train loss = 3.9661\n",
      "Step 1500 avg train loss = 3.9619\n",
      "Step 1600 avg train loss = 3.9993\n",
      "Step 1700 avg train loss = 3.9657\n",
      "Step 1800 avg train loss = 3.9693\n",
      "Step 1900 avg train loss = 4.0126\n",
      "Step 2000 avg train loss = 3.9917\n",
      "Step 2100 avg train loss = 3.9816\n",
      "Step 2200 avg train loss = 4.0228\n",
      "Step 2300 avg train loss = 4.0146\n",
      "Step 2400 avg train loss = 4.0020\n",
      "Validation loss after 92 epoch = 5.8003\n",
      "Step 0 avg train loss = 3.7245\n",
      "Step 100 avg train loss = 3.8412\n",
      "Step 200 avg train loss = 3.8521\n",
      "Step 300 avg train loss = 3.8757\n",
      "Step 400 avg train loss = 3.8935\n",
      "Step 500 avg train loss = 3.8924\n",
      "Step 600 avg train loss = 3.9022\n",
      "Step 700 avg train loss = 3.9126\n",
      "Step 800 avg train loss = 4.0324\n",
      "Step 900 avg train loss = 4.0304\n",
      "Step 1000 avg train loss = 3.9671\n",
      "Step 1100 avg train loss = 3.9494\n",
      "Step 1200 avg train loss = 3.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300 avg train loss = 3.9685\n",
      "Step 1400 avg train loss = 3.9683\n",
      "Step 1500 avg train loss = 3.9703\n",
      "Step 1600 avg train loss = 3.9635\n",
      "Step 1700 avg train loss = 3.9932\n",
      "Step 1800 avg train loss = 3.9561\n",
      "Step 1900 avg train loss = 3.9865\n",
      "Step 2000 avg train loss = 4.0964\n",
      "Step 2100 avg train loss = 4.0728\n",
      "Step 2200 avg train loss = 4.0731\n",
      "Step 2300 avg train loss = 4.0452\n",
      "Step 2400 avg train loss = 4.0327\n",
      "Validation loss after 93 epoch = 5.8164\n",
      "Step 0 avg train loss = 3.8871\n",
      "Step 100 avg train loss = 3.8646\n",
      "Step 200 avg train loss = 3.9034\n",
      "Step 300 avg train loss = 3.8951\n",
      "Step 400 avg train loss = 3.8991\n",
      "Step 500 avg train loss = 3.9278\n",
      "Step 600 avg train loss = 3.8884\n",
      "Step 700 avg train loss = 3.9233\n",
      "Step 800 avg train loss = 3.9258\n",
      "Step 900 avg train loss = 3.9142\n",
      "Step 1000 avg train loss = 3.9456\n",
      "Step 1100 avg train loss = 3.9310\n",
      "Step 1200 avg train loss = 3.9451\n",
      "Step 1300 avg train loss = 3.9461\n",
      "Step 1400 avg train loss = 3.9364\n",
      "Step 1500 avg train loss = 3.9531\n",
      "Step 1600 avg train loss = 3.9338\n",
      "Step 1700 avg train loss = 3.9525\n",
      "Step 1800 avg train loss = 3.9705\n",
      "Step 1900 avg train loss = 3.9694\n",
      "Step 2000 avg train loss = 3.9926\n",
      "Step 2100 avg train loss = 3.9647\n",
      "Step 2200 avg train loss = 3.9758\n",
      "Step 2300 avg train loss = 3.9931\n",
      "Step 2400 avg train loss = 4.0071\n",
      "Validation loss after 94 epoch = 5.8152\n",
      "Step 0 avg train loss = 3.7789\n",
      "Step 100 avg train loss = 3.8429\n",
      "Step 200 avg train loss = 3.8422\n",
      "Step 300 avg train loss = 3.8712\n",
      "Step 400 avg train loss = 3.8737\n",
      "Step 500 avg train loss = 3.9000\n",
      "Step 600 avg train loss = 3.9003\n",
      "Step 700 avg train loss = 3.9140\n",
      "Step 800 avg train loss = 3.9000\n",
      "Step 900 avg train loss = 3.9333\n",
      "Step 1000 avg train loss = 3.9503\n",
      "Step 1100 avg train loss = 3.9512\n",
      "Step 1200 avg train loss = 3.9347\n",
      "Step 1300 avg train loss = 3.9612\n",
      "Step 1400 avg train loss = 3.9628\n",
      "Step 1500 avg train loss = 3.9478\n",
      "Step 1600 avg train loss = 3.9638\n",
      "Step 1700 avg train loss = 3.9561\n",
      "Step 1800 avg train loss = 3.9435\n",
      "Step 1900 avg train loss = 3.9867\n",
      "Step 2000 avg train loss = 3.9679\n",
      "Step 2100 avg train loss = 3.9930\n",
      "Step 2200 avg train loss = 3.9804\n",
      "Step 2300 avg train loss = 3.9783\n",
      "Step 2400 avg train loss = 3.9836\n",
      "Validation loss after 95 epoch = 5.8331\n",
      "Step 0 avg train loss = 3.7596\n",
      "Step 100 avg train loss = 3.8658\n",
      "Step 200 avg train loss = 3.8723\n",
      "Step 300 avg train loss = 3.8510\n",
      "Step 400 avg train loss = 3.8869\n",
      "Step 500 avg train loss = 3.9135\n",
      "Step 600 avg train loss = 3.9042\n",
      "Step 700 avg train loss = 3.9262\n",
      "Step 800 avg train loss = 3.9684\n",
      "Step 900 avg train loss = 3.9295\n",
      "Step 1000 avg train loss = 3.9405\n",
      "Step 1100 avg train loss = 3.9567\n",
      "Step 1200 avg train loss = 3.9230\n",
      "Step 1300 avg train loss = 3.9492\n",
      "Step 1400 avg train loss = 3.9925\n",
      "Step 1500 avg train loss = 4.0308\n",
      "Step 1600 avg train loss = 4.0050\n",
      "Step 1700 avg train loss = 3.9953\n",
      "Step 1800 avg train loss = 3.9924\n",
      "Step 1900 avg train loss = 3.9931\n",
      "Step 2000 avg train loss = 3.9666\n",
      "Step 2100 avg train loss = 4.0013\n",
      "Step 2200 avg train loss = 3.9782\n",
      "Step 2300 avg train loss = 3.9699\n",
      "Step 2400 avg train loss = 3.9826\n",
      "Validation loss after 96 epoch = 5.8230\n",
      "Step 0 avg train loss = 3.7751\n",
      "Step 100 avg train loss = 3.8277\n",
      "Step 200 avg train loss = 3.9647\n",
      "Step 300 avg train loss = 3.9208\n",
      "Step 400 avg train loss = 3.9005\n",
      "Step 500 avg train loss = 3.9123\n",
      "Step 600 avg train loss = 3.9026\n",
      "Step 700 avg train loss = 3.9298\n",
      "Step 800 avg train loss = 3.9145\n",
      "Step 900 avg train loss = 3.9134\n",
      "Step 1000 avg train loss = 3.9207\n",
      "Step 1100 avg train loss = 3.9325\n",
      "Step 1200 avg train loss = 3.9446\n",
      "Step 1300 avg train loss = 3.9446\n",
      "Step 1400 avg train loss = 3.9595\n",
      "Step 1500 avg train loss = 3.9453\n",
      "Step 1600 avg train loss = 3.9497\n",
      "Step 1700 avg train loss = 3.9530\n",
      "Step 1800 avg train loss = 3.9649\n",
      "Step 1900 avg train loss = 3.9994\n",
      "Step 2000 avg train loss = 3.9833\n",
      "Step 2100 avg train loss = 3.9806\n",
      "Step 2200 avg train loss = 4.0320\n",
      "Step 2300 avg train loss = 4.0067\n",
      "Step 2400 avg train loss = 3.9760\n",
      "Validation loss after 97 epoch = 5.8338\n",
      "Step 0 avg train loss = 3.7575\n",
      "Step 100 avg train loss = 3.8708\n",
      "Step 200 avg train loss = 3.8774\n",
      "Step 300 avg train loss = 3.8882\n",
      "Step 400 avg train loss = 3.8786\n",
      "Step 500 avg train loss = 3.8911\n",
      "Step 600 avg train loss = 3.9043\n",
      "Step 700 avg train loss = 3.8967\n",
      "Step 800 avg train loss = 3.9000\n",
      "Step 900 avg train loss = 3.9634\n",
      "Step 1000 avg train loss = 4.0202\n",
      "Step 1100 avg train loss = 3.9701\n",
      "Step 1200 avg train loss = 3.9531\n",
      "Step 1300 avg train loss = 3.9229\n",
      "Step 1400 avg train loss = 3.9276\n",
      "Step 1500 avg train loss = 4.0138\n",
      "Step 1600 avg train loss = 4.0422\n",
      "Step 1700 avg train loss = 3.9830\n",
      "Step 1800 avg train loss = 3.9551\n",
      "Step 1900 avg train loss = 3.9661\n",
      "Step 2000 avg train loss = 3.9453\n",
      "Step 2100 avg train loss = 4.0017\n",
      "Step 2200 avg train loss = 3.9676\n",
      "Step 2300 avg train loss = 3.9793\n",
      "Step 2400 avg train loss = 3.9783\n",
      "Validation loss after 98 epoch = 5.8484\n",
      "Step 0 avg train loss = 3.9654\n",
      "Step 100 avg train loss = 3.8389\n",
      "Step 200 avg train loss = 3.8601\n",
      "Step 300 avg train loss = 3.9263\n",
      "Step 400 avg train loss = 3.9869\n",
      "Step 500 avg train loss = 3.9456\n",
      "Step 600 avg train loss = 3.9219\n",
      "Step 700 avg train loss = 3.9276\n",
      "Step 800 avg train loss = 3.9200\n",
      "Step 900 avg train loss = 3.9347\n",
      "Step 1000 avg train loss = 3.9463\n",
      "Step 1100 avg train loss = 3.9264\n",
      "Step 1200 avg train loss = 3.9202\n",
      "Step 1300 avg train loss = 3.9311\n",
      "Step 1400 avg train loss = 3.9274\n",
      "Step 1500 avg train loss = 3.9321\n",
      "Step 1600 avg train loss = 3.9355\n",
      "Step 1700 avg train loss = 3.9333\n",
      "Step 1800 avg train loss = 3.9509\n",
      "Step 1900 avg train loss = 3.9727\n",
      "Step 2000 avg train loss = 3.9753\n",
      "Step 2100 avg train loss = 3.9733\n",
      "Step 2200 avg train loss = 3.9897\n",
      "Step 2300 avg train loss = 3.9745\n",
      "Step 2400 avg train loss = 3.9753\n",
      "Validation loss after 99 epoch = 5.8413\n"
     ]
    }
   ],
   "source": [
    "plot_cache = []\n",
    "\n",
    "for epoch_number in range(100):\n",
    "    avg_loss=0\n",
    "    if not load_pretrained:\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        for i, (inp, target) in enumerate(loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_log_cache.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "            \n",
    "    #do valid\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(loaders['valid']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_losses.append(loss.item())\n",
    "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "        \n",
    "    plot_cache.append((avg_loss, avg_val_loss))\n",
    "\n",
    "    if load_pretrained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf8g0J91UFIZYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b930ba26550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves of RNN baseline model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 LSTM and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import LSTM\n",
    "# input_size, hidden_size, num_layers. \n",
    "# Optional: bias = False, dropout = 0 (probability of dropout)\n",
    "# bidirectional = False. \n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "\n",
    "# input: tensor containing feature of the input sequence \n",
    "# shape: seq_len, batch, input_size\n",
    "input = torch.randn(5,3,10)\n",
    "\n",
    "#h0: tensor contain hidden state for t = seq_len\n",
    "# shape: num_layers * num*directions, batch, hidden_size\n",
    "h0 = torch.randn(2,3,20)\n",
    "\n",
    "#c0: tensor contain cell state for t = seq_length\n",
    "# shape: num_layers * num_directions, batch, hidden_size \n",
    "c0 = torch.randn(2,3,20)\n",
    "\n",
    "# output: shape (seq_len, batch, num_direction * hidden size)\n",
    "output, (hn,cn) = rnn(input, (h0,c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the baseline, we will stop the epoch around 20 \n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], \n",
    "                            dropout=options['lstm_dropout'], batch_first=True, bias = options['bias'],\n",
    "                           bidirecitonal = options['bid'])\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "        # project of outputs \n",
    "        # rnn_outputs: tupple with second element being last hidden state. \n",
    "        logits = self.projection(lstm_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = LSTMModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 64\n",
    "    hidden_size = 128 # output of dimension \n",
    "    num_layers = 2\n",
    "    lstm_dropout = 0.1\n",
    "    input_size = lookup.weight.size(1)\n",
    "    vocab_size = lookup.weight.size(0)\n",
    "    \n",
    "    options = {\n",
    "        'num_embeddings': len(train_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': train_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'lstm_dropout': lstm_dropout,\n",
    "        'bias': True \n",
    "        'bid': True \n",
    "    }\n",
    "\n",
    "    \n",
    "    model = LSTMModel(options).to(current_device)\n",
    "\n",
    "# same as previous nn based \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(model_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 avg train loss = 10.4080\n",
      "Step 100 avg train loss = 7.8495\n",
      "Step 200 avg train loss = 7.1896\n",
      "Step 300 avg train loss = 7.1161\n",
      "Step 400 avg train loss = 7.0144\n",
      "Step 500 avg train loss = 6.8632\n",
      "Step 600 avg train loss = 6.7544\n",
      "Step 700 avg train loss = 6.6801\n",
      "Step 800 avg train loss = 6.6221\n",
      "Step 900 avg train loss = 6.5670\n",
      "Step 1000 avg train loss = 6.5000\n",
      "Step 1100 avg train loss = 6.4518\n",
      "Step 1200 avg train loss = 6.3973\n",
      "Step 1300 avg train loss = 6.3725\n",
      "Step 1400 avg train loss = 6.3393\n",
      "Step 1500 avg train loss = 6.3139\n",
      "Step 1600 avg train loss = 6.2876\n",
      "Step 1700 avg train loss = 6.2392\n",
      "Step 1800 avg train loss = 6.1988\n",
      "Step 1900 avg train loss = 6.1548\n",
      "Step 2000 avg train loss = 6.1457\n",
      "Step 2100 avg train loss = 6.1093\n",
      "Step 2200 avg train loss = 6.0843\n",
      "Step 2300 avg train loss = 6.0632\n",
      "Step 2400 avg train loss = 6.0399\n",
      "Validation loss after 0 epoch = 5.8600\n",
      "Step 0 avg train loss = 6.1723\n",
      "Step 100 avg train loss = 5.9746\n",
      "Step 200 avg train loss = 5.9631\n",
      "Step 300 avg train loss = 5.9200\n",
      "Step 400 avg train loss = 5.9007\n",
      "Step 500 avg train loss = 5.9017\n",
      "Step 600 avg train loss = 5.8739\n",
      "Step 700 avg train loss = 5.8698\n",
      "Step 800 avg train loss = 5.8447\n",
      "Step 900 avg train loss = 5.8667\n",
      "Step 1000 avg train loss = 5.8390\n",
      "Step 1100 avg train loss = 5.8245\n",
      "Step 1200 avg train loss = 5.7956\n",
      "Step 1300 avg train loss = 5.7880\n",
      "Step 1400 avg train loss = 5.7774\n",
      "Step 1500 avg train loss = 5.7822\n",
      "Step 1600 avg train loss = 5.7556\n",
      "Step 1700 avg train loss = 5.7420\n",
      "Step 1800 avg train loss = 5.7463\n",
      "Step 1900 avg train loss = 5.7332\n",
      "Step 2000 avg train loss = 5.7182\n",
      "Step 2100 avg train loss = 5.7038\n",
      "Step 2200 avg train loss = 5.6790\n",
      "Step 2300 avg train loss = 5.6844\n",
      "Step 2400 avg train loss = 5.6809\n",
      "Validation loss after 1 epoch = 5.5681\n",
      "Step 0 avg train loss = 5.7839\n",
      "Step 100 avg train loss = 5.5842\n",
      "Step 200 avg train loss = 5.5907\n",
      "Step 300 avg train loss = 5.5644\n",
      "Step 400 avg train loss = 5.5722\n",
      "Step 500 avg train loss = 5.5402\n",
      "Step 600 avg train loss = 5.5512\n",
      "Step 700 avg train loss = 5.5516\n",
      "Step 800 avg train loss = 5.5570\n",
      "Step 900 avg train loss = 5.5446\n",
      "Step 1000 avg train loss = 5.5365\n",
      "Step 1100 avg train loss = 5.5311\n",
      "Step 1200 avg train loss = 5.5311\n",
      "Step 1300 avg train loss = 5.5312\n",
      "Step 1400 avg train loss = 5.5386\n",
      "Step 1500 avg train loss = 5.5169\n",
      "Step 1600 avg train loss = 5.4851\n",
      "Step 1700 avg train loss = 5.5060\n",
      "Step 1800 avg train loss = 5.5135\n",
      "Step 1900 avg train loss = 5.4997\n",
      "Step 2000 avg train loss = 5.4824\n",
      "Step 2100 avg train loss = 5.4821\n",
      "Step 2200 avg train loss = 5.4872\n",
      "Step 2300 avg train loss = 5.4778\n",
      "Step 2400 avg train loss = 5.4620\n",
      "Validation loss after 2 epoch = 5.4344\n",
      "Step 0 avg train loss = 5.4298\n",
      "Step 100 avg train loss = 5.3515\n",
      "Step 200 avg train loss = 5.3480\n",
      "Step 300 avg train loss = 5.3540\n",
      "Step 400 avg train loss = 5.3603\n",
      "Step 500 avg train loss = 5.3602\n",
      "Step 600 avg train loss = 5.3476\n",
      "Step 700 avg train loss = 5.3304\n",
      "Step 800 avg train loss = 5.3402\n",
      "Step 900 avg train loss = 5.3367\n",
      "Step 1000 avg train loss = 5.3488\n",
      "Step 1100 avg train loss = 5.3392\n",
      "Step 1200 avg train loss = 5.3420\n",
      "Step 1300 avg train loss = 5.3170\n",
      "Step 1400 avg train loss = 5.3223\n",
      "Step 1500 avg train loss = 5.3267\n",
      "Step 1600 avg train loss = 5.3292\n",
      "Step 1700 avg train loss = 5.3281\n",
      "Step 1800 avg train loss = 5.3381\n",
      "Step 1900 avg train loss = 5.3253\n",
      "Step 2000 avg train loss = 5.3112\n",
      "Step 2100 avg train loss = 5.3289\n",
      "Step 2200 avg train loss = 5.3150\n",
      "Step 2300 avg train loss = 5.3203\n",
      "Step 2400 avg train loss = 5.2907\n",
      "Validation loss after 3 epoch = 5.3630\n",
      "Step 0 avg train loss = 5.1616\n",
      "Step 100 avg train loss = 5.1872\n",
      "Step 200 avg train loss = 5.1742\n",
      "Step 300 avg train loss = 5.1976\n",
      "Step 400 avg train loss = 5.1972\n",
      "Step 500 avg train loss = 5.1929\n",
      "Step 600 avg train loss = 5.1973\n",
      "Step 700 avg train loss = 5.1835\n",
      "Step 800 avg train loss = 5.1903\n",
      "Step 900 avg train loss = 5.1821\n",
      "Step 1000 avg train loss = 5.1982\n",
      "Step 1100 avg train loss = 5.1840\n",
      "Step 1200 avg train loss = 5.1708\n",
      "Step 1300 avg train loss = 5.2005\n",
      "Step 1400 avg train loss = 5.1874\n",
      "Step 1500 avg train loss = 5.1631\n",
      "Step 1600 avg train loss = 5.1806\n",
      "Step 1700 avg train loss = 5.1623\n",
      "Step 1800 avg train loss = 5.1721\n",
      "Step 1900 avg train loss = 5.1705\n",
      "Step 2000 avg train loss = 5.1598\n",
      "Step 2100 avg train loss = 5.1698\n",
      "Step 2200 avg train loss = 5.1653\n",
      "Step 2300 avg train loss = 5.1776\n",
      "Step 2400 avg train loss = 5.1778\n",
      "Validation loss after 4 epoch = 5.3205\n",
      "Step 0 avg train loss = 4.9352\n",
      "Step 100 avg train loss = 5.0276\n",
      "Step 200 avg train loss = 5.0474\n",
      "Step 300 avg train loss = 5.0541\n",
      "Step 400 avg train loss = 5.0547\n",
      "Step 500 avg train loss = 5.0444\n",
      "Step 600 avg train loss = 5.0656\n",
      "Step 700 avg train loss = 5.0672\n",
      "Step 800 avg train loss = 5.0701\n",
      "Step 900 avg train loss = 5.0652\n",
      "Step 1000 avg train loss = 5.0555\n",
      "Step 1100 avg train loss = 5.0644\n",
      "Step 1200 avg train loss = 5.0557\n",
      "Step 1300 avg train loss = 5.0556\n",
      "Step 1400 avg train loss = 5.0314\n",
      "Step 1500 avg train loss = 5.0477\n",
      "Step 1600 avg train loss = 5.0558\n",
      "Step 1700 avg train loss = 5.0634\n",
      "Step 1800 avg train loss = 5.0721\n",
      "Step 1900 avg train loss = 5.0891\n",
      "Step 2000 avg train loss = 5.0569\n",
      "Step 2100 avg train loss = 5.0637\n",
      "Step 2200 avg train loss = 5.0479\n",
      "Step 2300 avg train loss = 5.0392\n",
      "Step 2400 avg train loss = 5.0656\n",
      "Validation loss after 5 epoch = 5.3002\n",
      "Step 0 avg train loss = 5.1703\n",
      "Step 100 avg train loss = 4.9414\n",
      "Step 200 avg train loss = 4.9457\n",
      "Step 300 avg train loss = 4.9332\n",
      "Step 400 avg train loss = 4.9379\n",
      "Step 500 avg train loss = 4.9509\n",
      "Step 600 avg train loss = 4.9427\n",
      "Step 700 avg train loss = 4.9520\n",
      "Step 800 avg train loss = 4.9483\n",
      "Step 900 avg train loss = 4.9626\n",
      "Step 1000 avg train loss = 4.9482\n",
      "Step 1100 avg train loss = 4.9411\n",
      "Step 1200 avg train loss = 4.9552\n",
      "Step 1300 avg train loss = 4.9676\n",
      "Step 1400 avg train loss = 4.9725\n",
      "Step 1500 avg train loss = 4.9577\n",
      "Step 1600 avg train loss = 4.9600\n",
      "Step 1700 avg train loss = 4.9538\n",
      "Step 1800 avg train loss = 4.9493\n",
      "Step 1900 avg train loss = 4.9497\n",
      "Step 2000 avg train loss = 4.9427\n",
      "Step 2100 avg train loss = 4.9709\n",
      "Step 2200 avg train loss = 4.9656\n",
      "Step 2300 avg train loss = 4.9575\n",
      "Step 2400 avg train loss = 4.9506\n",
      "Validation loss after 6 epoch = 5.2964\n",
      "Step 0 avg train loss = 4.7513\n",
      "Step 100 avg train loss = 4.8324\n",
      "Step 200 avg train loss = 4.8348\n",
      "Step 300 avg train loss = 4.8354\n",
      "Step 400 avg train loss = 4.8659\n",
      "Step 500 avg train loss = 4.8459\n",
      "Step 600 avg train loss = 4.8513\n",
      "Step 700 avg train loss = 4.8679\n",
      "Step 800 avg train loss = 4.8527\n",
      "Step 900 avg train loss = 4.8493\n",
      "Step 1000 avg train loss = 4.8635\n",
      "Step 1100 avg train loss = 4.8531\n",
      "Step 1200 avg train loss = 4.8711\n",
      "Step 1300 avg train loss = 4.8627\n",
      "Step 1400 avg train loss = 4.8858\n",
      "Step 1500 avg train loss = 4.8785\n",
      "Step 1600 avg train loss = 4.8700\n",
      "Step 1700 avg train loss = 4.8764\n",
      "Step 1800 avg train loss = 4.8603\n",
      "Step 1900 avg train loss = 4.8860\n",
      "Step 2000 avg train loss = 4.8623\n",
      "Step 2100 avg train loss = 4.8977\n",
      "Step 2200 avg train loss = 4.8832\n",
      "Step 2300 avg train loss = 4.8714\n",
      "Step 2400 avg train loss = 4.8904\n",
      "Validation loss after 7 epoch = 5.3025\n",
      "Step 0 avg train loss = 4.7414\n",
      "Step 100 avg train loss = 4.7694\n",
      "Step 200 avg train loss = 4.7676\n",
      "Step 300 avg train loss = 4.7764\n",
      "Step 400 avg train loss = 4.7649\n",
      "Step 500 avg train loss = 4.7752\n",
      "Step 600 avg train loss = 4.7776\n",
      "Step 700 avg train loss = 4.7755\n",
      "Step 800 avg train loss = 4.7909\n",
      "Step 900 avg train loss = 4.7801\n",
      "Step 1000 avg train loss = 4.7796\n",
      "Step 1100 avg train loss = 4.8074\n",
      "Step 1200 avg train loss = 4.7843\n",
      "Step 1300 avg train loss = 4.7876\n",
      "Step 1400 avg train loss = 4.8023\n",
      "Step 1500 avg train loss = 4.7880\n",
      "Step 1600 avg train loss = 4.7861\n",
      "Step 1700 avg train loss = 4.8039\n",
      "Step 1800 avg train loss = 4.7766\n",
      "Step 1900 avg train loss = 4.7858\n",
      "Step 2000 avg train loss = 4.8063\n",
      "Step 2100 avg train loss = 4.7993\n",
      "Step 2200 avg train loss = 4.7953\n",
      "Step 2300 avg train loss = 4.7939\n",
      "Step 2400 avg train loss = 4.8104\n",
      "Validation loss after 8 epoch = 5.3084\n",
      "Step 0 avg train loss = 4.7094\n",
      "Step 100 avg train loss = 4.7015\n",
      "Step 200 avg train loss = 4.6849\n",
      "Step 300 avg train loss = 4.7026\n",
      "Step 400 avg train loss = 4.6803\n",
      "Step 500 avg train loss = 4.7187\n",
      "Step 600 avg train loss = 4.7284\n",
      "Step 700 avg train loss = 4.6992\n",
      "Step 800 avg train loss = 4.7193\n",
      "Step 900 avg train loss = 4.6899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 avg train loss = 4.7180\n",
      "Step 1100 avg train loss = 4.7381\n",
      "Step 1200 avg train loss = 4.7272\n",
      "Step 1300 avg train loss = 4.7269\n",
      "Step 1400 avg train loss = 4.7147\n",
      "Step 1500 avg train loss = 4.7149\n",
      "Step 1600 avg train loss = 4.7417\n",
      "Step 1700 avg train loss = 4.7449\n",
      "Step 1800 avg train loss = 4.7374\n",
      "Step 1900 avg train loss = 4.7166\n",
      "Step 2000 avg train loss = 4.7414\n",
      "Step 2100 avg train loss = 4.7318\n",
      "Step 2200 avg train loss = 4.7154\n",
      "Step 2300 avg train loss = 4.7471\n",
      "Step 2400 avg train loss = 4.7565\n",
      "Validation loss after 9 epoch = 5.3166\n"
     ]
    }
   ],
   "source": [
    "plot_cache = []\n",
    "\n",
    "for epoch_number in range(10):\n",
    "    avg_loss=0\n",
    "    if not load_pretrained:\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        for i, (inp, target) in enumerate(loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_log_cache.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "            \n",
    "    #do valid\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(loaders['valid']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_losses.append(loss.item())\n",
    "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "        \n",
    "    plot_cache.append((avg_loss, avg_val_loss))\n",
    "\n",
    "    if load_pretrained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDF+7v8DPqnfvBLNK8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b930ba649b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.array(list(range(len(plot_cache))))\n",
    "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
    "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('PPL curves of LSTM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199.61111627741514"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(2**(i[1]/np.log(2)) for i in plot_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (LSTM vs. Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Variation Based on Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Learned Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
    "\n",
    "Use `!pip install umap-learn` to install UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def umap_plot(weight_matrix, word_ids, words):\n",
    "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
    "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
    "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    to_plot = reduced[word_ids, :]\n",
    "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        current_point = to_plot[i]\n",
    "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAARiCAYAAAAp2gdjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3W+s3md93/HPVTs2HhZxssDBCRsekBnwqBz7LIqGthw3tdxpaMlQtnZCwhFFroIA8WDWjCIt0vZgXs00NrFoymCq205ytIiFbAKlwdsBDaVTbUyTOOCZBNPhWIHSONFJHTUO1x7knmX7e+zj5L5/PvjweknR/e8657qO/H301u93p/XeAwAAAABn+6XFPgAAAAAAP39EIwAAAAAK0QgAAACAQjQCAAAAoBCNAAAAAChEIwAAAAAK0QgAAACAQjQCAAAAoBCNAAAAAChEIwAAAACK5Yt9gIu57rrr+rp16xb7GJznpZdeypvf/ObFPgZLkNliSOaLoZgthmK2GIrZYihm68px8ODBP+29v3WhdT/X0WjdunU5cODAYh+D88zOzmZmZmaxj8ESZLYYkvliKGaLoZgthmK2GIrZunK01n54KevcngYAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAwFh67/nZz3622MdgwkQjAAAA4HU7duxY3ve+9+UTn/hENm3alD179mR6ejobNmzIvffee2bdunXrcu+992bTpk35wAc+kO9973uLeGpeD9EIAAAAeEOOHDmSj370ozl06FDuvvvuHDhwII8//ni+8Y1v5PHHHz+z7rrrrsu3v/3t3H333fnc5z63iCfm9RCNAAAAgDfkne98Z2655ZYkyezsbDZt2pSbbrophw8fzlNPPXVm3Yc//OEkyebNm3Ps2LHFOCpvwPLFPgAAAABwZXjo0PHseeRInj15Ktf2F/LqspVJkh/84Ad54IEH8uSTT+aaa67JXXfdlZdffvnMz61c+dq6ZcuW5fTp04tydl4/VxoBAAAAC3ro0PF89stP5PjJU+lJnnvx5Tz34st56NDxvPjii3nTm96Uq6++Os8991y+9rWvLfZxmQBXGgEAAAAL2vPIkZx65dVz3uu9Z88jR/KtXb+SG2+8MRs2bMi73vWufPCDH1ykUzJJohEAAACwoGdPnjrn9fKrp3L9b9535v1du3ZlZmam/NzZ32E0PT2d2dnZAU/JJLk9DQAAAFjQ9WtWva73ufKJRgAAAMCCdm5bn1VXLTvnvVVXLcvObesX6UQMze1pAAAAwILuuOmGJDnzf0+7fs2q7Ny2/sz7LD2iEQAAAHBJ7rjpBpHoF4jb0wAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoJhKNWmu/1lo70lr7fmtt1zyfr2ytPTD6/H+31tZNYl8AAAAAhjF2NGqtLUvy75P83STvT/KPW2vvP2/ZbyZ5vvf+niT/Jsm/GndfAAAAAIYziSuNbk7y/d77M733v0iyL8nt5625Pcne0fMHk9zWWmsT2BsAAACAAUwiGt2Q5P+e9fpHo/fmXdN7P53khSR/eQJ7AwAAADCA5RP4HfNdMdTfwJrXFra2I8mOJJmamsrs7OxYh2Py5ubm/LswCLPFkMwXQzFbDMVsMRSzxVDM1tIziWj0oyR/5azX70jy7AXW/Ki1tjzJ1Un+bL5f1nu/P8n9STI9Pd1nZmYmcEQmaXZ2Nv5dGILZYkjmi6GYLYZithiK2WIoZmvpmcTtaX+U5MbW2l9rra1I8htJHj5vzcNJto+e35nkf/Te573SCAAAAIDFN/aVRr330621TyZ5JMmyJP+p9364tfbPkxzovT+c5EtJfq+19v28doXRb4y7LwAAAADDmcTtaem9fzXJV89775+d9fzlJP9wEnsBAAAAMLxJ3J4GAAAAwBIjGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABcwMmTJ3PfffclSWZnZ/OhD31okU90+YhGAAAAABdwdjT6RSMaAQAAAFzArl278vTTT2fjxo3ZuXNn5ubmcuedd+a9731vPvKRj6T3niQ5ePBgbr311mzevDnbtm3LiRMnFvnk4xONAAAAAC5g9+7defe7353vfOc72bNnTw4dOpTPf/7zeeqpp/LMM8/kW9/6Vl555ZV86lOfyoMPPpiDBw/mYx/7WO65557FPvrYli/2AQAAAACuFDfffHPe8Y53JEk2btyYY8eOZc2aNXnyySezdevWJMmrr76atWvXLuYxJ0I0AgAAADjPQ4eOZ88jR/LDHx7Ln/3pS3no0PGsSbJy5coza5YtW5bTp0+n954NGzbkscceW7wDD8DtaQAAAABneejQ8Xz2y0/k+MlTaStW5S9OvZTPfvmJ/K+jP5l3/fr16/OTn/zkTDR65ZVXcvjw4ct55EG40ggAAADgLHseOZJTr7yaJFm26i1ZecP78/R/+K3sXrkqMxvfU9avWLEiDz74YD796U/nhRdeyOnTp/OZz3wmGzZsuNxHnyjRCAAAAOAsz548dc7rt/79nUmSluS/7/57Z97/whe+cOb5xo0b881vfvOynO9ycXsaAAAAwFmuX7Pqdb2/VIlGAAAAAGfZuW19Vl217Jz3Vl21LDu3rV+kEy0Ot6cBAAAAnOWOm25I8tp3Gz178lSuX7MqO7etP/P+LwrRCAAAAOA8d9x0wy9cJDqf29MAAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAIqxolFr7drW2qOttaOjx2vmWbOxtfZYa+1wa+3x1tqvj7MnAAAAAMMb90qjXUn2995vTLJ/9Pp8f57ko733DUl+LcnnW2trxtwXAAAAgAGNG41uT7J39HxvkjvOX9B7/z+996Oj588m+XGSt465LwAAAAADGjcaTfXeTyTJ6PFtF1vcWrs5yYokT4+5LwAAAAADar33iy9o7etJ3j7PR/ck2dt7X3PW2ud77+V7jUafrU0ym2R77/0PL7LfjiQ7kmRqamrzvn37FvobuMzm5uayevXqxT4GS5DZYkjmi6GYLYZithiK2WIoZuvKsWXLloO99+mF1i0YjS76w60dSTLTez/x/6NQ7339POvekteC0b/svf+XS/3909PT/cCBA2/4fAxjdnY2MzMzi30MliCzxZDMF0MxWwzFbDEUs8VQzNaVo7V2SdFo3NvTHk6yffR8e5KvzHOQFUn+a5LffT3BCAAAAIDFM2402p1ka2vtaJKto9dprU231r44WvOPkvydJHe11r4z+m/jmPsCAAAAMKDl4/xw7/2nSW6b5/0DST4+ev77SX5/nH0AAAAAuLzGvdIIAAAAgCVINAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKMaORq21a1trj7bWjo4er7nI2re01o631r4w7r4AAAAADGcSVxrtSrK/935jkv2j1xfyL5J8YwJ7AgAAADCgSUSj25PsHT3fm+SO+Ra11jYnmUryBxPYEwAAAIABTSIaTfXeTyTJ6PFt5y9orf1Skn+dZOcE9gMAAABgYK33vvCi1r6e5O3zfHRPkr299zVnrX2+937O9xq11j6Z5C/13n+7tXZXkune+ycvsNeOJDuSZGpqavO+ffsu9W/hMpmbm8vq1asX+xgsQWaLIZkvhmK2GIrZYihmi6GYrSvHli1bDvbepxdad0nR6KK/oLUjSWZ67ydaa2uTzPbe15+35j8n+dtJfpZkdZIVSe7rvV/s+48yPT3dDxw4MNb5mLzZ2dnMzMws9jFYgswWQzJfDMVsMRSzxVDMFkMxW1eO1tolRaPlE9jr4STbk+wePX7l/AW994+cdbC78tqVRhcNRgAAAAAsnkl8p9HuJFtba0eTbB29TmtturX2xQn8fgAAAAAus7GvNOq9/zTJbfO8fyDJx+d5/3eS/M64+wIAAAAwnElcaQQAAADAEiMaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAxVjRqLV2bWvt0dba0dHjNRdY91dba3/QWvtua+2p1tq6cfYFAAAAYFjjXmm0K8n+3vuNSfaPXs/nd5Ps6b2/L8nNSX485r4AAAAADGjcaHR7kr2j53uT3HH+gtba+5Ms770/miS997ne+5+PuS8AAAAAAxo3Gk313k8kyejxbfOs+etJTrbWvtxaO9Ra29NaWzbmvgAAAAAMqPXeL76gta8nefs8H92TZG/vfc1Za5/vvZ/zvUattTuTfCnJTUn+JMkDSb7ae//SBfbbkWRHkkxNTW3et2/fpf81XBZzc3NZvXr1Yh+DJchsMSTzxVDMFkMxWwzFbDEUs3Xl2LJly8He+/RC65YvtKD3/qsX+qy19lxrbW3v/URrbW3m/66iHyU51Ht/ZvQzDyW5Ja+FpPn2uz/J/UkyPT3dZ2ZmFjoil9ns7Gz8uzAEs8WQzBdDMVsMxWwxFLPFUMzW0jPu7WkPJ9k+er49yVfmWfNHSa5prb119PpXkjw15r4AAAAADGjcaLQ7ydbW2tEkW0ev01qbbq19MUl6768m+SdJ9rfWnkjSkvzHMfcFAAAAYEAL3p52Mb33nya5bZ73DyT5+FmvH03yy+PsBQAAAMDlM+6VRgAAAAAsQaIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAxdjRqrV3bWnu0tXZ09HjNBdb9dmvtcGvtu621f9daa+PuDQAAAMAwJnGl0a4k+3vvNybZP3p9jtba30rywSS/nORvJPmbSW6dwN4AAAAADGAS0ej2JHtHz/cmuWOeNT3Jm5KsSLIyyVVJnpvA3gAAAAAMYBLRaKr3fiJJRo9vO39B7/2xJP8zyYnRf4/03r87gb0BAAAAGEDrvS+8qLWvJ3n7PB/dk2Rv733NWWuf772f871GrbX3JPm3SX599NajSf5p7/2b8+y1I8mOJJmamtq8b9++S/xTuFzm5uayevXqxT4GS5DZYkjmi6GYLYZithiK2WIoZuvKsWXLloO99+mF1i2/lF/We//VC33WWnuutba2936itbY2yY/nWfYPkvxh731u9DNfS3JLkhKNeu/3J7k/Saanp/vMzMylHJHLaHZ2Nv5dGILZYkjmi6GYLYZithiK2WIoZmvpmcTtaQ8n2T56vj3JV+ZZ8ydJbm2tLW+tXZXXvgTb7WkAAAAAP6cmEY12J9naWjuaZOvodVpr0621L47WPJjk6SRPJPnjJH/ce/9vE9gbAAAAgAFc0u1pF9N7/2mS2+Z5/0CSj4+ev5rkt8bdCwAAAIDLYxJXGgEAAACwxIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGwP9r7/5j7b7r+46/3nJoSRx+eFpllB8aoKG0IWTJeseSIBYHooamKDRUUVrakjGJCK3dsqlNBwpaYVWmIFDFtFVDUSKEIJtV0QRKki6A2jtSDSqTJnITjDfUacUJ05iK2xgitVk+++MekJP39b0nPf4e59iPh2TJ5/hz7vdz5bfO/frp7zkHAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQCqTN8PAAAUvklEQVQAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKBZKBpV1XVV9VhVPVNVa1use0tVHayqb1TVexc5JgAAAADTW/RKo0eTvD3Jl461oKp2JPmtJD+Z5PwkP1dV5y94XAAAAAAmdNoiDx5jHEiSqtpq2euTfGOM8aeztXuTvC3J1xY5NgAAAADTWcZ7Gp2d5JtH3T40uw8AAACAF6htrzSqqi8mecUmf3TLGOOzcxxjs8uQxhbHuzHJjUmye/furK+vz3EIlunIkSP+XpiE2WJK5oupmC2mYraYitliKmbr5LNtNBpjXLngMQ4lOfeo2+ckeWKL492e5PYkWVtbG3v27Fnw8Bxv6+vr8ffCFMwWUzJfTMVsMRWzxVTMFlMxWyefZbw8bV+S11TVq6rqh5L8bJLfXcJxAQAAAPgbWigaVdW1VXUoyaVJ7quqB2b3n1VV9yfJGOPpJL+c5IEkB5L89hjjscW2DQAAAMCUFv30tHuS3LPJ/U8kufqo2/cnuX+RYwEAAACwPMt4eRoAAAAAK0Y0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoFopGVXVdVT1WVc9U1dox1pxbVX9QVQdma29a5JgAAAAATG/RK40eTfL2JF/aYs3TSX5ljPFjSS5J8ktVdf6CxwUAAABgQqct8uAxxoEkqaqt1nwrybdmv3+yqg4kOTvJ1xY5NgAAAADTWep7GlXVK5NcnOSPlnlcAAAAAJ6fGmNsvaDqi0lesckf3TLG+OxszXqSXx1jfHWLr3Nmkv+a5NYxxt1brLsxyY1Jsnv37h/fu3fvdt8DS3bkyJGceeaZJ3obnITMFlMyX0zFbDEVs8VUzBZTMVur44orrnhojLHpe1MfbduXp40xrlx0M1X1oiS/k+SurYLR7Hi3J7k9SdbW1saePXsWPTzH2fr6evy9MAWzxZTMF1MxW0zFbDEVs8VUzNbJZ/KXp9XGGx7dmeTAGOM3pz4eAAAAAItbKBpV1bVVdSjJpUnuq6oHZvefVVX3z5a9IckvJnlTVT0y+3X1QrsGAAAAYFKLfnraPUnu2eT+J5JcPfv9HyY59serAQAAAPCCs9RPTwMAAABgNYhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaHSCfeADH8hHPvKRE70NAAAAgGcRjQAAAABoRKMT4NZbb815552XK6+8MgcPHkySPPLII7nkkkty4YUX5tprr813vvOdJMm+ffty4YUX5tJLL83NN9+cCy644ERuHQAAADhFiEZL9tBDD2Xv3r15+OGHc/fdd2ffvn1Jkne+85350Ic+lP379+d1r3tdPvjBDyZJ3vWud+VjH/tYvvzlL2fHjh0ncusAAADAKeS0E72BU8FnHn48H37gYJ44/FTy6P35B5e+OWeccUaS5Jprrsl3v/vdHD58OJdffnmS5IYbbsh1112Xw4cP58knn8xll12WJHnHO96Re++994R9HwAAAMCpw5VGE/vMw4/nfXf/SR4//FRGkr946q/z+1//dj7z8OPbPnaMMf0GAQAAADYhGk3sww8czFN//f9+cPuHz31t/vLr/y233bs/Tz75ZD73uc9l586d2bVrVx588MEkySc/+clcfvnl2bVrV17ykpfkK1/5SpJk7969J+R7AAAAAE49Xp42sScOP/Ws2z/8ir+bnT/6xjz00XfnZx48P2984xuTJJ/4xCfynve8J9/73vfy6le/Oh//+MeTJHfeeWfe/e53Z+fOndmzZ09e9rKXLf17AAAAAE49otHEznr56Xn8OeHoZZddn/Ov/sf5/Hvf9Kz7v39F0dFe+9rXZv/+/UmS2267LWtra9NtFgAAAGDGy9MmdvNV5+X0Fz37U89Of9GO3HzVeXM9/r777stFF12UCy64IA8++GDe//73T7FNAAAAgGdxpdHEfvris5PkB5+edtbLT8/NV533g/u3c/311+f666+fcosAAAAAjWi0BD998dlzRyIAAACAFwIvTwMAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgWSgaVdV1VfVYVT1TVWvbrN1RVQ9X1b2LHBMAAACA6S16pdGjSd6e5EtzrL0pyYEFjwcAAADAEiwUjcYYB8YYB7dbV1XnJPmpJHcscjwAAAAAlmNZ72n00SS/luSZJR0PAAAAgAXUGGPrBVVfTPKKTf7oljHGZ2dr1pP86hjjq5s8/q1Jrh5j/NOq2jNb99YtjndjkhuTZPfu3T++d+/eOb8VluXIkSM588wzT/Q2OAmZLaZkvpiK2WIqZoupmC2mYrZWxxVXXPHQGGPL96ZOktO2WzDGuHLBvbwhyTVVdXWSFyd5aVV9aozxC8c43u1Jbk+StbW1sWfPngUPz/G2vr4efy9MwWwxJfPFVMwWUzFbTMVsMRWzdfLZ9kqjub7IFlcaPWfdnmxzpdFz1n87yf9aeIMcb387yf890ZvgpGS2mJL5Yipmi6mYLaZitpiK2Vodf2eM8SPbLdr2SqOtVNW1Sf59kh9Jcl9VPTLGuKqqzkpyxxjj6kW+/jzfAMtXVV+d5zI2eL7MFlMyX0zFbDEVs8VUzBZTMVsnn4Wi0RjjniT3bHL/E0laMBpjrCdZX+SYAAAAAExvWZ+eBgAAAMAKEY34m7j9RG+Ak5bZYkrmi6mYLaZitpiK2WIqZuskc1zeCBsAAACAk4srjQAAAABoRCO2VVW/UVX7q+qRqvr87NPxNlt3Q1X9j9mvG5a9T1ZPVX24qr4+m697qurlx1j3L6vqsap6tKr+c1W9eNl7ZfU8j/l6eVV9erb2QFVduuy9slrmna3Z2h1V9XBV3bvMPbKa5pmtqjq3qv5g9nz1WFXddCL2ymp5Hj8T31JVB6vqG1X13mXvk9VTVdfNnoueqapjfmqa8/nVJRoxjw+PMS4cY1yU5N4k//q5C6rqbyX59ST/MMnrk/x6Ve1a7jZZQV9IcsEY48Ik/z3J+567oKrOTvLPk6yNMS5IsiPJzy51l6yqbedr5t8l+S9jjB9N8veSHFjS/lhd885WktwUM8X85pmtp5P8yhjjx5JckuSXqur8Je6R1TTPOdeOJL+V5CeTnJ/k58wWc3g0yduTfOlYC5zPrzbRiG2NMf7yqJs7k2z2RlhXJfnCGOPPxxjfycYPprcsY3+srjHG58cYT89ufiXJOcdYelqS06vqtCRnJHliGftjtc0zX1X10iT/KMmds8f81Rjj8PJ2ySqa97mrqs5J8lNJ7ljW3lht88zWGONbY4w/nv3+yWxEybOXt0tW0ZzPW69P8o0xxp+OMf4qyd4kb1vWHllNY4wDY4yDcyx1Pr+iRCPmUlW3VtU3k/x8NrnSKBsnK9886vahOIHh+fknSX7vuXeOMR5P8pEkf5bkW0n+Yozx+SXvjdW36XwleXWSbyf5+OwlRHdU1c7lbo0Vd6zZSpKPJvm1JM8sbzucRLaarSRJVb0yycVJ/mgJ++HkcazZcj7PJJzPrzbRiCRJVX1x9vrS5/56W5KMMW4ZY5yb5K4kv7zZl9jkPh/Nx7azNVtzSzYut79rk8fvysb/cr0qyVlJdlbVLyxr/7ywLTpf2fhfr7+f5D+OMS5O8t0k3sOB4/Hc9dYk/2eM8dASt80KOA7PW99fc2aS30nyL55zVTinqOMwW87n2dQ8s7XN453Pr7DTTvQGeGEYY1w559L/lOS+bLx/0dEOJdlz1O1zkqwvvDFW3nazVRtvmv7WJG8eY2x2YnJlkv85xvj2bP3dSS5L8qnjvVdWz3GYr0NJDo0xvv+/9J+OaESOy2y9Ick1VXV1khcneWlVfWqM4ST5FHccZitV9aJsBKO7xhh3H/9dsoqO08/Ec4+6fU68hIg8r38rHovz+RXmSiO2VVWvOermNUm+vsmyB5L8RFXtmpXkn5jdB8dUVW9J8q+SXDPG+N4xlv1Zkkuq6oyqqiRvjjeVZQ7zzNcY438n+WZVnTe7681JvrakLbKi5pyt940xzhljvDIbb/b5+4IR25lntmY/C+9McmCM8ZvL3B+ra85zrn1JXlNVr6qqH8rGc9fvLmuPnNScz68w0Yh53Da7/HB/NmLQTUlSVWtVdUeSjDH+PMlvZOOHzb4k/2Z2H2zlPyR5SZIvVNUjVfWxJKmqs6rq/iSZXQHy6SR/nORPsvG8dfsJ2i+rZdv5mvlnSe6aPcddlOTfLn+rrJh5Zwuer3lm6w1JfjHJm2ZrHpld0QZbmeec6+lsvA3FA9n4B/1vjzEeO1EbZjVU1bVVdSjJpUnuq6oHZvc7nz9J1DGuegUAAADgFOZKIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGj+P66v5Hgnz2bLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vsize = 100                                 # e.g. len(dictionary)\n",
    "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
    "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
    "\n",
    "words = ['the', 'dog', 'ran']\n",
    "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
    "\n",
    "umap_plot(fake_weight_matrix, word_ids, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2 Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3 Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.2 Highest and Lowest scoring sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3.3 Modified sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4.3 Number of unique tokens and sequence length \n",
    "\n",
    "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4.4 Example Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
