{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Updated_lm_homework.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HCzN-nn8BsK",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNWcR__B8d-i",
        "colab_type": "code",
        "outputId": "7be21f90-e4dc-422e-f8f0-f1a71b012610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P3D72q38BsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import operator\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZXhR1La8BsS",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyYA19ji8BsU",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZfFvEXa8BsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V50F-WEd8Bsc",
        "colab_type": "code",
        "outputId": "5390ae2c-3d64-4ea5-f0e7-1d55190b0466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-07 20:20:04--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 185.235.236.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-07 20:20:04--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-07 20:20:05--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 185.235.236.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|185.235.236.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!W1PvBsvgnQnMyovLQeI1XL_FD2LxKqlTOaS_GlQM5MFUGnf6yOINwTz8nJYDVTeHzwlEbHnzHoVZjlk5kzNpCxzVFvrLRy1k5XAhJ-Ia5rbFF5z6qQAltPStp70uwlYaI_808zkQIbT3VkAXpz60VJwYeFw2tq-vdonlfBpiij4Hk9Ngb_lIXPhHA5ysR6xnk0ZACT-GDzb1bTVVB6jqmOf2dVg5pm5TjcVarNg9DCmpZn3kAgsiTXQjjR2qljFQ32MNi7N_7NpXmgjYi2TomLdsfObP-qGtjcnarOaWf6q9nqqS0dXA7rQrVjbZ-uJzj7YoUjN01SIZtvdjd6Iy4IHbJ4DZgPzUwhFDw0l78eP92zknc4f3SsAy9ljA9Rm2OjsOdsvabi0t7icfFolH_ET29HOdExSE83oCO-XB2auqNLhCsrsm6pBDxZecDpYKpFSrEjG0M_z4Zqsi2bpIG4jjMbCD92Z7gSzRc6wtlSQWFkmym99J0kkuo6602olm0qd7nNvJRfbFZfYZxMGVPkHxBv6S7O8bqwC3UhrlXmlc2GJCHNQNmfg1UUsuFP7vjQU3DZNmC56QVKcWo6SYVID5gsEM-WBISEEQqPPgnXXOfQRznGDrRmKzLziatNM795z11jzsx754hA8Kic24PRMzkGg2HVoJYEIZbsY8HNgRnhsQEMLt12X5pZO05o9J17lVg8_ZPvpm875UhYJFNU3jqhxVdDlHigsLXP9LgsH5b1g9jnApct5H7DZWDT-HEqglldSDyXfajq3QUORn5_UMSgAk5sxlV2stRONBQb2jWmmzUAWvQrWJyt813PgbIgVZlZTInYPrvVxUs_lMSFSaC5dUsFQd1zV_1K0yLtBvuy0XMidsIjm6O9lccySJ0svErUgYZlRDzdgmuOOuFesc9S4jDVFaIEhyoO9OQy4BbbQrpxEvu68iz5xV_yDspgnFRFvfEJXF4BzuoEGQ4zBw9-Qu_DiLVl4qJNU5cobDoixHFzQyWQKtERKCuxhl4GxyqFFNMLPqPGHvVXVxH_At-N4xYm3dh2-DkGkilJYTE_ek8IQFmMOteo6_3R9WQzPdho1hhUSRzV4Q6BlXRi_dLg4wP1vvDAug96Cpr7bFTsARqUXeNy3UAg-i-JAwfgMgajRnLzMxuJ3oWgJgfO7MRkEGgRYlaGdXTYXzeWTOp5nglvTa5tOhRDbI4oH5j9ZyJUDOW-xIQ15i8RuYHjEPPJHN3OGUo2qvfGls-r0uwRmOYavQ1BYOw9xDqa3XN9gNuw-ACvBOjG1ZxYr3sk4bdczXyCEu2lTiMnh0YCfP6goLUOsly9htp5gBgRXxd2Zc6kMNb4kdroIxDHrrmCSjXkQoRXI0O6qeQ4YMzUrJ-iZDOSW5rBdGYvK2BXNzSGUz9PW5LbsI-Oo./download [following]\n",
            "--2019-10-07 20:20:05--  https://public.boxcloud.com/d/1/b1!W1PvBsvgnQnMyovLQeI1XL_FD2LxKqlTOaS_GlQM5MFUGnf6yOINwTz8nJYDVTeHzwlEbHnzHoVZjlk5kzNpCxzVFvrLRy1k5XAhJ-Ia5rbFF5z6qQAltPStp70uwlYaI_808zkQIbT3VkAXpz60VJwYeFw2tq-vdonlfBpiij4Hk9Ngb_lIXPhHA5ysR6xnk0ZACT-GDzb1bTVVB6jqmOf2dVg5pm5TjcVarNg9DCmpZn3kAgsiTXQjjR2qljFQ32MNi7N_7NpXmgjYi2TomLdsfObP-qGtjcnarOaWf6q9nqqS0dXA7rQrVjbZ-uJzj7YoUjN01SIZtvdjd6Iy4IHbJ4DZgPzUwhFDw0l78eP92zknc4f3SsAy9ljA9Rm2OjsOdsvabi0t7icfFolH_ET29HOdExSE83oCO-XB2auqNLhCsrsm6pBDxZecDpYKpFSrEjG0M_z4Zqsi2bpIG4jjMbCD92Z7gSzRc6wtlSQWFkmym99J0kkuo6602olm0qd7nNvJRfbFZfYZxMGVPkHxBv6S7O8bqwC3UhrlXmlc2GJCHNQNmfg1UUsuFP7vjQU3DZNmC56QVKcWo6SYVID5gsEM-WBISEEQqPPgnXXOfQRznGDrRmKzLziatNM795z11jzsx754hA8Kic24PRMzkGg2HVoJYEIZbsY8HNgRnhsQEMLt12X5pZO05o9J17lVg8_ZPvpm875UhYJFNU3jqhxVdDlHigsLXP9LgsH5b1g9jnApct5H7DZWDT-HEqglldSDyXfajq3QUORn5_UMSgAk5sxlV2stRONBQb2jWmmzUAWvQrWJyt813PgbIgVZlZTInYPrvVxUs_lMSFSaC5dUsFQd1zV_1K0yLtBvuy0XMidsIjm6O9lccySJ0svErUgYZlRDzdgmuOOuFesc9S4jDVFaIEhyoO9OQy4BbbQrpxEvu68iz5xV_yDspgnFRFvfEJXF4BzuoEGQ4zBw9-Qu_DiLVl4qJNU5cobDoixHFzQyWQKtERKCuxhl4GxyqFFNMLPqPGHvVXVxH_At-N4xYm3dh2-DkGkilJYTE_ek8IQFmMOteo6_3R9WQzPdho1hhUSRzV4Q6BlXRi_dLg4wP1vvDAug96Cpr7bFTsARqUXeNy3UAg-i-JAwfgMgajRnLzMxuJ3oWgJgfO7MRkEGgRYlaGdXTYXzeWTOp5nglvTa5tOhRDbI4oH5j9ZyJUDOW-xIQ15i8RuYHjEPPJHN3OGUo2qvfGls-r0uwRmOYavQ1BYOw9xDqa3XN9gNuw-ACvBOjG1ZxYr3sk4bdczXyCEu2lTiMnh0YCfP6goLUOsly9htp5gBgRXxd2Zc6kMNb4kdroIxDHrrmCSjXkQoRXI0O6qeQ4YMzUrJ-iZDOSW5rBdGYvK2BXNzSGUz9PW5LbsI-Oo./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 185.235.236.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|185.235.236.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  7.92MB/s    in 1.5s    \n",
            "\n",
            "2019-10-07 20:20:07 (7.92 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJp7NFs8Bsk",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "820qKy6U8Bsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlaXszzj8Bsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# delta = 0.0005\n",
        "# for n in [2, 3, 4]:\n",
        "#     lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm.estimate(datasets['train'])\n",
        "\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7cWyvDm8BtA",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5imstdWS8BtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwE6KP0P8BtF",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj8Z2ly_8BtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXlT_7WV8BtK",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f59E088X8BtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler,DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDZSVhLr8BtP",
        "colab_type": "text"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXP5LO2P8BtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>') # validation token is not seen in the training dataset\n",
        "                                # <unk> is in the original training dataset. \n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkqJe4HI8BtU",
        "colab_type": "code",
        "outputId": "bb4cb916-5950-4308-8a7f-e5552a3a430b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "train_dict=Dictionary(datasets) # excluding validation dataset #109\n",
        "#all_dict=Dictionary(datasets, include_valid = True)\n",
        "\n",
        "# example\n",
        "rand_int = np.random.randint(1, len(datasets['valid']))\n",
        "print(' '.join(datasets['valid'][rand_int]))\n",
        "encoded = train_dict.encode_token_seq(datasets['valid'][rand_int])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = train_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "# checking\n",
        "print('length of train_dict is ', len(train_dict)) # that's because <unk> is already in the dataset \n",
        "print('unique words in training dataset is ', len(vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:14<00:00, 583.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "This freeway section was linked to the rest of Route 29 by a tunnel completed in 2002 .\n",
            "\n",
            " encoded - [504, 11565, 5749, 118, 5116, 13, 6, 215, 5, 11563, 943, 28, 20, 16264, 1214, 33, 2407, 39]\n",
            "\n",
            " decoded - ['This', 'freeway', 'section', 'was', 'linked', 'to', 'the', 'rest', 'of', 'Route', '29', 'by', 'a', 'tunnel', 'completed', 'in', '2002', '.']\n",
            "length of train_dict is  33178\n",
            "unique words in training dataset is  33175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhkIjBnj8BtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given the dictionary from above, now write a function that tokenize the all datasets into id's\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets\n",
        "\n",
        "# Given a tokenzied dataset with ngram defined, slice the input sequences into n-grams \n",
        "# [0,1,2,3,4,5], 2 -> [0,1], [1,2], [2,3], [3,4], [4,5]\n",
        "def slice_sequences_given_order(tokenized_dataset_with_spec, ngram_order=2):\n",
        "    sliced_datasets = {}\n",
        "    for split, dataset in tokenized_dataset_with_spec.items():\n",
        "        _list_of_sliced_ngrams = []\n",
        "        for seq in tqdm(dataset):\n",
        "            ngrams = [seq[i:i+ngram_order] for i in range(len(seq)-ngram_order+1)]\n",
        "            _list_of_sliced_ngrams.extend(ngrams)\n",
        "        sliced_datasets[split] = _list_of_sliced_ngrams\n",
        "\n",
        "    return sliced_datasets\n",
        "\n",
        "# # Now we create a dataset\n",
        "class NgramDataset(Dataset):\n",
        "    def __init__(self, sliced_dataset_split):\n",
        "        super().__init__()\n",
        "\n",
        "        # for each sample: [:-1] is input, [-1] is target\n",
        "        self.sequences = [torch.tensor(i, dtype=torch.long) for i in sliced_dataset_split]\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        sample = self.sequences[i]\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "def batchify(list_minibatch):\n",
        "    inp_list = [i[:-1] for i in list_minibatch]\n",
        "    tar_list = [i[-1] for i in list_minibatch]\n",
        "\n",
        "    inp_tensor = torch.stack(inp_list, dim=0) # list of tensors and create a new tensor a u-dimension specified by dim\n",
        "    tar_tensor = torch.stack(tar_list, dim=0)\n",
        "    # cat: take a list of tensors and use existing dimension to concatent. Cannot create a new u-dimension speicfied. \n",
        "\n",
        "    return inp_tensor, tar_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY8VffsL8Btg",
        "colab_type": "code",
        "outputId": "bd8e8329-1723-4051-b5ae-1eb5e7a64c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#  returns dictionary of three items with 'train', 'valid' and 'test' with lists of token ids \n",
        "tokenized_ngram = tokenize_dataset(datasets, train_dict, ngram_order=2)\n",
        "\n",
        "# returns dictionary of three items, each item is a list of sliced n-grams\n",
        "sliced_ngram = slice_sequences_given_order(tokenized_ngram, ngram_order=2)\n",
        "\n",
        "# check that the sentence is encoded with (n-1)<bos> and can be decoded back to tokens \n",
        "decoded_with_spec = train_dict.decode_idx_seq(tokenized_ngram['train'][3010])\n",
        "print(f'\\n decoded with spec - {decoded_with_spec}')\n",
        "\n",
        "ngram_datasets = {}\n",
        "ngram_loaders = {}\n",
        "for split, dataset_sliced in sliced_ngram.items():\n",
        "    if split == 'train':\n",
        "        shuffle_ = True\n",
        "    else:\n",
        "        shuffle_ = False\n",
        "    dataset_ = NgramDataset(dataset_sliced)\n",
        "    ngram_datasets[split] = dataset_\n",
        "    ngram_loaders[split] = DataLoader(dataset_, batch_size=2048, shuffle=shuffle_, collate_fn=batchify)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 98675.51it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 110083.78it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 120949.29it/s]\n",
            "100%|██████████| 78274/78274 [00:02<00:00, 31684.36it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 18767.50it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 121286.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " decoded with spec - ['<bos>', 'The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu1jlFox8Btk",
        "colab_type": "code",
        "outputId": "28831efb-1cc5-4f91-bcf7-6a3def1adf4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('The length of original training dataset is: ', len(datasets['train']))\n",
        "print('The length of tokenzied training dataset is: ', len(tokenized_ngram['train']))\n",
        "print('Slided_ngram for training dataset now has length of ', len(sliced_ngram['train']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of original training dataset is:  78274\n",
            "The length of tokenzied training dataset is:  78274\n",
            "Slided_ngram for training dataset now has length of  2003028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJBw4I88Bto",
        "colab_type": "code",
        "outputId": "571172ea-9ef6-4ac4-c841-35ab791c900b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(tokenized_ngram['valid'][10])\n",
        "print(train_dict.decode_idx_seq(tokenized_ngram['valid'][10]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 504, 1506, 7106, 741, 459, 20, 140, 98, 432, 19366, 10, 150, 15605, 13, 260, 3748, 955, 1826, 1384, 1145, 98, 1722, 4303, 39, 1]\n",
            "['<bos>', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSWY6ayI8Bt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "    \n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors]) # t.size(-1) changes the type from torch.Size to int.\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:  # dim = 0: cat on row, dim = 1: add on column \n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = persona_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM1MWdTo8BuD",
        "colab_type": "code",
        "outputId": "d60d7149-c74a-418a-82df-def011835f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tensor_dataset = {}\n",
        "\n",
        "# split: train, valid, test\n",
        "for split, listoflists in tokenized_ngram.items():\n",
        "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "tensor_dataset['train'][24]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  0, 282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39]]),\n",
              " tensor([[282,  13, 283, 181, 194, 195, 284,  13,  20, 285, 286,  39,   1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRvBF-uB8BuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "loaders = {}\n",
        "batch_size = 32\n",
        "for split, dataset in tensor_dataset.items():\n",
        "    loaders[split] = DataLoader(dataset, batch_size=batch_size, shuffle = True, collate_fn=pad_collate_fn)\n",
        "    \n",
        "    # why is shuffle always TRUE in this loader? even for valid and test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1xV4hU28BuO",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model: RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlERvjpg8BuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import RNNBase, RNN\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Linear, functional\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvqpivh8Buu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        # before nn.RNN is hidden\n",
        "        # Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
        "        # RNN natrually takes multi sentence inputs and outputs hidden_size \n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        # project of outputs \n",
        "        # rnn_outputs: tuple with second element being last hidden state. \n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXHigHsEntxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'rnn_dropout': rnn_dropout,\n",
        "    }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKYJzsLto6K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check\n",
        "# define lookup, rnn, and projection\n",
        "# lookup table: stores embeddings of a fixed dictionary and size.\n",
        "lookup = nn.Embedding(num_embeddings=options['num_embeddings'], \n",
        "                      embedding_dim=options['embedding_dim'], \n",
        "                      padding_idx=options['padding_idx'])\n",
        "\n",
        "# before nn.RNN is hidden\n",
        "# Now: you do lookup table, and returns the tensors of sequence. No need of concat, becuase RNN naturally takes care of this\n",
        "# RNN natrually takes multi sentence inputs and outputs hidden_size \n",
        "rnn = nn.RNN(options['input_size'], options['hidden_size'], \n",
        "             options['num_layers'], dropout=options['rnn_dropout'], \n",
        "             batch_first=True)\n",
        "# input_size == embedding_dimension\n",
        "projection = nn.Linear(options['hidden_size'], options['num_embeddings'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1TlokVsRm4",
        "colab_type": "code",
        "outputId": "9168d5ce-8a99-4b70-c100-213de9a1458d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(33178, 64, padding_idx=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYq0aW18Bux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt') # change the model name\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 64\n",
        "    hidden_size = 128 # output of dimension \n",
        "    num_layers = 2\n",
        "    rnn_dropout = 0.1\n",
        "    input_size = lookup.weight.size(1) # embedding_dim\n",
        "    vocab_size = lookup.weight.size(0) # num_embeddings\n",
        "    \n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "# change the type from generator to list \n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giDfRLtCAmOS",
        "colab_type": "code",
        "outputId": "979c46ce-4dd9-41be-f3b0-15c0aeba8dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lookup # embedding_size, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(33178, 300, padding_idx=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6sdjrdxAqcB",
        "colab_type": "code",
        "outputId": "e6eca81e-64b4-455b-a071-8adef5d925f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "rnn # embedding_dim, hidden_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(300, 300, num_layers=2, batch_first=True, dropout=0.1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 477
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVpIUfUeAdug",
        "colab_type": "code",
        "outputId": "9e963ccb-6261-4d50-c671-ee7e46a912be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "projection \n",
        "# in_features – size of each input sample = hidden_size\n",
        "# out_features – size of each output sample = number of possible next token\n",
        "# bias – If set to False, the layer will not learn an additive bias. Default: True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=300, out_features=33178, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 478
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGnsOgMI8Bu0",
        "colab_type": "code",
        "outputId": "6e0729cc-d0e3-4f67-dea1-26456a7697ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# check model\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLanguageModel(\n",
              "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
              "  (rnn): RNN(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 479
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pka57Cz78Bu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the following two chunks out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOgwouSJ8Bu9",
        "colab_type": "code",
        "outputId": "2dc086ea-937d-4d11-c1b6-902b0d37a19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        }
      },
      "source": [
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(100):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4347\n",
            "Step 100 avg train loss = 7.7255\n",
            "Step 200 avg train loss = 6.9448\n",
            "Step 300 avg train loss = 6.7479\n",
            "Step 400 avg train loss = 6.6020\n",
            "Step 500 avg train loss = 6.5238\n",
            "Step 600 avg train loss = 6.4444\n",
            "Step 700 avg train loss = 6.3687\n",
            "Step 800 avg train loss = 6.3088\n",
            "Step 900 avg train loss = 6.2642\n",
            "Step 1000 avg train loss = 6.2030\n",
            "Step 1100 avg train loss = 6.1762\n",
            "Step 1200 avg train loss = 6.1313\n",
            "Step 1300 avg train loss = 6.1102\n",
            "Step 1400 avg train loss = 6.0654\n",
            "Step 1500 avg train loss = 6.0286\n",
            "Step 1600 avg train loss = 6.0137\n",
            "Step 1700 avg train loss = 5.9976\n",
            "Step 1800 avg train loss = 5.9683\n",
            "Step 1900 avg train loss = 5.9564\n",
            "Step 2000 avg train loss = 5.9412\n",
            "Step 2100 avg train loss = 5.9246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-a6274d42989d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfw_BFDp8BvB",
        "colab_type": "code",
        "outputId": "95abf217-cf3c-485e-c073-d684766f7b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of RNN baseline model')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZaYCQy1B3-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8WPonsMCDjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODfcbzQ8BvG",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS8rQKQ98BvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import LSTM\n",
        "# input_size, hidden_size, num_layers. \n",
        "# Optional: bias = False, dropout = 0 (probability of dropout)\n",
        "# bidirectional = False. \n",
        "rnn = nn.LSTM(10, 20, 2)\n",
        "\n",
        "# input: tensor containing feature of the input sequence \n",
        "# shape: seq_len, batch, input_size\n",
        "input = torch.randn(5,3,10)\n",
        "\n",
        "# h0: tensor contain hidden state for t = seq_len\n",
        "# shape: num_layers * num*directions, batch, hidden_size\n",
        "h0 = torch.randn(2,3,20)\n",
        "\n",
        "# c0: tensor contain cell state for t = seq_length\n",
        "# shape: num_layers * num_directions, batch, hidden_size \n",
        "c0 = torch.randn(2,3,20)\n",
        "\n",
        "# output: shape (seq_len, batch, num_direction * hidden size)\n",
        "output, (hn,cn) = rnn(input, (h0,c0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUGwapsC8BvL",
        "colab_type": "code",
        "outputId": "2adff3c8-e936-42b3-c6c6-62bd1b1e6a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# example embedding layer\n",
        "lookup = Embedding(num_embeddings=len(train_dict), embedding_dim=64, padding_idx=train_dict.get_id('<pad>'))\n",
        "lookup.weight.size()\n",
        "# train_dict = vocab size + 3"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33178, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoKNOBmB8BvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the baseline, we will stop the epoch around 20 \n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], \n",
        "                            dropout=options['lstm_dropout'], batch_first=True, bias = options['bias'],\n",
        "                           bidirectional = options['bid'])\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_outputs = self.lstm(embeddings)\n",
        "        # project of outputs \n",
        "        # rnn_outputs: tupple with second element being last hidden state. \n",
        "        logits = self.projection(lstm_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cnF0Xq98HPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = LSTMModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 64\n",
        "    hidden_size = 128 # output of dimension \n",
        "    num_layers = 2\n",
        "    lstm_dropout = 0.1\n",
        "#     input_size = lookup.weight.size(1)\n",
        "    vocab_size = len(train_dict)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "\n",
        "    \n",
        "    model = LSTMModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4LKhWXs9MeR",
        "colab_type": "code",
        "outputId": "2cef94d7-0113-4f6c-9fa4-15f9c52d9c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 64, padding_idx=2)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi5oI78I6b1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following cell geives the performance of the baseline LSTM\n",
        "# TAKE A LONG TIME TO RUN. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp6kNjF_9OD_",
        "colab_type": "code",
        "outputId": "e4017a58-b2ed-46f9-cd05-5309c8b39ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model = LSTMModel(options).to(current_device)\n",
        "plot_cache = []\n",
        "min_val_loss = 20 \n",
        "for epoch_number in range(20):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "        \n",
        "print('Saving best model...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './baseline_LSTM.pt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 7.0863\n",
            "Step 100 avg train loss = 7.0200\n",
            "Step 200 avg train loss = 6.9266\n",
            "Step 300 avg train loss = 6.8133\n",
            "Step 400 avg train loss = 6.7200\n",
            "Step 500 avg train loss = 6.6318\n",
            "Step 600 avg train loss = 6.5733\n",
            "Step 700 avg train loss = 6.5168\n",
            "Step 800 avg train loss = 6.4810\n",
            "Step 900 avg train loss = 6.4369\n",
            "Step 1000 avg train loss = 6.3943\n",
            "Step 1100 avg train loss = 6.3615\n",
            "Step 1200 avg train loss = 6.3266\n",
            "Step 1300 avg train loss = 6.2870\n",
            "Step 1400 avg train loss = 6.2620\n",
            "Step 1500 avg train loss = 6.2168\n",
            "Step 1600 avg train loss = 6.2051\n",
            "Step 1700 avg train loss = 6.1710\n",
            "Step 1800 avg train loss = 6.1293\n",
            "Step 1900 avg train loss = 6.1156\n",
            "Step 2000 avg train loss = 6.0862\n",
            "Step 2100 avg train loss = 6.0944\n",
            "Step 2200 avg train loss = 6.0612\n",
            "Step 2300 avg train loss = 6.0266\n",
            "Step 2400 avg train loss = 6.0032\n",
            "Validation loss after 0 epoch = 5.8233\n",
            "Step 0 avg train loss = 5.8466\n",
            "Step 100 avg train loss = 5.9111\n",
            "Step 200 avg train loss = 5.8860\n",
            "Step 300 avg train loss = 5.8898\n",
            "Step 400 avg train loss = 5.8895\n",
            "Step 500 avg train loss = 5.8599\n",
            "Step 600 avg train loss = 5.8661\n",
            "Step 700 avg train loss = 5.8486\n",
            "Step 800 avg train loss = 5.8073\n",
            "Step 900 avg train loss = 5.8136\n",
            "Step 1000 avg train loss = 5.8237\n",
            "Step 1100 avg train loss = 5.7940\n",
            "Step 1200 avg train loss = 5.7931\n",
            "Step 1300 avg train loss = 5.7645\n",
            "Step 1400 avg train loss = 5.7615\n",
            "Step 1500 avg train loss = 5.7428\n",
            "Step 1600 avg train loss = 5.7537\n",
            "Step 1700 avg train loss = 5.7298\n",
            "Step 1800 avg train loss = 5.6969\n",
            "Step 1900 avg train loss = 5.7121\n",
            "Step 2000 avg train loss = 5.6991\n",
            "Step 2100 avg train loss = 5.6891\n",
            "Step 2200 avg train loss = 5.6726\n",
            "Step 2300 avg train loss = 5.6940\n",
            "Step 2400 avg train loss = 5.6580\n",
            "Validation loss after 1 epoch = 5.5607\n",
            "Step 0 avg train loss = 5.5434\n",
            "Step 100 avg train loss = 5.5452\n",
            "Step 200 avg train loss = 5.5621\n",
            "Step 300 avg train loss = 5.5332\n",
            "Step 400 avg train loss = 5.5507\n",
            "Step 500 avg train loss = 5.5510\n",
            "Step 600 avg train loss = 5.5591\n",
            "Step 700 avg train loss = 5.5285\n",
            "Step 800 avg train loss = 5.5261\n",
            "Step 900 avg train loss = 5.5465\n",
            "Step 1000 avg train loss = 5.5328\n",
            "Step 1100 avg train loss = 5.5330\n",
            "Step 1200 avg train loss = 5.5034\n",
            "Step 1300 avg train loss = 5.5002\n",
            "Step 1400 avg train loss = 5.5018\n",
            "Step 1500 avg train loss = 5.4922\n",
            "Step 1600 avg train loss = 5.4849\n",
            "Step 1700 avg train loss = 5.4831\n",
            "Step 1800 avg train loss = 5.4891\n",
            "Step 1900 avg train loss = 5.4797\n",
            "Step 2000 avg train loss = 5.4614\n",
            "Step 2100 avg train loss = 5.4622\n",
            "Step 2200 avg train loss = 5.4617\n",
            "Step 2300 avg train loss = 5.4520\n",
            "Step 2400 avg train loss = 5.4403\n",
            "Validation loss after 2 epoch = 5.4297\n",
            "Step 0 avg train loss = 5.1113\n",
            "Step 100 avg train loss = 5.3614\n",
            "Step 200 avg train loss = 5.3298\n",
            "Step 300 avg train loss = 5.3446\n",
            "Step 400 avg train loss = 5.3246\n",
            "Step 500 avg train loss = 5.3085\n",
            "Step 600 avg train loss = 5.3320\n",
            "Step 700 avg train loss = 5.3338\n",
            "Step 800 avg train loss = 5.3105\n",
            "Step 900 avg train loss = 5.3265\n",
            "Step 1000 avg train loss = 5.3130\n",
            "Step 1100 avg train loss = 5.3274\n",
            "Step 1200 avg train loss = 5.3137\n",
            "Step 1300 avg train loss = 5.3129\n",
            "Step 1400 avg train loss = 5.3313\n",
            "Step 1500 avg train loss = 5.3077\n",
            "Step 1600 avg train loss = 5.2862\n",
            "Step 1700 avg train loss = 5.3240\n",
            "Step 1800 avg train loss = 5.2860\n",
            "Step 1900 avg train loss = 5.2976\n",
            "Step 2000 avg train loss = 5.2729\n",
            "Step 2100 avg train loss = 5.2752\n",
            "Step 2200 avg train loss = 5.2920\n",
            "Step 2300 avg train loss = 5.2710\n",
            "Step 2400 avg train loss = 5.2845\n",
            "Validation loss after 3 epoch = 5.3562\n",
            "Step 0 avg train loss = 5.1434\n",
            "Step 100 avg train loss = 5.1718\n",
            "Step 200 avg train loss = 5.1707\n",
            "Step 300 avg train loss = 5.1684\n",
            "Step 400 avg train loss = 5.1573\n",
            "Step 500 avg train loss = 5.1514\n",
            "Step 600 avg train loss = 5.1843\n",
            "Step 700 avg train loss = 5.1482\n",
            "Step 800 avg train loss = 5.1617\n",
            "Step 900 avg train loss = 5.1539\n",
            "Step 1000 avg train loss = 5.1785\n",
            "Step 1100 avg train loss = 5.1625\n",
            "Step 1200 avg train loss = 5.1502\n",
            "Step 1300 avg train loss = 5.1451\n",
            "Step 1400 avg train loss = 5.1660\n",
            "Step 1500 avg train loss = 5.1323\n",
            "Step 1600 avg train loss = 5.1497\n",
            "Step 1700 avg train loss = 5.1697\n",
            "Step 1800 avg train loss = 5.1509\n",
            "Step 1900 avg train loss = 5.1492\n",
            "Step 2000 avg train loss = 5.1416\n",
            "Step 2100 avg train loss = 5.1433\n",
            "Step 2200 avg train loss = 5.1633\n",
            "Step 2300 avg train loss = 5.1416\n",
            "Step 2400 avg train loss = 5.1608\n",
            "Validation loss after 4 epoch = 5.3118\n",
            "Step 0 avg train loss = 4.9447\n",
            "Step 100 avg train loss = 5.0148\n",
            "Step 200 avg train loss = 5.0230\n",
            "Step 300 avg train loss = 5.0276\n",
            "Step 400 avg train loss = 5.0230\n",
            "Step 500 avg train loss = 5.0330\n",
            "Step 600 avg train loss = 5.0120\n",
            "Step 700 avg train loss = 5.0337\n",
            "Step 800 avg train loss = 5.0339\n",
            "Step 900 avg train loss = 5.0303\n",
            "Step 1000 avg train loss = 5.0622\n",
            "Step 1100 avg train loss = 5.0296\n",
            "Step 1200 avg train loss = 5.0335\n",
            "Step 1300 avg train loss = 5.0202\n",
            "Step 1400 avg train loss = 5.0432\n",
            "Step 1500 avg train loss = 5.0423\n",
            "Step 1600 avg train loss = 5.0470\n",
            "Step 1700 avg train loss = 5.0465\n",
            "Step 1800 avg train loss = 5.0368\n",
            "Step 1900 avg train loss = 5.0461\n",
            "Step 2000 avg train loss = 5.0525\n",
            "Step 2100 avg train loss = 5.0449\n",
            "Step 2200 avg train loss = 5.0178\n",
            "Step 2300 avg train loss = 5.0546\n",
            "Step 2400 avg train loss = 5.0173\n",
            "Validation loss after 5 epoch = 5.2922\n",
            "Step 0 avg train loss = 4.7017\n",
            "Step 100 avg train loss = 4.9204\n",
            "Step 200 avg train loss = 4.9169\n",
            "Step 300 avg train loss = 4.9215\n",
            "Step 400 avg train loss = 4.9211\n",
            "Step 500 avg train loss = 4.8891\n",
            "Step 600 avg train loss = 4.9412\n",
            "Step 700 avg train loss = 4.9295\n",
            "Step 800 avg train loss = 4.9425\n",
            "Step 900 avg train loss = 4.9328\n",
            "Step 1000 avg train loss = 4.9519\n",
            "Step 1100 avg train loss = 4.9247\n",
            "Step 1200 avg train loss = 4.9585\n",
            "Step 1300 avg train loss = 4.9448\n",
            "Step 1400 avg train loss = 4.9278\n",
            "Step 1500 avg train loss = 4.9288\n",
            "Step 1600 avg train loss = 4.9582\n",
            "Step 1700 avg train loss = 4.9372\n",
            "Step 1800 avg train loss = 4.9196\n",
            "Step 1900 avg train loss = 4.9321\n",
            "Step 2000 avg train loss = 4.9250\n",
            "Step 2100 avg train loss = 4.9101\n",
            "Step 2200 avg train loss = 4.9163\n",
            "Step 2300 avg train loss = 4.9302\n",
            "Step 2400 avg train loss = 4.9549\n",
            "Validation loss after 6 epoch = 5.2841\n",
            "Step 0 avg train loss = 4.8695\n",
            "Step 100 avg train loss = 4.8045\n",
            "Step 200 avg train loss = 4.8294\n",
            "Step 300 avg train loss = 4.8278\n",
            "Step 400 avg train loss = 4.8329\n",
            "Step 500 avg train loss = 4.8239\n",
            "Step 600 avg train loss = 4.8349\n",
            "Step 700 avg train loss = 4.8690\n",
            "Step 800 avg train loss = 4.8324\n",
            "Step 900 avg train loss = 4.8435\n",
            "Step 1000 avg train loss = 4.8450\n",
            "Step 1100 avg train loss = 4.8368\n",
            "Step 1200 avg train loss = 4.8505\n",
            "Step 1300 avg train loss = 4.8468\n",
            "Step 1400 avg train loss = 4.8510\n",
            "Step 1500 avg train loss = 4.8339\n",
            "Step 1600 avg train loss = 4.8627\n",
            "Step 1700 avg train loss = 4.8453\n",
            "Step 1800 avg train loss = 4.8560\n",
            "Step 1900 avg train loss = 4.8612\n",
            "Step 2000 avg train loss = 4.8578\n",
            "Step 2100 avg train loss = 4.8430\n",
            "Step 2200 avg train loss = 4.8702\n",
            "Step 2300 avg train loss = 4.8353\n",
            "Step 2400 avg train loss = 4.8638\n",
            "Validation loss after 7 epoch = 5.2809\n",
            "Step 0 avg train loss = 4.7470\n",
            "Step 100 avg train loss = 4.7459\n",
            "Step 200 avg train loss = 4.7354\n",
            "Step 300 avg train loss = 4.7444\n",
            "Step 400 avg train loss = 4.7343\n",
            "Step 500 avg train loss = 4.7408\n",
            "Step 600 avg train loss = 4.7603\n",
            "Step 700 avg train loss = 4.7504\n",
            "Step 800 avg train loss = 4.7704\n",
            "Step 900 avg train loss = 4.7664\n",
            "Step 1000 avg train loss = 4.7505\n",
            "Step 1100 avg train loss = 4.7623\n",
            "Step 1200 avg train loss = 4.7743\n",
            "Step 1300 avg train loss = 4.7701\n",
            "Step 1400 avg train loss = 4.7791\n",
            "Step 1500 avg train loss = 4.7600\n",
            "Step 1600 avg train loss = 4.7797\n",
            "Step 1700 avg train loss = 4.7896\n",
            "Step 1800 avg train loss = 4.7708\n",
            "Step 1900 avg train loss = 4.7825\n",
            "Step 2000 avg train loss = 4.7981\n",
            "Step 2100 avg train loss = 4.7845\n",
            "Step 2200 avg train loss = 4.8000\n",
            "Step 2300 avg train loss = 4.7924\n",
            "Step 2400 avg train loss = 4.7921\n",
            "Validation loss after 8 epoch = 5.2987\n",
            "Step 0 avg train loss = 4.6857\n",
            "Step 100 avg train loss = 4.6830\n",
            "Step 200 avg train loss = 4.6996\n",
            "Step 300 avg train loss = 4.6605\n",
            "Step 400 avg train loss = 4.6979\n",
            "Step 500 avg train loss = 4.6663\n",
            "Step 600 avg train loss = 4.6855\n",
            "Step 700 avg train loss = 4.6916\n",
            "Step 800 avg train loss = 4.6988\n",
            "Step 900 avg train loss = 4.7003\n",
            "Step 1000 avg train loss = 4.6932\n",
            "Step 1100 avg train loss = 4.6996\n",
            "Step 1200 avg train loss = 4.7004\n",
            "Step 1300 avg train loss = 4.7015\n",
            "Step 1400 avg train loss = 4.7098\n",
            "Step 1500 avg train loss = 4.7027\n",
            "Step 1600 avg train loss = 4.7106\n",
            "Step 1700 avg train loss = 4.7222\n",
            "Step 1800 avg train loss = 4.7225\n",
            "Step 1900 avg train loss = 4.7178\n",
            "Step 2000 avg train loss = 4.7214\n",
            "Step 2100 avg train loss = 4.7269\n",
            "Step 2200 avg train loss = 4.7266\n",
            "Step 2300 avg train loss = 4.7068\n",
            "Step 2400 avg train loss = 4.7275\n",
            "Validation loss after 9 epoch = 5.3080\n",
            "Step 0 avg train loss = 4.5204\n",
            "Step 100 avg train loss = 4.5975\n",
            "Step 200 avg train loss = 4.5952\n",
            "Step 300 avg train loss = 4.6278\n",
            "Step 400 avg train loss = 4.6466\n",
            "Step 500 avg train loss = 4.6292\n",
            "Step 600 avg train loss = 4.6288\n",
            "Step 700 avg train loss = 4.6286\n",
            "Step 800 avg train loss = 4.6309\n",
            "Step 900 avg train loss = 4.6270\n",
            "Step 1000 avg train loss = 4.6611\n",
            "Step 1100 avg train loss = 4.6526\n",
            "Step 1200 avg train loss = 4.6538\n",
            "Step 1300 avg train loss = 4.6346\n",
            "Step 1400 avg train loss = 4.6592\n",
            "Step 1500 avg train loss = 4.6529\n",
            "Step 1600 avg train loss = 4.6495\n",
            "Step 1700 avg train loss = 4.6491\n",
            "Step 1800 avg train loss = 4.6481\n",
            "Step 1900 avg train loss = 4.6594\n",
            "Step 2000 avg train loss = 4.6472\n",
            "Step 2100 avg train loss = 4.6552\n",
            "Step 2200 avg train loss = 4.6634\n",
            "Step 2300 avg train loss = 4.6580\n",
            "Step 2400 avg train loss = 4.6653\n",
            "Validation loss after 10 epoch = 5.3153\n",
            "Step 0 avg train loss = 4.6411\n",
            "Step 100 avg train loss = 4.5584\n",
            "Step 200 avg train loss = 4.5436\n",
            "Step 300 avg train loss = 4.5719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-8114e8b6de41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfunetAO9QlT",
        "colab_type": "code",
        "outputId": "f9899005-ace4-4685-dc05-71233a219567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of LSTM model')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VFXex/HPL4X0AkkoKRhKKKGH\nwKICioACoqKrCKtrXVldXRX3eWzrs5Z1d3WLivroLq6NRwUUC3ZXsIEKEiD0ktBTIAkQIAnp5/nj\n3oQBUiYhk8lMfu/Xa14zc+65d85M4HvvnHvnHDHGoJRSynv5uLsBSimlXEuDXimlvJwGvVJKeTkN\neqWU8nIa9Eop5eU06JVSystp0CtVDxE5V0QyRKRIRKa5uz3uJCJGRHo7Ue98EclqjTYp52nQtyMi\nsltEjtvBdUBEXhORUHvZNyJSai8rEJH3RKSbvew1EXncva13i8eA540xocaYD05daH+eE+paUUQe\nFJFd9ueZJSIL7fJNdlmRiFQ5fOZF9jo32KH69Cnbu8wuf80Vb1R5Nw369ucSY0wokAKkAg85LLvD\nXtYHiASermN9lxMRP3e8bh3OAjY1dSURuR74JTDB/jxTgaUAxpgB9o4jFFiG/Znbtz/bm9gBTD/l\nc7ge2H4G70W1Yxr07ZQxJhv4DBhYx7JDwLt1LWuMiIwWkR9EpFBE9onIDXb5NyLyK4d6N4jIcofn\nRkRuF5EMIENEXhSRv5+y7cUico/9OFZE3hWRfPvI+U6HeiNFJE1EjtrfXJ5qoL23iEimiBwSkQ9F\nJNYu3wH0BD6yj7YDmvAxjAC+MMbsADDG7DfGzG3C+vuBDcBFdls6AecAHzbwPs63vzncKyJ5IpIr\nItNEZIqIbLff34MO9QNE5BkRybFvzzi+RxH5b3sbOSJy0ymvFSAifxeRvfbn+08RCWrC+1OtTIO+\nnRKRBGAKsLaOZdHAz+ta1sg2z8LaeTwHxABDgfQmbGIa8DMgGZgPXC0iYm+7I3AhsEBEfICPgHVA\nHDAeuFtELrK3MweYY4wJB3oBb9fT3guAvwDTgW7AHmABgDGmF7AX+xuQMaasCe9jBXCdHZapIuLb\nhHVrzAOusx/PABYDjbWhKxCI9Zn8AXgJuBYYDowB/kdEeth1fw+MwvobDQFGYn+7E5FJwH8BE4Ek\n4NTuqSewvvUNBXo7vJ5qozTo258PRKQQWA58C/zZYdmz9rJ1QC5wTxO3/QtgiTFmvjGmwhhz0BjT\nlKD/izHmkDHmOFa3hsEKKIArgR+NMTlYR8wxxpjHjDHlxpidWKE2w65bAfQWkWhjTJExZkU9r3cN\n8IoxZo0d5A8AZ4tIYhPafBpjzBvAb7GOyL8F8kTkviZu5n3gfBGJwAr8eU6sUwH8yRhTgbXDisba\n4R0zxmwCNmOFOljv/TFjTJ4xJh94FKu7Cawd36vGmI3GmGLgkZoXsHe8s4DZ9t/qGNa/oZrPXrVB\nGvTtzzRjTKQx5ixjzG/sUK1xp70szhhzjR0ATZGA1b/cXPtqHhhrtL0FwEy76BfAm/bjs4BYu3uo\n0N45PQh0sZffjHXEuVVEVonI1HpeLxbrKL7mNYuAg1hHqGfEGPOmMWYC1rmOW4E/OnzjcGb948An\nWEfZUcaY751Y7aAxpsp+XPN3PeCw/DgQaj8+6b3bj2Mdlu07ZVmNGCAYWO3w2X9ul6s2SoNetaR9\nWF0ldSnGCogaXeuoc+pQqvOBK+0uoZ9hnTeoeZ1d9k6p5hZmjJkCYIzJMMbMBDoDTwKLRCSkjtfL\nwdppAGDXiQKyG3qTTWF/s3kHWE/Tz3nMA34HvNFS7XFw0nsHuttlYH2bSzhlWY0CrB3GAIfPPsI+\nuazaKA165SxfEQl0uHWoo86bwAQRmS4ifiISJSJD7WXpwBUiEizW9dg3N/aCxpi1WMHyb6yTm4X2\nop+AYyJyn4gEiYiviAwUkREAInKtiMQYY6qBmnWq63iJ+cCNIjLUPhH5Z2ClMWa3U5+Ixf+Uz8XP\nPtF8sYiEiYiPiEwGBgArm7BdsLp9JmKd82hp84GHRCTGPifzB07sUN4GbhCRZBEJBh6uWcn+TF8C\nnhaRzgAiEteUbyuq9WnQK2fdj3UkV3P76tQKxpi9WCd4fwccwgr3mj7hp4FyrK6E1znRDdOYt7BO\nBr7l8DpVwFSsk4G7OLEziLCrTAI2iUgR1onZGad0UdVsZwnwP1jfFHKxvo00ta/5U07+XB4BjmJ1\nJe3F2tH8FbjNGLO8nm3UyViW2ldBtbTHgTSsbxobgDV2GcaYz4BnsP7GmZz+t77PLl8hIkeBJUBf\nF7RRtRDRiUeUUsq76RG9Ukp5OQ16pZTychr0Sinl5TTolVLKy7WJwaOio6NNYmKiu5uhlFIeZfXq\n1QXGmEZ/rNYmgj4xMZG0tDR3N0MppTyKiOxpvJZ23SillNfToFdKKS+nQa+UUl6uTfTRK6W8X0VF\nBVlZWZSWlrq7KR4nMDCQ+Ph4/P39m7W+Br1SqlVkZWURFhZGYmIi9nwyygnGGA4ePEhWVhY9evRo\nfIU6aNeNUqpVlJaWEhUVpSHfRCJCVFTUGX0T0qBXSrUaDfnmOdPPzaODPjOviMc+2kx5ZV1DjSul\nlIImBL09ucNaEfnYft5DRFaKSKaILKyZiMKeIX6hXb7yTOffbMi+QyW88v0uvtp6oPHKSql27eDB\ngwwdOpShQ4fStWtX4uLiap+Xl5c7tY0bb7yRbdu2ubSd8fHxFBYWNl6xCZpyRH8XsMXh+ZPA08aY\n3sBhTswYdDNw2C5/2q7nEmP7xNA1PJCFq/Y1Xlkp1a5FRUWRnp5Oeno6t956K7Nnz6593qGDNWGa\nMYbq6vp7CF599VX69vW8OVacCnoRiQcuxprFp2Ym+AuARXaV14Fp9uPL7OfYy8eLizrmfH2E6anx\nfLs9n5zC0yYQUkqpRmVmZpKcnMw111zDgAEDyM3NZdasWaSmpjJgwAAee+yx2rqjR48mPT2dyspK\nIiMjuf/++xkyZAhnn302eXl5p237oYce4vrrr2fUqFEkJSXxyiuvALBkyRLGjRvH5MmT6du3L7ff\nfjuunATK2csrnwHuBcLs51FAoTGm0n6eBcTZj+OwZ5A3xlSKyBG7foHjBkVkFjALoHt3x7mHm+aq\n1ASe/SqTRauzuHN8UrO3o5RqPY9+tInNOUdbdJvJseE8fMmAZq27detW5s2bR2pqKgBPPPEEnTp1\norKyknHjxnHllVeSnJx80jpHjhzhvPPO44knnuCee+7hlVde4f777z9t2xs2bOCHH37g6NGjpKSk\ncPHFFwOwcuVKNm/eTEJCAhMnTmTx4sVMmzbttPVbQqNH9CIyFcgzxqxuyRc2xsw1xqQaY1JjYhod\nfK1eCZ2CGd07mrfT9lFdrdMiKqWarlevXrUhDzB//nxSUlJISUlhy5YtbN68+bR1goKCmDx5MgDD\nhw9n9+7ddW572rRpBAYG0rlzZ8aOHcuqVasAGDVqFImJifj6+jJjxgyWL2/SlMJN4swR/bnApSIy\nBQgEwrEmXI4UET/7qD4eyLbrZwMJQJaI+GFN2HywxVvuYPqIBO6cv5YfdhxkdFK0K19KKdUCmnvk\n7SohISG1jzMyMpgzZw4//fQTkZGRXHvttXVew17Trw/g6+tLZWXlaXXg9Esja57XV+4KjR7RG2Me\nMMbEG2MSgRnAV8aYa4CvgSvtatcDi+3HH9rPsZd/ZVw8A/mFyV2IDPZnwaq9rnwZpVQ7cPToUcLC\nwggPDyc3N5cvvvjijLb3wQcfUFZWRn5+PsuWLav95rBixQr27t1LVVUVb7/9NqNHj26J5tfpTK6j\nvw+4R0QysfrgX7bLXwai7PJ7gNM7rVpYoL8vlw+L4z+bDnC42LnLpJRSqi4pKSkkJyfTr18/rrvu\nOs4999wz2t7AgQM577zzOOecc3j00Ufp0qULACNHjuTWW28lOTmZvn37cumll7ZE8+skLj7Ydkpq\naqo504lHtu4/yqRnlvGHqcncNLp540EopVxny5Yt9O/f393NaFUPPfQQ0dHR3H333SeVL1myhOef\nf54PPvjA6W3V9fmJyGpjTGo9q9Ty6F/GOurXNZwhCZEsXLXPpZcpKaWUp/Gq0SuvTk3gwfc3sC7r\nCEMTIt3dHKVUO/f444/XWT5hwgQmTJjQau3wmiN6gEuGdCPI35eFelJWKaVqeVXQhwX6M3VwNz5M\nz6G4rO5LnZRSqr3xqqAHuHpEAsXlVXyyIdfdTVFKqTbB64J++Fkd6RUTwts60JlSSgFeGPQiwtUj\nEkjbc5jMvGPubo5Sqo0YN27caT9+euaZZ7jtttsaXC80NBSAnJwcrrzyyjrrnH/++ZzpJeIA33zz\nDVOnTj3j7ZzK64Ie4IqUePx8RIcvVkrVmjlzJgsWLDipbMGCBcycOdOp9WNjY1m0aFHjFdsgrwz6\n6NAAJvTvwntrsnX2KaUUAFdeeSWffPJJ7SQju3fvJicnhzFjxlBUVMT48eNJSUlh0KBBLF68+LT1\nd+/ezcCBAwE4fvw4M2bMoH///lx++eUcP173MOmJiYnce++9DBo0iJEjR5KZmQnADTfcwK233kpq\naip9+vTh448/dtG7tnjVdfSOrh6ZwOeb9rN0ywEmD+rm7uYopRx9dj/s39Cy2+w6CCY/Ue/iTp06\nMXLkSD777DMuu+wyFixYwPTp0xERAgMDef/99wkPD6egoIBRo0Zx6aWX1jvQ2IsvvkhwcDBbtmxh\n/fr1pKSk1Pu6ERERbNiwgXnz5nH33XfXhvru3bv56aef2LFjB+PGjavdCbiCVx7RA4xNiqFbRCAL\n07T7Rillcey+cey2Mcbw4IMPMnjwYCZMmEB2djYHDtQ/Rel3333HtddeC8DgwYMZPHhwg69Zc//j\njz/Wlk+fPh0fHx+SkpLo2bMnW7duPeP3Vx+vPaL39RGuGh7Pc19nklN4nNjIIHc3SSlVo4Ejb1e6\n7LLLmD17NmvWrKGkpIThw4cD8Oabb5Kfn8/q1avx9/cnMTGxzqGJm8PxW0F9j+t63pK89ogerNmn\nAN5Jy3JzS5RSbUFoaCjjxo3jpptuOukk7JEjR+jcuTP+/v58/fXX7Nmzp8HtjB07lrfeeguAjRs3\nsn79+nrrLly4sPb+7LPPri1/5513qK6uZseOHezcudOlc9F67RE9WLNPndvLmn3qtxf0xsfHdXtM\npZRnmDlzJpdffvlJV+Bcc801XHLJJQwaNIjU1FT69evX4DZuu+02brzxRvr370///v1rvxnU5fDh\nwwwePJiAgADmz59fW969e3dGjhzJ0aNH+ec//0lgYOCZv7l6eM0wxfX5aF0Ov52/lv+7eSRjkpo/\nZaFS6sy0x2GKExMTSUtLIzr65JnvbrjhBqZOnVrvdfl10WGKG3DhgJrZp/SkrFKqffLqrhuAAD9r\n9qk3V+zlUHE5nUI6NL6SUkq1gPomDH/ttddatR1ef0QP1kBn5VXVvL82u/HKSimXaQtdxZ7oTD+3\ndhH0NbNPva2zTynlNoGBgRw8eFD/DzaRMYaDBw+e0clar++6qTFjRAIPvLeB9H2FDOve0d3NUard\niY+PJysri/z8fHc3xeMEBgYSHx/f7PXbTdBfMiSWP368mYWr9mnQK+UG/v7+9OjRw93NaJfaRdcN\nQGiAHxcP6sZH63T2KaVU+9Jugh5gxkh79qn1OvuUUqr9aFdBn9Ldmn1KBzpTSrUnjQa9iASKyE8i\nsk5ENonIo3b5ayKyS0TS7dtQu1xE5FkRyRSR9SJS//idrUxEmDGiO6v3HCbjgM4+pZRqH5w5oi8D\nLjDGDAGGApNEZJS97L+NMUPtW7pdNhlIsm+zgBdbutFn4vKUOPx9dfYppVT70WjQG0uR/dTfvjV0\nIexlwDx7vRVApIi0mZk/amefWquzTyml2gen+uhFxFdE0oE84EtjzEp70Z/s7pmnRSTALosDHA+X\ns+yyU7c5S0TSRCStta+rvXpEAoeKy1mypf6JBZRSyls4FfTGmCpjzFAgHhgpIgOBB4B+wAigE3Bf\nU17YGDPXGJNqjEmNiWndUSXHJMUQGxGo3TdKqXahSVfdGGMKga+BScaYXLt7pgx4FRhpV8sGEhxW\ni7fL2gxfH+HK1AS+y8gnu7DuSX2VUspbOHPVTYyIRNqPg4CJwNaafnex5r+aBmy0V/kQuM6++mYU\ncMQY0+YuXL9quPVz4nf0UkullJdz5oi+G/C1iKwHVmH10X8MvCkiG4ANQDTwuF3/U2AnkAm8BPym\nxVvdAhI6BTO6dzTvpGVRVa2DLCmlvFejY90YY9YDw+oov6Ce+ga4/cyb5npXj0jgjrfW8n1mAWP7\n6OxTSinv1K5+GXuqicld6BjsrydllVJezbODvvQI/PgCNHN8a2v2qXj+s3k/h4rLW7hxSinVNnh2\n0G/9FL54ANYvbPYmrh6RQEWV4b01WS3YMKWUajs8O+gHXw1xqfDlH6D0aLM20bdrGEMTInk7TWef\nUkp5J88Oeh8fmPJXKDoA3/2t2ZuZMSKB7QeKWLuvsAUbp5RSbYNnBz1A3HAYdi2seBEKMpq1ialD\nYgnu4MvCn/SkrFLK+3h+0AOMfwT8g+Cz+5p1YjY0wI+pg7vx0focinT2KaWUl/GOoA+NgfMfgB1L\nYdtnzdrE1SO6U1JexSfrc1q4cUop5V7eEfQAI2+BmH7WVTgVpU1ePaV7JL07h+o19Uopr+M9Qe/r\nD5OfhMO74Yfnmry6NftUAmv2FrJdZ59SSnkR7wl6gJ7nQ/9LYdk/4EjTr4u/fJjOPqWU8j7eFfQA\nF/3Juv/PQ01eNSo0gInJXXh/bTZllVUt3DCllHIP7wv6yO4wejZseh92fdfk1a8e0d2afWpzngsa\np5RSrc/7gh7g3DutwP/sPqhq2uWSo3tHW7NP6Tj1Sikv4Z1B7x8EF/0Z8jZD2stNWtXXR7gqNYFl\nGflkHS5xUQOVUqr1eGfQA/SbCj3Hwdd/guKCJq16VWrN7FM60JlSyvN5b9CLWJdblhfD0seatGp8\nR2v2qUWrdfYppZTn896gB4jpCz+7FdbMg+w1TVr16hEJZBceZ3lm074NKKVUW+PdQQ9w3n0QEgOf\n3QvV1U6vdmL2qb0ubJxSSrme9wd9YDhMeASyVsH6BU6vFuDnyxUp8Xy5+QAHi8pc1jyllHI17w96\ngCEzIX4EfPlwkyYoqZl96v212S5snFJKuVb7CHofH+vEbHE+fPuk06v16RLGsO6RLFyls08ppTxX\n+wh6ODFBycp/Qv42p1e7OjWBjLwi1uzV2aeUUp6p/QQ9wPiHwT8EPr/f6QlKamef0pOySikP1WjQ\ni0igiPwkIutEZJOIPGqX9xCRlSKSKSILRaSDXR5gP8+0lye69i00QWgMjHsQdnwFWz9xbpUAPy4Z\nHMvH63N19imllEdy5oi+DLjAGDMEGApMEpFRwJPA08aY3sBh4Ga7/s3AYbv8abte2zHiZojpb09Q\nctypVaaPSKCkvIqP1+nsU0opz9No0BtLkf3U374Z4AJgkV3+OjDNfnyZ/Rx7+XgRkRZr8Zny9Ycp\nf4XCvU5PUJLSPZKkzqE60JlSyiM51UcvIr4ikg7kAV8CO4BCY0xNX0YWEGc/jgP2AdjLjwBRLdno\nM9ZjLCRPg2VPWYHfCBHh6hEJrN1byLb9OvuUUsqzOBX0xpgqY8xQIB4YCfQ70xcWkVkikiYiafn5\n+We6uaa78HHr3skJSnT2KaWUp2rSVTfGmELga+BsIFJE/OxF8UDNr4qygQQAe3kEcLCObc01xqQa\nY1JjYmKa2fwzEJkAY+6BzYth57eNVo8KDeDC5K68vzZLZ59SSnkUZ666iRGRSPtxEDAR2IIV+Ffa\n1a4HFtuPP7SfYy//yrTVXxudcydEnmVPUFLRaPXpIxI4XFLBl5sPtELjlFKqZThzRN8N+FpE1gOr\ngC+NMR8D9wH3iEgmVh98zQwfLwNRdvk9wP0t3+wW4h9oTVCSvwVW/bvR6qN7RxMXGaTdN0opj+LX\nWAVjzHpgWB3lO7H6608tLwWuapHWtYZ+F0OvC+Drv8DAK61r7evh6yNcOTyeZ7/KYN+hEhI6Bbdi\nQ5VSqnna1y9j6yICk56EimJY+mij1Wtnn1qts08ppTyDBj1ATB9rgpK1b0D26garxncMZkxSDIvS\n9unsU0opj6BBX+O8+yC0M3za+AQlV6cmkHOklGUZbrgsVCmlmkiDvkZgOEx4FLLTYN38BqtOSO5M\nVEgHnvx8GyXlOv6NUqpt06B3NPhqa4KSJQ9D6ZF6qwX4+fL3q4awdf9R/vud9TpWvVKqTdOgd+Tj\nA1P+BsUF8O1fG6w6rl9n7p/Uj0825PL8V5mt1ECllGo6DfpTxQ6DlOucmqBk1tieXD4sjn98uZ3P\nN+5vpQYqpVTTaNDXZfwfoEMIfHZvgxOUiAh/uWIQQxIiueftdLbud34+WqWUai0a9HUJiYZxv4ed\n38DWjxusGujvy9xfDic0wI9fvZ7GoeLy1mmjUko5SYO+Pqk3Q+dk+OLBRico6RIeyNzrUsk7VsZt\nb6ymoqrhyzOVUqo1adDXx9cPJtsTlHw/p9HqQxMieeKKQazcdYhHP9rUCg1USinnaNA3pMcYGHA5\nLH/aqQlKrkiJ59dje/LGir28sWJPKzRQKaUap0HfmAsfBwS++L1T1e+d1I/z+8bwyIebWLHztGH4\nlVKq1WnQNyYiHsb8DrZ8aJ2cbYSvj/DszGF0jwrmtjdWs+9QievbqJRSDdCgd8Y5v4WOiU5PUBIe\n6M+/r0ulqtpwy7w0ist0mASllPto0DvDPxAu+gvkb4WfXnJqlZ4xoTz/ixS2HzjGPW+nU60jXSql\n3ESD3ll9J0Ov8fDNX6Aoz6lVxvaJ4fcXJ/PFpgM8szTDxQ1USqm6adA7SwQmPwkVJU5NUFLjpnMT\nuWp4PM8uzeCT9bkubKBSStVNg74popNg1G3WBCVZDU9QUkNEePzygaR0j+S/3lnHppz6R8VUSilX\n0KBvqrH3QmgX+PS/Gp2gpEaAny///OVwIoP9mTVvNQVFZS5upFJKnaBB31SB4TDxMchZAz/MaXDQ\nM0edwwKZ+8tUCoqsYRLKK3WYBKVU69Cgb45B06HPJFjyCCy4Boqcm1JwUHwEf7tqCKt2H+YPizfq\nhCVKqVahQd8cPj4wYz5c+CfI/BJePBu2furUqpcOieX2cb1YsGof837UYRKUUq6nQd9cPj5wzh0w\n61sI7QoLZsKHv4WyY42u+ruJfZnQvzOPfbyZ7zMLWqGxSqn2TIP+THVJhluWwujZsOb/4MVzYe+K\nBlfx8RGevnoovWJC+M2ba9hzsLiVGquUao8aDXoRSRCRr0Vks4hsEpG77PJHRCRbRNLt2xSHdR4Q\nkUwR2SYiF7nyDbQJfgEw4RG48TPr+auTrf77yvonIQkL9Oel61IRgV+9nsax0saHVlBKqeZw5oi+\nEvidMSYZGAXcLiLJ9rKnjTFD7dunAPayGcAAYBLwgoj4uqDtbc9ZZ8Nt38PQa6yhjf99AeRtqb96\nVAgv/CKFnQXFzF6owyQopVyj0aA3xuQaY9bYj48BW4C4Bla5DFhgjCkzxuwCMoGRLdFYjxAQBpc9\nb52sPZoL/zoPfvzfeq+5P6d3NH+YmsySLXn848uGJyNXSqnmaFIfvYgkAsOAlXbRHSKyXkReEZGO\ndlkcsM9htSzq2DGIyCwRSRORtPx85y5P9Cj9psBvVkDv8dZ0hPMuhcJ9dVa97uyzmDkygf/9egeL\n07NbuaFKKW/ndNCLSCjwLnC3MeYo8CLQCxgK5AL/aMoLG2PmGmNSjTGpMTExTVnVc4TGwIy34NLn\nIGctvHgOrFt42o+sRIRHLx3IiMSO3LtoPRuydJgEpVTLcSroRcQfK+TfNMa8B2CMOWCMqTLGVAMv\ncaJ7JhtIcFg93i5rn0Qg5Tq4dbk12fj7s+Cd66Hk0EnVOvj58OK1w4kODeCWeWnkHSt1U4OVUt7G\nmatuBHgZ2GKMecqhvJtDtcuBjfbjD4EZIhIgIj2AJOCnlmuyh+rUA278FMY/bP246oWzIWPJSVWi\nQwOYe91wjhyv4Nf/t5qyyio3NVYp5U2cOaI/F/glcMEpl1L+VUQ2iMh6YBwwG8AYswl4G9gMfA7c\nbozRxALw8YUx98AtX0FQR3jz5/DxPVB+4jr6AbERPDV9CGv3FvL793WYBKXUmZO2ECSpqakmLS3N\n3c1oXRWl8NUfrStyOvWEK+ZCfGrt4qe/3M6cpRk8dHF/fjWmpxsbqpRqq0RktTEmtbF6+stYd/EP\nhIv+BNd/CJVl8PKF8PWfa+ekvWt8EpMGdOXPn27hu+1eeFWSUqrVaNC7W4+x8JsfYNBV8O2T8PJE\nyN+Oj4/wj+lD6NMljDveWsPO/CJ3t1Qp5aE06NuCwAi44l9w1etweDf8awysnEuIvw8vXZeKn68P\nv5qXxlEdJkEp1Qwa9G3JgGnWj6wSR8Nn/w1vXEGCXyEvXJPC3oMl3Dl/LVU6TIJSqok06NuasK5w\nzSK4+CnYtxJeOJtRJd/w6GUD+GZbPn/9fKu7W6iU8jAa9G2RCIy4GX69DKJ6waKbuGbfY9yS2pF/\nfbeT99ZkubuFSikP4ufuBqgGRPeGm/4Dy5+Cb57gwdAfMHF3cO8iIf9YGbeM6YmPj7i7lUqpNk6P\n6Ns6Xz8471741ZdIhxAeOvggr0S/xaLPl3D9qz+Rd1SHSlBKNUx/MOVJyktgycOYVf9GTDVbTXf+\n4zOG1Km3cM7wYe5unVKqlTn7gykNek9UlAeb3uf4mgUEHVgDwN7QIXQb80v8B14BIVFubqBSqjVo\n0LcTZXk7WP7+P+me/QlJPtkYHz+k1wUwaDr0nQwBoe5uolLKRTTo25mlm/czd9FHTKz8jpnBPxFS\nuh/8g6HvFOtXt70uAL8O7m6mUqoFadC3Q3lHS5n9djo/ZOZzR+8C7ohOJ2DbYjh+2BotM3maFfrd\nzwYfPQ+vlKfToG+nqqsN//puJ//4zza6hAfy7FXJDK9aBxvega2fQEUJhMfBwJ9bod91kHXdvlLK\n42jQt3Pp+wq5a8Fa9h0q4c5MrUqtAAAVTklEQVTxSdwxrjd+Vcdh22dW6GcugepKiO5rBf6gn1vD\nJSulPIYGvaKorJI/fLCR99ZmMyKxI8/MGEZcZJC1sOQQbP4A1r8De3+wyuJSrdAfcDmEdXFfw5VS\nTtGgV7U+WJvNQx9sxEfgiZ8PZsqgbidXKNwHG9+FDYvgwAYQH+hxnhX6/adao2sqpdocDXp1kj0H\ni7lzQTrr9hUyc2QC/zM1meAOdYyAkbfFCvwN70DhHvANgD4XWaGfdKE1YYpSqk3QoFenqaiq5qkv\nt/PPb3fQMzqE52amkBwbXndlYyArzQr8Te9BcT4EhFtX7MQOhW5DIXYYhHere32llMtp0Kt6fZ9Z\nwOyF6RSWVPDAlH7ccE4i0tCVN1WVsOtb2PS+Ff4F28BUW8tCu5wI/ZodgIa/Uq1Cg1416FBxOfcu\nWseSLXlc0K8zf7tyMFGhAc6tXF4M+zdATjrkrIXcdCjY7hD+XR2O+u2dQFhX170ZpdopDXrVKGMM\n837cw58+3UJEkD9PTR/CmKSY5m2sNvzXWjuA3HTI3wbY/75qwj922IkdgIa/UmdEg145bUvuUe6c\nv5aMvCJ+PbYnv7uwLx38WuCXs2VFVvjnpp/YARRspzb8w7qdfNTfbahe1qlUE2jQqyY5Xl7F459s\n5s2VexkcH8GzM4aRGB3S8i9UVgT715846s9ZCwUZnBT+jkf9Gv7KG1RXQWUZVJY63OznwdEQmdCs\nzbZY0ItIAjAP6IL1v3GuMWaOiHQCFgKJwG5gujHmsFhn9eYAU4AS4AZjzJqGXkODvu34fGMu9727\ngcqqav44bSBXpMS7/kXLjp3e7XNS+MdC534Q0hlCoiEkxuEWfeLeP8j1bVWerboaKo9DxXFrOJDy\nEuu+4rhD+B53CGWH+4q6yktPr1dZChWnlFdX1N+m0bNhwiPNejstGfTdgG7GmDUiEgasBqYBNwCH\njDFPiMj9QEdjzH0iMgX4LVbQ/wyYY4z5WUOvoUHftuQUHufuhen8tOsQ04bG8sdpAwkL9G/dRpQd\ng9z19lF/OhzMgOKDUJxn/cepS4ewU3YE0XU8tx8HdbJm71JthzFQVW6d76moCWP7cV1ltUFdX5lD\niNesX3m8+e3z7QB+geAXAH5B9n3gyff+dZUHnl7PL9D6TYpfIET1huikZjXJZV03IrIYeN6+nW+M\nybV3Bt8YY/qKyL/sx/Pt+ttq6tW3TQ36tqeq2vDC15k8szSDuMgg5swYyrDuHd3dLCsMyout6/qL\nC+x7+1Zy0OF5wYl7U1XHhgSCO9W9EwiJtr5O1zzuEGL/xwyy7nUQuMbV/J1KDtq3Q3D8kMNzu8zx\n/vghK+ibRKy/j3+QNSy3fzB0sO9PK3N4XrPcsW5d4V0Txr4BbXLEV5cEvYgkAt8BA4G9xphIu1yA\nw8aYSBH5GHjCGLPcXrYUuM8Yk3bKtmYBswC6d+8+fM+ePU63Q7We1XsOcef8dA4cLWX2xD7cel4v\nfD1pQvLqaigtPHmHUFxwyk7C4XFpYePb9A2wAyDIyXuHncRJ9wENr+vXAcQXfPzAx74Xn9bf0Rhj\nHRmfFNKHT3luB3WJQ5jXG9r2TjY4yvpmFRxlP+9k/SjvpLB2COi6AtwvoF3veJ0Neqe/u4pIKPAu\ncLcx5qjjD2yMMUZEmvTVwBgzF5gL1hF9U9ZVrWf4WZ349K4x/P79Dfzti20sTs/mrvF9mDywKz6e\nEPg+PidCJKZv4/Ury098MyixdwgVJXaf6/FG7kuh9ChU5p3o83W8p4X+mYvvieCvCf+TdgY1yxt6\n7md9Nic9t+sAHC88JbTL6muMNddBcJR1izzLOole8/ykMLcDPTDixOuoVuFU0IuIP1bIv2mMec8u\nPiAi3Ry6bvLs8mzA8RRyvF2mPFREkD/PzRzG5IHdeHrJdm5/aw19uoRy5/gkpgzs5hmB7yy/DtYv\ne1v61701/c917QBqTuiduvOoLLe6naqrrCGlTbV1X11ZR1nN85r6DT2vtE9KloM57rB+lbXcGAiK\ntK4EiR1SR1hHnTgi19D2CI0Gvd0t8zKwxRjzlMOiD4HrgSfs+8UO5XeIyAKsk7FHGuqfV55BRLh4\ncDcmDezKJxtyeXZpBne8tZY+XTK8M/BbmojdVePkr4+VakHOXHUzGlgGbADs37jzILASeBvoDuzB\nurzykL1jeB6YhHV55Y2n9s+fSk/Gep6qasOnG3KZszSDzLwi7z3CV6oN0x9MqVZxauAndQ7lrgka\n+Eq1Bg161apqAv/ZpRlk2IF/5/gkpgzq5llX6SjlQTTolVtUVxs+3ZjLnCUa+Eq5mrNB3/Z+AaA8\nmo+PMHVwLF/cPZbnfzEMgN/OX8ukZ77jo3U5VFW7/8BCqfZGg165hAa+Um2HBr1yqfoC/6JnvuND\nDXylWoX20atWVV1t+GzjfuYs3c72A0X0tvvwL9Y+fKWaTE/GqjZNA1+pM6dBrzyCBr5SzadBrzzK\nqYHfKyaEO8cnMXVwrAa+UvXQoFceqbra8Pmm/cxZksG2A8c08JVqgAa98mh1Bf5dE/owdZAOraBU\nDf3BlPJoPj7ClEHd+OyuMbxwTQq+PsKd89cyac53fLohl2q9LFMpp2nQqzatJvA/v2ssz80cRlW1\n4TdvrmHKs8v4fON+2sI3UqXaOg165RF8fIRLhsTyn9nnMWfGUMorq7n1jdVMfW45SzYf0MBXqgHa\nR688UmVVNYvTc3j2qwz2HCxhcHwEsyf04fy+MUg7nkNUtS96Mla1C5VV1by3Nptnl2aQdfg4QxMi\nmT2xD2OTojXwldfToFftSkVVNe+uzuK5rzLJLjzO8LM6cs/EPpzTK0oDX3ktDXrVLpVXVvN22j7+\n9+tMco+UMrJHJ+6Z2IdRPaPc3TSlWpwGvWrXSiuqWLjKCvy8Y2Wc3TOKey7sw4jETu5umlItRoNe\nKazAf2vlXl74ZgcFRWWMSYrm7gl9GH5WR3c3TakzpkGvlIPj5VW8uXIPL36zg4PF5ZzXJ4bZE/sw\nNCHS3U1Tqtk06JWqQ0l5JfN+3MO/vt3B4ZIKxvfrzN0T+jAoPsLdTVOqyTTolWpAUVklr/+wm7nf\n7eTI8QomJnfh7glJDIjVwFeeQ4NeKSccK63g1e9389KynRwrrWTywK7cNSGJfl3D3d00pRrVYoOa\nicgrIpInIhsdyh4RkWwRSbdvUxyWPSAimSKyTUQuav5bUMr1wgL9uXN8Esvvu4C7xiexPKOASc8s\n4/a31pBx4Ji7m6dUi2j0iF5ExgJFwDxjzEC77BGgyBjz91PqJgPzgZFALLAE6GOMqWroNfSIXrUV\nhSXlvLx8F68s30VJRRVTB8cyMbkLg+IiOKtTsA6RrNoUZ4/o/RqrYIz5TkQSnXzdy4AFxpgyYJeI\nZGKF/o9Orq+UW0UGd+B3F/blxnN78NKyncz7YTcfrcsBICzAjwFx4QyKi2CgfesRFaLhr9q8RoO+\nAXeIyHVAGvA7Y8xhIA5Y4VAnyy47jYjMAmYBdO/e/QyaoVTL6xTSgfsm9eOeiX3YfuAYG7OPsCH7\nCBuyj/L6j3sor6wGIDTAj+RYK/xrdgA9ozX8VdvS3KB/EfgjYOz7fwA3NWUDxpi5wFywum6a2Q6l\nXMrf14cBsREMiI3g6hFWWUVVNRkHimrDf2POEd5YsYcyO/xDOviSHBvOQDv8B8VF0DMmVKdCVG7T\nrKA3xhyoeSwiLwEf20+zgQSHqvF2mVJew9/Xh+TYcJJjw5k+wvrnXllVTWZ+ERuyjtTuAOb/tJdX\nK6zwD+7gS3K3E+E/MC6CXjEh+PnqlBDK9ZoV9CLSzRiTaz+9HKi5IudD4C0ReQrrZGwS8NMZt1Kp\nNs7P14d+XcPp1zWcq1JPhP+O/GLrqN8O/4Wr9vHaD7sBCPT3IbnbiT7/QfER9I4J1fBXLc6Zq27m\nA+cD0cAB4GH7+VCsrpvdwK9rgl9Efo/VjVMJ3G2M+ayxRuhVN6q9qKo27Mwvsvv7rR3AppyjlJRb\nF6YF+PnQ3w7/lLMiObdXNJ3DA93catVW6Q+mlPIQVdWGXQV2+GcdtcP/CMV2+PftEsbopGhGJ0Xz\nsx6dCO5wJtdQKG+iQa+UB6uqNmzJPcqyjAKWZ+azavdhyiur8fcVUrp3ZExSNKOTYhgUF6Enedsx\nDXqlvMjx8ipW7T7E95kFLMsoYHPuUQAigvw5p1eUdcTfO5qzokLc3FLVmjTolfJiBUVlfJ9ZwPKM\nApZnFpB7pBSAhE5BjO4dw5ikaM7pFUVkcAc3t1S5kga9Uu2EMYadBcUsz7CO9lfsPEhRWSUiMDgu\ngtFJ0ZzbO5rhZ3UkwM/X3c1VLUiDXql2qqKqmnX7ClluH/Gv3VdIVbUhyN+XkT062f370fTtEqYT\np3s4DXqlFGANxbxi5yGWZ+SzPLOAHfnFAESHBjC6dxSjk6yuni56GafHabFBzZRSni0s0J+JyV2Y\nmNwFgJzC47VH+8syCvgg3Rq0LalzqNXN0yuaIQmRxIQFuLPZqgXpEb1S7Vh1tWHr/mMsz8xnWUYB\nP+06VDtmT5fwAAbGRjAgLoKB9tg93SICtbunDdEjeqVUo3x8pHbcnllje1FaUcW6fYVszDnKJnvA\ntq+35VFtHw92CunAgNhwBsRGMDAunIGxEXTXcfrbPA16pVStQH9fftYzip/1jKotO15exZb9dvBn\nH2VT7hFeXr6Tiior/cPsoZoHxkUwwL7vGa0DtrUlGvRKqQYFdfAlpXtHUrp3rC0rr6xm+4FjbMqx\nwn9jzhHeXLmHUnu0zkB/a8yegfaR/4DYCPp0CaODn4a/O2gfvVKqRVRWVbOzoPhE+GcfYXPOUY6V\nVQLg7yv06RJ2IvzjIujfNZygDnptf3Pp5ZVKKberrjbsPVTCRjv8rZ3AEQ6XVADgI9C7c6g9uUs4\nyd3C6d0llJjQAD3p6wQ9GauUcjsfHyExOoTE6BCmDo4FrF/y5h4pZWP2kdqTvj/sKOD9tSfmKIoI\n8iepcyhJXULp3Tms9nHXcL3qpzk06JVSrUpEiI0MIjYyiAsHdK0tzz9WxvYDx8g4cIyMvCIy8or4\nfON+Dpfsq60TGuBH786htcGf1DmM3p1DiYsM0it/GqBBr5RqE2LCAogJC+Dc3tEnlR8sKqsN/kx7\nJ/DN9nzeWZ1VWyfI37d2B9Db3gEkdQ4loVOwDuOMBr1Sqo2LCg0gKjSAUQ6XfAIUlpSTae8AMg4U\nkZF3jB93HuQ9hy6gDn4+9IqxvwE4dAWdFRWMfzu6/FODXinlkSKDO5Ca2InUxE4nlR8rrajdAWTm\nFZFx4Bhr9h7mw3U5tXX8fYUe0SG1XT+9O4fSMyaEHtEhXjmDl/e9I6VUuxYW6M+w7h0Z5nDdP0BJ\neSU78orJyDtW+y1gU84RPt2Yi+PFh13DA+kRHUKPmBB6Rlvh3yM6hIROnvstQINeKdUuBHfwY1B8\nBIPiI04qL62oYmd+MbsPFrOroJid+cXsKijisw25tZeBAvj6CN07BdcGf49oe0cQE0KXsMA2fTJY\ng14p1a4F+vvWjvdzqsPF5ew6WMyufGsnsKugmJ0Fxfywo6D2V8BgnQxOjD75G0DNN4K2MMuXBr1S\nStWjY0gHOoZ0OGn4B7B+CHbgWCm78q3gr9kJbM49yueb9lNVfaIvqGOwvx3+J84D9IgOITEqpNV+\nFaxBr5RSTeTjI3SLCKJbRBDnnHI5aEVVNfsOlZz0DWBXfjHfZxbw7pqsk+rGRgRy0+ge/GpMT5e2\nV4NeKaVakL+vDz1jQukZE3rasuKyytpzATXdQa0xwUujQS8irwBTgTxjzEC7rBOwEEgEdgPTjTGH\nxfpt8hxgClAC3GCMWeOapiullGcJCfCzx/WJaLxyC3LmWqHXgEmnlN0PLDXGJAFL7ecAk4Ek+zYL\neLFlmqmUUqq5Gg16Y8x3wKFTii8DXrcfvw5McyifZywrgEgR6dZSjVVKKdV0zb36v4sxJtd+vB/o\nYj+OA/Y51Muyy04jIrNEJE1E0vLz85vZDKWUUo054595GWtA+yYPam+MmWuMSTXGpMbExJxpM5RS\nStWjuUF/oKZLxr7Ps8uzgQSHevF2mVJKKTdpbtB/CFxvP74eWOxQfp1YRgFHHLp4lFJKuYEzl1fO\nB84HokUkC3gYeAJ4W0RuBvYA0+3qn2JdWpmJdXnljS5os1JKqSZoNOiNMTPrWTS+jroGuP1MG6WU\nUqrltInJwUUkH+ubQXNEAwUt2BxPoO+5fdD33D6cyXs+yxjT6NUsbSLoz4SIpDkzC7o30ffcPuh7\nbh9a4z175ij6SimlnKZBr5RSXs4bgn6uuxvgBvqe2wd9z+2Dy9+zx/fRK6WUapg3HNErpZRqgAa9\nUkp5OY8OehGZJCLbRCRTRO5vfA3PJiIJIvK1iGwWkU0icpe729QaRMRXRNaKyMfubktrEZFIEVkk\nIltFZIuInO3uNrmSiMy2/01vFJH5IhLo7ja5goi8IiJ5IrLRoayTiHwpIhn2fceGttEcHhv0IuIL\n/C/WZCfJwEwRSXZvq1yuEvidMSYZGAXc3g7eM8BdwBZ3N6KVzQE+N8b0A4bgxe9fROKAO4FUexY7\nX2CGe1vlMq/h/EROLcZjgx4YCWQaY3YaY8qBBVgTn3gtY0xuzdSMxphjWP/56xzv31uISDxwMfBv\nd7eltYhIBDAWeBnAGFNujCl0b6tczg8IEhE/IBjIcXN7XKKJEzm1GE8OeqcnOfFGIpIIDANWurcl\nLvcMcC9Q7e6GtKIeQD7wqt1l9W8RCXF3o1zFGJMN/B3YC+RijXr7H/e2qlXVN5FTi/HkoG+3RCQU\neBe42xhz1N3tcRURqZmUfrW729LK/IAU4EVjzDCgGBd8nW8r7D7py7B2cLFAiIhc695WuUdzJ3Jq\njCcHfbuc5ERE/LFC/k1jzHvubo+LnQtcKiK7sbrmLhCRN9zbpFaRBWQZY2q+rS3CCn5vNQHYZYzJ\nN8ZUAO8B57i5Ta2pvomcWownB/0qIElEeohIB6yTNx+6uU0uJSKC1W+7xRjzlLvb42rGmAeMMfHG\nmESsv+9XxhivP9IzxuwH9olIX7toPLDZjU1ytb3AKBEJtv+Nj8eLTz7Xob6JnFpMo+PRt1XGmEoR\nuQP4Auss/SvGmE1ubparnQv8EtggIul22YPGmE/d2CblGr8F3rQPYnbixZP4GGNWisgiYA3WlWVr\n8dKhEJo4kVPLva4OgaCUUt7Nk7tulFJKOUGDXimlvJwGvVJKeTkNeqWU8nIa9Eop5eU06JVSystp\n0CullJf7f9MERIOsxlWJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H658b0MW9UJ4",
        "colab_type": "code",
        "outputId": "7ce53cfc-cded-49fa-a571-9e2972ed3b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "perp = [2**(i[1]/np.log(2)) for i in plot_cache] \n",
        "print('The minimum validation loss occurred at {} and it is {}'.format(perp.index(min(perp)),min(perp)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum validation loss occurred at 7 and it is 196.54008318910067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAM0w4gp8BvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model with hyperparamters chosen: embed_dim: 300, hidden_zise = 300\n",
        "\n",
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = LSTMModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 300\n",
        "    hidden_size = 300 # output of dimension \n",
        "    num_layers = 2\n",
        "    lstm_dropout = 0.1\n",
        "#     input_size = lookup.weight.size(1)\n",
        "    vocab_size = len(train_dict)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "\n",
        "    \n",
        "    model = LSTMModel(options).to(current_device)\n",
        "\n",
        "# same as previous nn based \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mztEAFSg8BvU",
        "colab_type": "code",
        "outputId": "e155f7e6-9541-4700-8c2e-814d7c1f264c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# verify the model \n",
        "model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmVR8WOW8BvY",
        "colab_type": "code",
        "outputId": "4cb4889e-c6ed-408f-b581-1466bf9dca08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model = LSTMModel(options).to(current_device)\n",
        "# train the model with hyperparamters chosen: embed_dim: 300, hidden_zise = 300\n",
        "\n",
        "plot_cache = []\n",
        "min_val_loss = 20 \n",
        "epoch_num = 20\n",
        "for epoch_number in range(epoch_num):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "            print(\"update the best to:\")\n",
        "            print(best_model)\n",
        "            print(\"current validation loss is:\")\n",
        "            print(min_val_loss)\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "        \n",
        "print('Saving best model...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './best_300_300_LSTM.pt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 7.0963\n",
            "Step 100 avg train loss = 7.1643\n",
            "Step 200 avg train loss = 6.9912\n",
            "Step 300 avg train loss = 6.8097\n",
            "Step 400 avg train loss = 6.6316\n",
            "Step 500 avg train loss = 6.5043\n",
            "Step 600 avg train loss = 6.4055\n",
            "Step 700 avg train loss = 6.3217\n",
            "Step 800 avg train loss = 6.2781\n",
            "Step 900 avg train loss = 6.1911\n",
            "Step 1000 avg train loss = 6.1272\n",
            "Step 1100 avg train loss = 6.0845\n",
            "Step 1200 avg train loss = 6.0255\n",
            "Step 1300 avg train loss = 6.0025\n",
            "Step 1400 avg train loss = 5.9498\n",
            "Step 1500 avg train loss = 5.9080\n",
            "Step 1600 avg train loss = 5.8947\n",
            "Step 1700 avg train loss = 5.8399\n",
            "Step 1800 avg train loss = 5.7976\n",
            "Step 1900 avg train loss = 5.8105\n",
            "Step 2000 avg train loss = 5.7670\n",
            "Step 2100 avg train loss = 5.7527\n",
            "Step 2200 avg train loss = 5.7363\n",
            "Step 2300 avg train loss = 5.6854\n",
            "Step 2400 avg train loss = 5.6774\n",
            "Validation loss after 0 epoch = 5.5204\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.520387977024295\n",
            "Step 0 avg train loss = 5.6179\n",
            "Step 100 avg train loss = 5.5519\n",
            "Step 200 avg train loss = 5.4950\n",
            "Step 300 avg train loss = 5.5017\n",
            "Step 400 avg train loss = 5.4774\n",
            "Step 500 avg train loss = 5.4747\n",
            "Step 600 avg train loss = 5.4848\n",
            "Step 700 avg train loss = 5.4545\n",
            "Step 800 avg train loss = 5.4363\n",
            "Step 900 avg train loss = 5.4233\n",
            "Step 1000 avg train loss = 5.4134\n",
            "Step 1100 avg train loss = 5.3924\n",
            "Step 1200 avg train loss = 5.3886\n",
            "Step 1300 avg train loss = 5.3621\n",
            "Step 1400 avg train loss = 5.3610\n",
            "Step 1500 avg train loss = 5.3277\n",
            "Step 1600 avg train loss = 5.3297\n",
            "Step 1700 avg train loss = 5.3323\n",
            "Step 1800 avg train loss = 5.3010\n",
            "Step 1900 avg train loss = 5.2779\n",
            "Step 2000 avg train loss = 5.3112\n",
            "Step 2100 avg train loss = 5.2738\n",
            "Step 2200 avg train loss = 5.2660\n",
            "Step 2300 avg train loss = 5.2601\n",
            "Step 2400 avg train loss = 5.2794\n",
            "Validation loss after 1 epoch = 5.2590\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.258957803474282\n",
            "Step 0 avg train loss = 5.0729\n",
            "Step 100 avg train loss = 5.0421\n",
            "Step 200 avg train loss = 5.0402\n",
            "Step 300 avg train loss = 5.0608\n",
            "Step 400 avg train loss = 5.0638\n",
            "Step 500 avg train loss = 5.0504\n",
            "Step 600 avg train loss = 5.0652\n",
            "Step 700 avg train loss = 5.0654\n",
            "Step 800 avg train loss = 5.0178\n",
            "Step 900 avg train loss = 5.0349\n",
            "Step 1000 avg train loss = 5.0329\n",
            "Step 1100 avg train loss = 5.0048\n",
            "Step 1200 avg train loss = 5.0134\n",
            "Step 1300 avg train loss = 4.9993\n",
            "Step 1400 avg train loss = 4.9877\n",
            "Step 1500 avg train loss = 4.9917\n",
            "Step 1600 avg train loss = 4.9959\n",
            "Step 1700 avg train loss = 4.9841\n",
            "Step 1800 avg train loss = 4.9921\n",
            "Step 1900 avg train loss = 4.9914\n",
            "Step 2000 avg train loss = 4.9707\n",
            "Step 2100 avg train loss = 4.9773\n",
            "Step 2200 avg train loss = 4.9577\n",
            "Step 2300 avg train loss = 4.9680\n",
            "Step 2400 avg train loss = 4.9633\n",
            "Validation loss after 2 epoch = 5.1532\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.153161520328162\n",
            "Step 0 avg train loss = 4.8499\n",
            "Step 100 avg train loss = 4.7384\n",
            "Step 200 avg train loss = 4.7116\n",
            "Step 300 avg train loss = 4.7450\n",
            "Step 400 avg train loss = 4.7599\n",
            "Step 500 avg train loss = 4.7415\n",
            "Step 600 avg train loss = 4.7406\n",
            "Step 700 avg train loss = 4.7455\n",
            "Step 800 avg train loss = 4.7482\n",
            "Step 900 avg train loss = 4.7498\n",
            "Step 1000 avg train loss = 4.7576\n",
            "Step 1100 avg train loss = 4.7450\n",
            "Step 1200 avg train loss = 4.7322\n",
            "Step 1300 avg train loss = 4.7445\n",
            "Step 1400 avg train loss = 4.7419\n",
            "Step 1500 avg train loss = 4.7139\n",
            "Step 1600 avg train loss = 4.7399\n",
            "Step 1700 avg train loss = 4.7451\n",
            "Step 1800 avg train loss = 4.7524\n",
            "Step 1900 avg train loss = 4.7283\n",
            "Step 2000 avg train loss = 4.7272\n",
            "Step 2100 avg train loss = 4.7242\n",
            "Step 2200 avg train loss = 4.7086\n",
            "Step 2300 avg train loss = 4.7202\n",
            "Step 2400 avg train loss = 4.7051\n",
            "Validation loss after 3 epoch = 5.1240\n",
            "update the best to:\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "current validation loss is:\n",
            "5.1240091377834105\n",
            "Step 0 avg train loss = 4.2403\n",
            "Step 100 avg train loss = 4.4755\n",
            "Step 200 avg train loss = 4.5189\n",
            "Step 300 avg train loss = 4.4997\n",
            "Step 400 avg train loss = 4.5024\n",
            "Step 500 avg train loss = 4.5085\n",
            "Step 600 avg train loss = 4.4997\n",
            "Step 700 avg train loss = 4.5424\n",
            "Step 800 avg train loss = 4.5209\n",
            "Step 900 avg train loss = 4.5155\n",
            "Step 1000 avg train loss = 4.5030\n",
            "Step 1100 avg train loss = 4.5263\n",
            "Step 1200 avg train loss = 4.5174\n",
            "Step 1300 avg train loss = 4.5216\n",
            "Step 1400 avg train loss = 4.5076\n",
            "Step 1500 avg train loss = 4.5216\n",
            "Step 1600 avg train loss = 4.5220\n",
            "Step 1700 avg train loss = 4.5271\n",
            "Step 1800 avg train loss = 4.5288\n",
            "Step 1900 avg train loss = 4.5162\n",
            "Step 2000 avg train loss = 4.5171\n",
            "Step 2100 avg train loss = 4.5284\n",
            "Step 2200 avg train loss = 4.5220\n",
            "Step 2300 avg train loss = 4.5491\n",
            "Step 2400 avg train loss = 4.5348\n",
            "Validation loss after 4 epoch = 5.1514\n",
            "Step 0 avg train loss = 4.2330\n",
            "Step 100 avg train loss = 4.2637\n",
            "Step 200 avg train loss = 4.2819\n",
            "Step 300 avg train loss = 4.3145\n",
            "Step 400 avg train loss = 4.2894\n",
            "Step 500 avg train loss = 4.3025\n",
            "Step 600 avg train loss = 4.3162\n",
            "Step 700 avg train loss = 4.3113\n",
            "Step 800 avg train loss = 4.3035\n",
            "Step 900 avg train loss = 4.3288\n",
            "Step 1000 avg train loss = 4.3191\n",
            "Step 1100 avg train loss = 4.3400\n",
            "Step 1200 avg train loss = 4.3778\n",
            "Step 1300 avg train loss = 4.3485\n",
            "Step 1400 avg train loss = 4.3576\n",
            "Step 1500 avg train loss = 4.3466\n",
            "Step 1600 avg train loss = 4.3487\n",
            "Step 1700 avg train loss = 4.3467\n",
            "Step 1800 avg train loss = 4.3449\n",
            "Step 1900 avg train loss = 4.3766\n",
            "Step 2000 avg train loss = 4.3859\n",
            "Step 2100 avg train loss = 4.3532\n",
            "Step 2200 avg train loss = 4.3767\n",
            "Step 2300 avg train loss = 4.3666\n",
            "Step 2400 avg train loss = 4.3729\n",
            "Validation loss after 5 epoch = 5.1989\n",
            "Step 0 avg train loss = 4.0421\n",
            "Step 100 avg train loss = 4.1299\n",
            "Step 200 avg train loss = 4.1321\n",
            "Step 300 avg train loss = 4.1553\n",
            "Step 400 avg train loss = 4.1473\n",
            "Step 500 avg train loss = 4.1434\n",
            "Step 600 avg train loss = 4.1833\n",
            "Step 700 avg train loss = 4.1568\n",
            "Step 800 avg train loss = 4.1612\n",
            "Step 900 avg train loss = 4.1660\n",
            "Step 1000 avg train loss = 4.1943\n",
            "Step 1100 avg train loss = 4.1805\n",
            "Step 1200 avg train loss = 4.1957\n",
            "Step 1300 avg train loss = 4.2071\n",
            "Step 1400 avg train loss = 4.1799\n",
            "Step 1500 avg train loss = 4.2031\n",
            "Step 1600 avg train loss = 4.1892\n",
            "Step 1700 avg train loss = 4.2055\n",
            "Step 1800 avg train loss = 4.1970\n",
            "Step 1900 avg train loss = 4.1906\n",
            "Step 2000 avg train loss = 4.2205\n",
            "Step 2100 avg train loss = 4.2090\n",
            "Step 2200 avg train loss = 4.2018\n",
            "Step 2300 avg train loss = 4.2241\n",
            "Step 2400 avg train loss = 4.2278\n",
            "Validation loss after 6 epoch = 5.2634\n",
            "Step 0 avg train loss = 3.9514\n",
            "Step 100 avg train loss = 3.9639\n",
            "Step 200 avg train loss = 3.9919\n",
            "Step 300 avg train loss = 3.9997\n",
            "Step 400 avg train loss = 3.9899\n",
            "Step 500 avg train loss = 4.0071\n",
            "Step 600 avg train loss = 4.0111\n",
            "Step 700 avg train loss = 4.0287\n",
            "Step 800 avg train loss = 4.0385\n",
            "Step 900 avg train loss = 4.0413\n",
            "Step 1000 avg train loss = 4.0285\n",
            "Step 1100 avg train loss = 4.0451\n",
            "Step 1200 avg train loss = 4.0490\n",
            "Step 1300 avg train loss = 4.0460\n",
            "Step 1400 avg train loss = 4.0713\n",
            "Step 1500 avg train loss = 4.0879\n",
            "Step 1600 avg train loss = 4.0937\n",
            "Step 1700 avg train loss = 4.0824\n",
            "Step 1800 avg train loss = 4.0584\n",
            "Step 1900 avg train loss = 4.0970\n",
            "Step 2000 avg train loss = 4.0975\n",
            "Step 2100 avg train loss = 4.1142\n",
            "Step 2200 avg train loss = 4.1017\n",
            "Step 2300 avg train loss = 4.1026\n",
            "Step 2400 avg train loss = 4.0853\n",
            "Validation loss after 7 epoch = 5.3301\n",
            "Step 0 avg train loss = 3.9261\n",
            "Step 100 avg train loss = 3.8440\n",
            "Step 200 avg train loss = 3.8684\n",
            "Step 300 avg train loss = 3.8729\n",
            "Step 400 avg train loss = 3.8734\n",
            "Step 500 avg train loss = 3.9069\n",
            "Step 600 avg train loss = 3.9219\n",
            "Step 700 avg train loss = 3.8951\n",
            "Step 800 avg train loss = 3.8965\n",
            "Step 900 avg train loss = 3.9264\n",
            "Step 1000 avg train loss = 3.9477\n",
            "Step 1100 avg train loss = 3.9400\n",
            "Step 1200 avg train loss = 3.9614\n",
            "Step 1300 avg train loss = 3.9445\n",
            "Step 1400 avg train loss = 3.9456\n",
            "Step 1500 avg train loss = 3.9390\n",
            "Step 1600 avg train loss = 3.9407\n",
            "Step 1700 avg train loss = 3.9592\n",
            "Step 1800 avg train loss = 3.9591\n",
            "Step 1900 avg train loss = 3.9809\n",
            "Step 2000 avg train loss = 3.9808\n",
            "Step 2100 avg train loss = 3.9648\n",
            "Step 2200 avg train loss = 3.9733\n",
            "Step 2300 avg train loss = 3.9929\n",
            "Step 2400 avg train loss = 4.0095\n",
            "Validation loss after 8 epoch = 5.3986\n",
            "Step 0 avg train loss = 3.8569\n",
            "Step 100 avg train loss = 3.7448\n",
            "Step 200 avg train loss = 3.7514\n",
            "Step 300 avg train loss = 3.7689\n",
            "Step 400 avg train loss = 3.7628\n",
            "Step 500 avg train loss = 3.7833\n",
            "Step 600 avg train loss = 3.7902\n",
            "Step 700 avg train loss = 3.7783\n",
            "Step 800 avg train loss = 3.8215\n",
            "Step 900 avg train loss = 3.8231\n",
            "Step 1000 avg train loss = 3.8275\n",
            "Step 1100 avg train loss = 3.8398\n",
            "Step 1200 avg train loss = 3.8314\n",
            "Step 1300 avg train loss = 3.8419\n",
            "Step 1400 avg train loss = 3.8455\n",
            "Step 1500 avg train loss = 3.8639\n",
            "Step 1600 avg train loss = 3.8601\n",
            "Step 1700 avg train loss = 3.8649\n",
            "Step 1800 avg train loss = 3.8622\n",
            "Step 1900 avg train loss = 3.8639\n",
            "Step 2000 avg train loss = 3.8649\n",
            "Step 2100 avg train loss = 3.8894\n",
            "Step 2200 avg train loss = 3.9049\n",
            "Step 2300 avg train loss = 3.8968\n",
            "Step 2400 avg train loss = 3.8814\n",
            "Validation loss after 9 epoch = 5.4724\n",
            "Step 0 avg train loss = 3.6322\n",
            "Step 100 avg train loss = 3.6339\n",
            "Step 200 avg train loss = 3.6528\n",
            "Step 300 avg train loss = 3.6707\n",
            "Step 400 avg train loss = 3.6762\n",
            "Step 500 avg train loss = 3.6733\n",
            "Step 600 avg train loss = 3.6885\n",
            "Step 700 avg train loss = 3.7217\n",
            "Step 800 avg train loss = 3.7021\n",
            "Step 900 avg train loss = 3.7318\n",
            "Step 1000 avg train loss = 3.7315\n",
            "Step 1100 avg train loss = 3.7547\n",
            "Step 1200 avg train loss = 3.7369\n",
            "Step 1300 avg train loss = 3.7508\n",
            "Step 1400 avg train loss = 3.7451\n",
            "Step 1500 avg train loss = 3.7455\n",
            "Step 1600 avg train loss = 3.7796\n",
            "Step 1700 avg train loss = 3.7875\n",
            "Step 1800 avg train loss = 3.7810\n",
            "Step 1900 avg train loss = 3.7766\n",
            "Step 2000 avg train loss = 3.7877\n",
            "Step 2100 avg train loss = 3.7848\n",
            "Step 2200 avg train loss = 3.7925\n",
            "Step 2300 avg train loss = 3.8148\n",
            "Step 2400 avg train loss = 3.8224\n",
            "Validation loss after 10 epoch = 5.5384\n",
            "Step 0 avg train loss = 3.6623\n",
            "Step 100 avg train loss = 3.5528\n",
            "Step 200 avg train loss = 3.5669\n",
            "Step 300 avg train loss = 3.5739\n",
            "Step 400 avg train loss = 3.5682\n",
            "Step 500 avg train loss = 3.5993\n",
            "Step 600 avg train loss = 3.6179\n",
            "Step 700 avg train loss = 3.6319\n",
            "Step 800 avg train loss = 3.6124\n",
            "Step 900 avg train loss = 3.6349\n",
            "Step 1000 avg train loss = 3.6409\n",
            "Step 1100 avg train loss = 3.6784\n",
            "Step 1200 avg train loss = 3.6471\n",
            "Step 1300 avg train loss = 3.6662\n",
            "Step 1400 avg train loss = 3.6523\n",
            "Step 1500 avg train loss = 3.6712\n",
            "Step 1600 avg train loss = 3.7040\n",
            "Step 1700 avg train loss = 3.7016\n",
            "Step 1800 avg train loss = 3.6965\n",
            "Step 1900 avg train loss = 3.7071\n",
            "Step 2000 avg train loss = 3.7121\n",
            "Step 2100 avg train loss = 3.6970\n",
            "Step 2200 avg train loss = 3.7061\n",
            "Step 2300 avg train loss = 3.7240\n",
            "Step 2400 avg train loss = 3.7387\n",
            "Validation loss after 11 epoch = 5.6109\n",
            "Step 0 avg train loss = 3.5031\n",
            "Step 100 avg train loss = 3.4773\n",
            "Step 200 avg train loss = 3.4853\n",
            "Step 300 avg train loss = 3.5031\n",
            "Step 400 avg train loss = 3.5186\n",
            "Step 500 avg train loss = 3.5199\n",
            "Step 600 avg train loss = 3.5490\n",
            "Step 700 avg train loss = 3.5425\n",
            "Step 800 avg train loss = 3.5337\n",
            "Step 900 avg train loss = 3.5510\n",
            "Step 1000 avg train loss = 3.5587\n",
            "Step 1100 avg train loss = 3.5905\n",
            "Step 1200 avg train loss = 3.5673\n",
            "Step 1300 avg train loss = 3.5937\n",
            "Step 1400 avg train loss = 3.5926\n",
            "Step 1500 avg train loss = 3.6081\n",
            "Step 1600 avg train loss = 3.5871\n",
            "Step 1700 avg train loss = 3.6191\n",
            "Step 1800 avg train loss = 3.6468\n",
            "Step 1900 avg train loss = 3.6066\n",
            "Step 2000 avg train loss = 3.6250\n",
            "Step 2100 avg train loss = 3.6412\n",
            "Step 2200 avg train loss = 3.6447\n",
            "Step 2300 avg train loss = 3.6349\n",
            "Step 2400 avg train loss = 3.6611\n",
            "Validation loss after 12 epoch = 5.6649\n",
            "Step 0 avg train loss = 3.2465\n",
            "Step 100 avg train loss = 3.3980\n",
            "Step 200 avg train loss = 3.3929\n",
            "Step 300 avg train loss = 3.4516\n",
            "Step 400 avg train loss = 3.4609\n",
            "Step 500 avg train loss = 3.4520\n",
            "Step 600 avg train loss = 3.4679\n",
            "Step 700 avg train loss = 3.4703\n",
            "Step 800 avg train loss = 3.4921\n",
            "Step 900 avg train loss = 3.5049\n",
            "Step 1000 avg train loss = 3.5061\n",
            "Step 1100 avg train loss = 3.5030\n",
            "Step 1200 avg train loss = 3.5061\n",
            "Step 1300 avg train loss = 3.5051\n",
            "Step 1400 avg train loss = 3.5276\n",
            "Step 1500 avg train loss = 3.5379\n",
            "Step 1600 avg train loss = 3.5539\n",
            "Step 1700 avg train loss = 3.5217\n",
            "Step 1800 avg train loss = 3.5708\n",
            "Step 1900 avg train loss = 3.5493\n",
            "Step 2000 avg train loss = 3.5485\n",
            "Step 2100 avg train loss = 3.5556\n",
            "Step 2200 avg train loss = 3.5588\n",
            "Step 2300 avg train loss = 3.5681\n",
            "Step 2400 avg train loss = 3.5727\n",
            "Validation loss after 13 epoch = 5.7428\n",
            "Step 0 avg train loss = 3.4843\n",
            "Step 100 avg train loss = 3.3408\n",
            "Step 200 avg train loss = 3.3699\n",
            "Step 300 avg train loss = 3.3662\n",
            "Step 400 avg train loss = 3.3690\n",
            "Step 500 avg train loss = 3.3666\n",
            "Step 600 avg train loss = 3.3983\n",
            "Step 700 avg train loss = 3.4059\n",
            "Step 800 avg train loss = 3.4280\n",
            "Step 900 avg train loss = 3.4331\n",
            "Step 1000 avg train loss = 3.4389\n",
            "Step 1100 avg train loss = 3.4261\n",
            "Step 1200 avg train loss = 3.4480\n",
            "Step 1300 avg train loss = 3.4433\n",
            "Step 1400 avg train loss = 3.4438\n",
            "Step 1500 avg train loss = 3.4724\n",
            "Step 1600 avg train loss = 3.4493\n",
            "Step 1700 avg train loss = 3.4870\n",
            "Step 1800 avg train loss = 3.4867\n",
            "Step 1900 avg train loss = 3.4986\n",
            "Step 2000 avg train loss = 3.4951\n",
            "Step 2100 avg train loss = 3.5089\n",
            "Step 2200 avg train loss = 3.5053\n",
            "Step 2300 avg train loss = 3.4890\n",
            "Step 2400 avg train loss = 3.5005\n",
            "Validation loss after 14 epoch = 5.8012\n",
            "Step 0 avg train loss = 3.2950\n",
            "Step 100 avg train loss = 3.2704\n",
            "Step 200 avg train loss = 3.3078\n",
            "Step 300 avg train loss = 3.2987\n",
            "Step 400 avg train loss = 3.3082\n",
            "Step 500 avg train loss = 3.3257\n",
            "Step 600 avg train loss = 3.3474\n",
            "Step 700 avg train loss = 3.3411\n",
            "Step 800 avg train loss = 3.3460\n",
            "Step 900 avg train loss = 3.3694\n",
            "Step 1000 avg train loss = 3.3675\n",
            "Step 1100 avg train loss = 3.3700\n",
            "Step 1200 avg train loss = 3.3982\n",
            "Step 1300 avg train loss = 3.3869\n",
            "Step 1400 avg train loss = 3.4140\n",
            "Step 1500 avg train loss = 3.3982\n",
            "Step 1600 avg train loss = 3.3937\n",
            "Step 1700 avg train loss = 3.3875\n",
            "Step 1800 avg train loss = 3.4254\n",
            "Step 1900 avg train loss = 3.4274\n",
            "Step 2000 avg train loss = 3.4398\n",
            "Step 2100 avg train loss = 3.4476\n",
            "Step 2200 avg train loss = 3.4547\n",
            "Step 2300 avg train loss = 3.4491\n",
            "Step 2400 avg train loss = 3.4558\n",
            "Validation loss after 15 epoch = 5.8667\n",
            "Step 0 avg train loss = 3.0764\n",
            "Step 100 avg train loss = 3.2389\n",
            "Step 200 avg train loss = 3.2277\n",
            "Step 300 avg train loss = 3.2444\n",
            "Step 400 avg train loss = 3.2594\n",
            "Step 500 avg train loss = 3.2803\n",
            "Step 600 avg train loss = 3.2746\n",
            "Step 700 avg train loss = 3.2957\n",
            "Step 800 avg train loss = 3.2991\n",
            "Step 900 avg train loss = 3.3178\n",
            "Step 1000 avg train loss = 3.3319\n",
            "Step 1100 avg train loss = 3.3365\n",
            "Step 1200 avg train loss = 3.3164\n",
            "Step 1300 avg train loss = 3.3491\n",
            "Step 1400 avg train loss = 3.3146\n",
            "Step 1500 avg train loss = 3.3328\n",
            "Step 1600 avg train loss = 3.3364\n",
            "Step 1700 avg train loss = 3.3402\n",
            "Step 1800 avg train loss = 3.3682\n",
            "Step 1900 avg train loss = 3.3743\n",
            "Step 2000 avg train loss = 3.3754\n",
            "Step 2100 avg train loss = 3.3694\n",
            "Step 2200 avg train loss = 3.3999\n",
            "Step 2300 avg train loss = 3.3946\n",
            "Step 2400 avg train loss = 3.4192\n",
            "Validation loss after 16 epoch = 5.9372\n",
            "Step 0 avg train loss = 3.3933\n",
            "Step 100 avg train loss = 3.1784\n",
            "Step 200 avg train loss = 3.1701\n",
            "Step 300 avg train loss = 3.1890\n",
            "Step 400 avg train loss = 3.1968\n",
            "Step 500 avg train loss = 3.2134\n",
            "Step 600 avg train loss = 3.2178\n",
            "Step 700 avg train loss = 3.2144\n",
            "Step 800 avg train loss = 3.2375\n",
            "Step 900 avg train loss = 3.2455\n",
            "Step 1000 avg train loss = 3.2681\n",
            "Step 1100 avg train loss = 3.2639\n",
            "Step 1200 avg train loss = 3.2868\n",
            "Step 1300 avg train loss = 3.2853\n",
            "Step 1400 avg train loss = 3.2861\n",
            "Step 1500 avg train loss = 3.2977\n",
            "Step 1600 avg train loss = 3.3114\n",
            "Step 1700 avg train loss = 3.3193\n",
            "Step 1800 avg train loss = 3.3016\n",
            "Step 1900 avg train loss = 3.3397\n",
            "Step 2000 avg train loss = 3.3420\n",
            "Step 2100 avg train loss = 3.3277\n",
            "Step 2200 avg train loss = 3.3378\n",
            "Step 2300 avg train loss = 3.3350\n",
            "Step 2400 avg train loss = 3.3512\n",
            "Validation loss after 17 epoch = 5.9951\n",
            "Step 0 avg train loss = 3.0336\n",
            "Step 100 avg train loss = 3.1353\n",
            "Step 200 avg train loss = 3.1264\n",
            "Step 300 avg train loss = 3.1186\n",
            "Step 400 avg train loss = 3.1541\n",
            "Step 500 avg train loss = 3.1671\n",
            "Step 600 avg train loss = 3.1556\n",
            "Step 700 avg train loss = 3.2066\n",
            "Step 800 avg train loss = 3.1836\n",
            "Step 900 avg train loss = 3.2164\n",
            "Step 1000 avg train loss = 3.2107\n",
            "Step 1100 avg train loss = 3.2105\n",
            "Step 1200 avg train loss = 3.2309\n",
            "Step 1300 avg train loss = 3.2257\n",
            "Step 1400 avg train loss = 3.2504\n",
            "Step 1500 avg train loss = 3.2449\n",
            "Step 1600 avg train loss = 3.2530\n",
            "Step 1700 avg train loss = 3.2546\n",
            "Step 1800 avg train loss = 3.2687\n",
            "Step 1900 avg train loss = 3.2740\n",
            "Step 2000 avg train loss = 3.2803\n",
            "Step 2100 avg train loss = 3.2730\n",
            "Step 2200 avg train loss = 3.3055\n",
            "Step 2300 avg train loss = 3.3069\n",
            "Step 2400 avg train loss = 3.2893\n",
            "Validation loss after 18 epoch = 6.0573\n",
            "Step 0 avg train loss = 2.9826\n",
            "Step 100 avg train loss = 3.1011\n",
            "Step 200 avg train loss = 3.0790\n",
            "Step 300 avg train loss = 3.1038\n",
            "Step 400 avg train loss = 3.0980\n",
            "Step 500 avg train loss = 3.1306\n",
            "Step 600 avg train loss = 3.1302\n",
            "Step 700 avg train loss = 3.1368\n",
            "Step 800 avg train loss = 3.1633\n",
            "Step 900 avg train loss = 3.1501\n",
            "Step 1000 avg train loss = 3.1537\n",
            "Step 1100 avg train loss = 3.1750\n",
            "Step 1200 avg train loss = 3.1863\n",
            "Step 1300 avg train loss = 3.1878\n",
            "Step 1400 avg train loss = 3.1940\n",
            "Step 1500 avg train loss = 3.2031\n",
            "Step 1600 avg train loss = 3.2004\n",
            "Step 1700 avg train loss = 3.1932\n",
            "Step 1800 avg train loss = 3.2144\n",
            "Step 1900 avg train loss = 3.1903\n",
            "Step 2000 avg train loss = 3.2335\n",
            "Step 2100 avg train loss = 3.2522\n",
            "Step 2200 avg train loss = 3.2534\n",
            "Step 2300 avg train loss = 3.2527\n",
            "Step 2400 avg train loss = 3.2656\n",
            "Validation loss after 19 epoch = 6.1049\n",
            "Saving best model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyqYWy08Bve",
        "colab_type": "code",
        "outputId": "002ffbb5-89b5-4713-b58b-c27229979efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "epochs = np.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/np.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/np.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves of tuned LSTM model')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK8jRW5c8Bvj",
        "colab_type": "code",
        "outputId": "1a803577-0fa6-4184-b8d3-3110bc75b830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "perp = [2**(i[1]/np.log(2)) for i in plot_cache] \n",
        "print('The minimum validation loss occurred at {} and it is {}'.format(perp.index(min(perp)),min(perp)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum validation loss occurred at 3 and it is 168.0075867791675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps3DtmuR8Bvo",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbgtu3AL3wSr",
        "colab_type": "code",
        "outputId": "58f751c5-5ab7-4a4e-b548-5c4b4ce030cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# first hyperparameter tuning: embedding dimension: 100 - 500\n",
        "\n",
        "\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "# single grid search for the hyperparameter tuning\n",
        "param_grid_emb = {\"embedding_size\": [100,200,300,400,500]}      \n",
        "\n",
        "# list of parameters=========== \n",
        "epoch_num = 20\n",
        "hidden_size = 128 # the second hyperparameter to be tuned. \n",
        "learning_rate=0.001\n",
        "# Loop over embedding size:\n",
        "for embedding_size in param_grid_emb[\"embedding_size\"]:\n",
        "  \n",
        "  embedding_size = embedding_size\n",
        "  hidden_size = hidden_size # output of dimension \n",
        "  num_layers = 2\n",
        "  lstm_dropout = 0.1\n",
        "# input_size = lookup.weight.size(1)\n",
        "  vocab_size = len(train_dict)\n",
        "  \n",
        "  options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "  \n",
        "  model = LSTMModel(options).to(current_device)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "  model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = optim.Adam(model_parameters, lr=learning_rate)\n",
        "  \n",
        "  print(model)\n",
        "  \n",
        "  plot_cache = []\n",
        "  min_val_loss = 20   # why is it 20??\n",
        "\n",
        "  for epoch_number in range(epoch_num):\n",
        "    \n",
        "    # do train \n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "                        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "\n",
        "# save the best model in this single grid search:         \n",
        "print('Saving best model with best embedding dimension...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model.state_dict()\n",
        "        }, './emb_tune_best_LSTM.pt')\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 100, padding_idx=2)\n",
            "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4213\n",
            "Step 100 avg train loss = 7.8746\n",
            "Step 200 avg train loss = 7.1487\n",
            "Step 300 avg train loss = 6.9989\n",
            "Step 400 avg train loss = 6.8819\n",
            "Step 500 avg train loss = 6.7695\n",
            "Step 600 avg train loss = 6.6904\n",
            "Step 700 avg train loss = 6.6061\n",
            "Step 800 avg train loss = 6.5169\n",
            "Step 900 avg train loss = 6.4813\n",
            "Step 1000 avg train loss = 6.4483\n",
            "Step 1100 avg train loss = 6.3791\n",
            "Step 1200 avg train loss = 6.3451\n",
            "Step 1300 avg train loss = 6.3354\n",
            "Step 1400 avg train loss = 6.2372\n",
            "Step 1500 avg train loss = 6.2236\n",
            "Step 1600 avg train loss = 6.1985\n",
            "Step 1700 avg train loss = 6.1570\n",
            "Step 1800 avg train loss = 6.1361\n",
            "Step 1900 avg train loss = 6.1115\n",
            "Step 2000 avg train loss = 6.0735\n",
            "Step 2100 avg train loss = 6.0716\n",
            "Step 2200 avg train loss = 6.0352\n",
            "Step 2300 avg train loss = 6.0043\n",
            "Step 2400 avg train loss = 5.9858\n",
            "Validation loss after 0 epoch = 5.8060\n",
            "Step 0 avg train loss = 5.9210\n",
            "Step 100 avg train loss = 5.8924\n",
            "Step 200 avg train loss = 5.8761\n",
            "Step 300 avg train loss = 5.8770\n",
            "Step 400 avg train loss = 5.8671\n",
            "Step 500 avg train loss = 5.8507\n",
            "Step 600 avg train loss = 5.8032\n",
            "Step 700 avg train loss = 5.8165\n",
            "Step 800 avg train loss = 5.7875\n",
            "Step 900 avg train loss = 5.7804\n",
            "Step 1000 avg train loss = 5.7752\n",
            "Step 1100 avg train loss = 5.7494\n",
            "Step 1200 avg train loss = 5.7438\n",
            "Step 1300 avg train loss = 5.7528\n",
            "Step 1400 avg train loss = 5.7070\n",
            "Step 1500 avg train loss = 5.7132\n",
            "Step 1600 avg train loss = 5.6873\n",
            "Step 1700 avg train loss = 5.6781\n",
            "Step 1800 avg train loss = 5.6919\n",
            "Step 1900 avg train loss = 5.6888\n",
            "Step 2000 avg train loss = 5.6731\n",
            "Step 2100 avg train loss = 5.6440\n",
            "Step 2200 avg train loss = 5.6168\n",
            "Step 2300 avg train loss = 5.6168\n",
            "Step 2400 avg train loss = 5.6350\n",
            "Validation loss after 1 epoch = 5.5183\n",
            "Step 0 avg train loss = 5.2898\n",
            "Step 100 avg train loss = 5.5252\n",
            "Step 200 avg train loss = 5.5130\n",
            "Step 300 avg train loss = 5.4821\n",
            "Step 400 avg train loss = 5.5221\n",
            "Step 500 avg train loss = 5.5088\n",
            "Step 600 avg train loss = 5.4730\n",
            "Step 700 avg train loss = 5.4989\n",
            "Step 800 avg train loss = 5.4681\n",
            "Step 900 avg train loss = 5.4549\n",
            "Step 1000 avg train loss = 5.4655\n",
            "Step 1100 avg train loss = 5.4779\n",
            "Step 1200 avg train loss = 5.4838\n",
            "Step 1300 avg train loss = 5.4428\n",
            "Step 1400 avg train loss = 5.4224\n",
            "Step 1500 avg train loss = 5.4363\n",
            "Step 1600 avg train loss = 5.4266\n",
            "Step 1700 avg train loss = 5.4350\n",
            "Step 1800 avg train loss = 5.4167\n",
            "Step 1900 avg train loss = 5.4456\n",
            "Step 2000 avg train loss = 5.4338\n",
            "Step 2100 avg train loss = 5.3956\n",
            "Step 2200 avg train loss = 5.4055\n",
            "Step 2300 avg train loss = 5.3788\n",
            "Step 2400 avg train loss = 5.4058\n",
            "Validation loss after 2 epoch = 5.3863\n",
            "Step 0 avg train loss = 5.3325\n",
            "Step 100 avg train loss = 5.2772\n",
            "Step 200 avg train loss = 5.2860\n",
            "Step 300 avg train loss = 5.2600\n",
            "Step 400 avg train loss = 5.2802\n",
            "Step 500 avg train loss = 5.2665\n",
            "Step 600 avg train loss = 5.2443\n",
            "Step 700 avg train loss = 5.2463\n",
            "Step 800 avg train loss = 5.2773\n",
            "Step 900 avg train loss = 5.2569\n",
            "Step 1000 avg train loss = 5.2485\n",
            "Step 1100 avg train loss = 5.2619\n",
            "Step 1200 avg train loss = 5.2589\n",
            "Step 1300 avg train loss = 5.2629\n",
            "Step 1400 avg train loss = 5.2452\n",
            "Step 1500 avg train loss = 5.2442\n",
            "Step 1600 avg train loss = 5.2689\n",
            "Step 1700 avg train loss = 5.2641\n",
            "Step 1800 avg train loss = 5.2548\n",
            "Step 1900 avg train loss = 5.2363\n",
            "Step 2000 avg train loss = 5.2428\n",
            "Step 2100 avg train loss = 5.2338\n",
            "Step 2200 avg train loss = 5.2127\n",
            "Step 2300 avg train loss = 5.2210\n",
            "Step 2400 avg train loss = 5.2042\n",
            "Validation loss after 3 epoch = 5.3096\n",
            "Step 0 avg train loss = 4.8802\n",
            "Step 100 avg train loss = 5.1177\n",
            "Step 200 avg train loss = 5.1024\n",
            "Step 300 avg train loss = 5.1025\n",
            "Step 400 avg train loss = 5.1164\n",
            "Step 500 avg train loss = 5.1124\n",
            "Step 600 avg train loss = 5.1118\n",
            "Step 700 avg train loss = 5.1105\n",
            "Step 800 avg train loss = 5.1044\n",
            "Step 900 avg train loss = 5.0983\n",
            "Step 1000 avg train loss = 5.1144\n",
            "Step 1100 avg train loss = 5.1187\n",
            "Step 1200 avg train loss = 5.1188\n",
            "Step 1300 avg train loss = 5.0738\n",
            "Step 1400 avg train loss = 5.0609\n",
            "Step 1500 avg train loss = 5.0963\n",
            "Step 1600 avg train loss = 5.0890\n",
            "Step 1700 avg train loss = 5.1065\n",
            "Step 1800 avg train loss = 5.0918\n",
            "Step 1900 avg train loss = 5.0748\n",
            "Step 2000 avg train loss = 5.0908\n",
            "Step 2100 avg train loss = 5.0621\n",
            "Step 2200 avg train loss = 5.1015\n",
            "Step 2300 avg train loss = 5.0835\n",
            "Step 2400 avg train loss = 5.1103\n",
            "Validation loss after 4 epoch = 5.2752\n",
            "Step 0 avg train loss = 4.7336\n",
            "Step 100 avg train loss = 4.9430\n",
            "Step 200 avg train loss = 4.9663\n",
            "Step 300 avg train loss = 4.9658\n",
            "Step 400 avg train loss = 4.9664\n",
            "Step 500 avg train loss = 4.9587\n",
            "Step 600 avg train loss = 4.9794\n",
            "Step 700 avg train loss = 4.9756\n",
            "Step 800 avg train loss = 4.9788\n",
            "Step 900 avg train loss = 4.9832\n",
            "Step 1000 avg train loss = 4.9915\n",
            "Step 1100 avg train loss = 4.9797\n",
            "Step 1200 avg train loss = 4.9717\n",
            "Step 1300 avg train loss = 4.9927\n",
            "Step 1400 avg train loss = 4.9951\n",
            "Step 1500 avg train loss = 4.9762\n",
            "Step 1600 avg train loss = 4.9692\n",
            "Step 1700 avg train loss = 4.9871\n",
            "Step 1800 avg train loss = 4.9658\n",
            "Step 1900 avg train loss = 4.9636\n",
            "Step 2000 avg train loss = 4.9914\n",
            "Step 2100 avg train loss = 4.9707\n",
            "Step 2200 avg train loss = 4.9736\n",
            "Step 2300 avg train loss = 4.9931\n",
            "Step 2400 avg train loss = 4.9727\n",
            "Validation loss after 5 epoch = 5.2612\n",
            "Step 0 avg train loss = 4.9070\n",
            "Step 100 avg train loss = 4.8496\n",
            "Step 200 avg train loss = 4.8462\n",
            "Step 300 avg train loss = 4.8680\n",
            "Step 400 avg train loss = 4.8684\n",
            "Step 500 avg train loss = 4.8638\n",
            "Step 600 avg train loss = 4.8692\n",
            "Step 700 avg train loss = 4.8554\n",
            "Step 800 avg train loss = 4.8742\n",
            "Step 900 avg train loss = 4.8756\n",
            "Step 1000 avg train loss = 4.8830\n",
            "Step 1100 avg train loss = 4.8991\n",
            "Step 1200 avg train loss = 4.8882\n",
            "Step 1300 avg train loss = 4.8722\n",
            "Step 1400 avg train loss = 4.8845\n",
            "Step 1500 avg train loss = 4.8755\n",
            "Step 1600 avg train loss = 4.8785\n",
            "Step 1700 avg train loss = 4.8972\n",
            "Step 1800 avg train loss = 4.8711\n",
            "Step 1900 avg train loss = 4.8826\n",
            "Step 2000 avg train loss = 4.8773\n",
            "Step 2100 avg train loss = 4.8837\n",
            "Step 2200 avg train loss = 4.8666\n",
            "Step 2300 avg train loss = 4.8724\n",
            "Step 2400 avg train loss = 4.8667\n",
            "Validation loss after 6 epoch = 5.2549\n",
            "Step 0 avg train loss = 4.9683\n",
            "Step 100 avg train loss = 4.7569\n",
            "Step 200 avg train loss = 4.7748\n",
            "Step 300 avg train loss = 4.7621\n",
            "Step 400 avg train loss = 4.7830\n",
            "Step 500 avg train loss = 4.7746\n",
            "Step 600 avg train loss = 4.7811\n",
            "Step 700 avg train loss = 4.7867\n",
            "Step 800 avg train loss = 4.7782\n",
            "Step 900 avg train loss = 4.7891\n",
            "Step 1000 avg train loss = 4.7958\n",
            "Step 1100 avg train loss = 4.7838\n",
            "Step 1200 avg train loss = 4.7977\n",
            "Step 1300 avg train loss = 4.7782\n",
            "Step 1400 avg train loss = 4.8013\n",
            "Step 1500 avg train loss = 4.8037\n",
            "Step 1600 avg train loss = 4.8004\n",
            "Step 1700 avg train loss = 4.7862\n",
            "Step 1800 avg train loss = 4.7852\n",
            "Step 1900 avg train loss = 4.7985\n",
            "Step 2000 avg train loss = 4.7969\n",
            "Step 2100 avg train loss = 4.7974\n",
            "Step 2200 avg train loss = 4.7890\n",
            "Step 2300 avg train loss = 4.7932\n",
            "Step 2400 avg train loss = 4.8149\n",
            "Validation loss after 7 epoch = 5.2613\n",
            "Step 0 avg train loss = 4.7626\n",
            "Step 100 avg train loss = 4.6603\n",
            "Step 200 avg train loss = 4.6610\n",
            "Step 300 avg train loss = 4.6759\n",
            "Step 400 avg train loss = 4.6923\n",
            "Step 500 avg train loss = 4.7105\n",
            "Step 600 avg train loss = 4.7110\n",
            "Step 700 avg train loss = 4.6866\n",
            "Step 800 avg train loss = 4.7092\n",
            "Step 900 avg train loss = 4.6980\n",
            "Step 1000 avg train loss = 4.6930\n",
            "Step 1100 avg train loss = 4.7068\n",
            "Step 1200 avg train loss = 4.7120\n",
            "Step 1300 avg train loss = 4.7182\n",
            "Step 1400 avg train loss = 4.7141\n",
            "Step 1500 avg train loss = 4.7250\n",
            "Step 1600 avg train loss = 4.7170\n",
            "Step 1700 avg train loss = 4.7178\n",
            "Step 1800 avg train loss = 4.7172\n",
            "Step 1900 avg train loss = 4.7579\n",
            "Step 2000 avg train loss = 4.7407\n",
            "Step 2100 avg train loss = 4.7469\n",
            "Step 2200 avg train loss = 4.7218\n",
            "Step 2300 avg train loss = 4.7487\n",
            "Step 2400 avg train loss = 4.7335\n",
            "Validation loss after 8 epoch = 5.2732\n",
            "Step 0 avg train loss = 4.2264\n",
            "Step 100 avg train loss = 4.5932\n",
            "Step 200 avg train loss = 4.6094\n",
            "Step 300 avg train loss = 4.6086\n",
            "Step 400 avg train loss = 4.6216\n",
            "Step 500 avg train loss = 4.6283\n",
            "Step 600 avg train loss = 4.6185\n",
            "Step 700 avg train loss = 4.6355\n",
            "Step 800 avg train loss = 4.6510\n",
            "Step 900 avg train loss = 4.6355\n",
            "Step 1000 avg train loss = 4.6416\n",
            "Step 1100 avg train loss = 4.6430\n",
            "Step 1200 avg train loss = 4.6357\n",
            "Step 1300 avg train loss = 4.6544\n",
            "Step 1400 avg train loss = 4.6548\n",
            "Step 1500 avg train loss = 4.6553\n",
            "Step 1600 avg train loss = 4.6641\n",
            "Step 1700 avg train loss = 4.6564\n",
            "Step 1800 avg train loss = 4.6535\n",
            "Step 1900 avg train loss = 4.6741\n",
            "Step 2000 avg train loss = 4.6606\n",
            "Step 2100 avg train loss = 4.6679\n",
            "Step 2200 avg train loss = 4.6566\n",
            "Step 2300 avg train loss = 4.6698\n",
            "Step 2400 avg train loss = 4.6707\n",
            "Validation loss after 9 epoch = 5.2908\n",
            "Step 0 avg train loss = 4.3609\n",
            "Step 100 avg train loss = 4.5336\n",
            "Step 200 avg train loss = 4.5316\n",
            "Step 300 avg train loss = 4.5584\n",
            "Step 400 avg train loss = 4.5622\n",
            "Step 500 avg train loss = 4.5577\n",
            "Step 600 avg train loss = 4.5820\n",
            "Step 700 avg train loss = 4.5767\n",
            "Step 800 avg train loss = 4.5726\n",
            "Step 900 avg train loss = 4.5919\n",
            "Step 1000 avg train loss = 4.5902\n",
            "Step 1100 avg train loss = 4.5911\n",
            "Step 1200 avg train loss = 4.5693\n",
            "Step 1300 avg train loss = 4.6063\n",
            "Step 1400 avg train loss = 4.5837\n",
            "Step 1500 avg train loss = 4.5770\n",
            "Step 1600 avg train loss = 4.6167\n",
            "Step 1700 avg train loss = 4.6097\n",
            "Step 1800 avg train loss = 4.5910\n",
            "Step 1900 avg train loss = 4.5874\n",
            "Step 2000 avg train loss = 4.5941\n",
            "Step 2100 avg train loss = 4.6049\n",
            "Step 2200 avg train loss = 4.6131\n",
            "Step 2300 avg train loss = 4.6140\n",
            "Step 2400 avg train loss = 4.6092\n",
            "Validation loss after 10 epoch = 5.3071\n",
            "Step 0 avg train loss = 4.5234\n",
            "Step 100 avg train loss = 4.4774\n",
            "Step 200 avg train loss = 4.4867\n",
            "Step 300 avg train loss = 4.5109\n",
            "Step 400 avg train loss = 4.5060\n",
            "Step 500 avg train loss = 4.4964\n",
            "Step 600 avg train loss = 4.4925\n",
            "Step 700 avg train loss = 4.5249\n",
            "Step 800 avg train loss = 4.5542\n",
            "Step 900 avg train loss = 4.5296\n",
            "Step 1000 avg train loss = 4.5133\n",
            "Step 1100 avg train loss = 4.5333\n",
            "Step 1200 avg train loss = 4.5208\n",
            "Step 1300 avg train loss = 4.5406\n",
            "Step 1400 avg train loss = 4.5366\n",
            "Step 1500 avg train loss = 4.5508\n",
            "Step 1600 avg train loss = 4.5247\n",
            "Step 1700 avg train loss = 4.5251\n",
            "Step 1800 avg train loss = 4.5395\n",
            "Step 1900 avg train loss = 4.5391\n",
            "Step 2000 avg train loss = 4.5513\n",
            "Step 2100 avg train loss = 4.5668\n",
            "Step 2200 avg train loss = 4.5612\n",
            "Step 2300 avg train loss = 4.5668\n",
            "Step 2400 avg train loss = 4.5683\n",
            "Validation loss after 11 epoch = 5.3244\n",
            "Step 0 avg train loss = 4.6189\n",
            "Step 100 avg train loss = 4.4407\n",
            "Step 200 avg train loss = 4.4447\n",
            "Step 300 avg train loss = 4.4481\n",
            "Step 400 avg train loss = 4.4525\n",
            "Step 500 avg train loss = 4.4532\n",
            "Step 600 avg train loss = 4.4556\n",
            "Step 700 avg train loss = 4.4450\n",
            "Step 800 avg train loss = 4.4729\n",
            "Step 900 avg train loss = 4.4709\n",
            "Step 1000 avg train loss = 4.4684\n",
            "Step 1100 avg train loss = 4.4602\n",
            "Step 1200 avg train loss = 4.4782\n",
            "Step 1300 avg train loss = 4.4995\n",
            "Step 1400 avg train loss = 4.4816\n",
            "Step 1500 avg train loss = 4.4980\n",
            "Step 1600 avg train loss = 4.4720\n",
            "Step 1700 avg train loss = 4.4893\n",
            "Step 1800 avg train loss = 4.4811\n",
            "Step 1900 avg train loss = 4.5094\n",
            "Step 2000 avg train loss = 4.5121\n",
            "Step 2100 avg train loss = 4.5122\n",
            "Step 2200 avg train loss = 4.5054\n",
            "Step 2300 avg train loss = 4.5327\n",
            "Step 2400 avg train loss = 4.5322\n",
            "Validation loss after 12 epoch = 5.3482\n",
            "Step 0 avg train loss = 4.5102\n",
            "Step 100 avg train loss = 4.3774\n",
            "Step 200 avg train loss = 4.4062\n",
            "Step 300 avg train loss = 4.3978\n",
            "Step 400 avg train loss = 4.4094\n",
            "Step 500 avg train loss = 4.3967\n",
            "Step 600 avg train loss = 4.4133\n",
            "Step 700 avg train loss = 4.4134\n",
            "Step 800 avg train loss = 4.3941\n",
            "Step 900 avg train loss = 4.4223\n",
            "Step 1000 avg train loss = 4.4277\n",
            "Step 1100 avg train loss = 4.4349\n",
            "Step 1200 avg train loss = 4.4507\n",
            "Step 1300 avg train loss = 4.4455\n",
            "Step 1400 avg train loss = 4.4516\n",
            "Step 1500 avg train loss = 4.4522\n",
            "Step 1600 avg train loss = 4.4395\n",
            "Step 1700 avg train loss = 4.4390\n",
            "Step 1800 avg train loss = 4.4805\n",
            "Step 1900 avg train loss = 4.4753\n",
            "Step 2000 avg train loss = 4.4691\n",
            "Step 2100 avg train loss = 4.4474\n",
            "Step 2200 avg train loss = 4.4603\n",
            "Step 2300 avg train loss = 4.4719\n",
            "Step 2400 avg train loss = 4.4617\n",
            "Validation loss after 13 epoch = 5.3702\n",
            "Step 0 avg train loss = 4.5009\n",
            "Step 100 avg train loss = 4.3261\n",
            "Step 200 avg train loss = 4.3439\n",
            "Step 300 avg train loss = 4.3785\n",
            "Step 400 avg train loss = 4.3482\n",
            "Step 500 avg train loss = 4.3721\n",
            "Step 600 avg train loss = 4.3677\n",
            "Step 700 avg train loss = 4.3822\n",
            "Step 800 avg train loss = 4.3751\n",
            "Step 900 avg train loss = 4.3660\n",
            "Step 1000 avg train loss = 4.3929\n",
            "Step 1100 avg train loss = 4.3891\n",
            "Step 1200 avg train loss = 4.4001\n",
            "Step 1300 avg train loss = 4.4061\n",
            "Step 1400 avg train loss = 4.3954\n",
            "Step 1500 avg train loss = 4.3983\n",
            "Step 1600 avg train loss = 4.4164\n",
            "Step 1700 avg train loss = 4.4249\n",
            "Step 1800 avg train loss = 4.4160\n",
            "Step 1900 avg train loss = 4.4266\n",
            "Step 2000 avg train loss = 4.4438\n",
            "Step 2100 avg train loss = 4.4120\n",
            "Step 2200 avg train loss = 4.3997\n",
            "Step 2300 avg train loss = 4.4218\n",
            "Step 2400 avg train loss = 4.4174\n",
            "Validation loss after 14 epoch = 5.3874\n",
            "Step 0 avg train loss = 4.2326\n",
            "Step 100 avg train loss = 4.3039\n",
            "Step 200 avg train loss = 4.2992\n",
            "Step 300 avg train loss = 4.3161\n",
            "Step 400 avg train loss = 4.3237\n",
            "Step 500 avg train loss = 4.3170\n",
            "Step 600 avg train loss = 4.3386\n",
            "Step 700 avg train loss = 4.3483\n",
            "Step 800 avg train loss = 4.3323\n",
            "Step 900 avg train loss = 4.3385\n",
            "Step 1000 avg train loss = 4.3675\n",
            "Step 1100 avg train loss = 4.3489\n",
            "Step 1200 avg train loss = 4.3481\n",
            "Step 1300 avg train loss = 4.3677\n",
            "Step 1400 avg train loss = 4.3548\n",
            "Step 1500 avg train loss = 4.3598\n",
            "Step 1600 avg train loss = 4.3679\n",
            "Step 1700 avg train loss = 4.3411\n",
            "Step 1800 avg train loss = 4.3936\n",
            "Step 1900 avg train loss = 4.3764\n",
            "Step 2000 avg train loss = 4.3731\n",
            "Step 2100 avg train loss = 4.3836\n",
            "Step 2200 avg train loss = 4.3778\n",
            "Step 2300 avg train loss = 4.4034\n",
            "Step 2400 avg train loss = 4.3864\n",
            "Validation loss after 15 epoch = 5.4136\n",
            "Step 0 avg train loss = 4.2984\n",
            "Step 100 avg train loss = 4.2669\n",
            "Step 200 avg train loss = 4.2530\n",
            "Step 300 avg train loss = 4.2793\n",
            "Step 400 avg train loss = 4.2854\n",
            "Step 500 avg train loss = 4.3026\n",
            "Step 600 avg train loss = 4.2912\n",
            "Step 700 avg train loss = 4.2836\n",
            "Step 800 avg train loss = 4.2947\n",
            "Step 900 avg train loss = 4.3052\n",
            "Step 1000 avg train loss = 4.2963\n",
            "Step 1100 avg train loss = 4.3239\n",
            "Step 1200 avg train loss = 4.3211\n",
            "Step 1300 avg train loss = 4.2995\n",
            "Step 1400 avg train loss = 4.3260\n",
            "Step 1500 avg train loss = 4.3411\n",
            "Step 1600 avg train loss = 4.3345\n",
            "Step 1700 avg train loss = 4.3502\n",
            "Step 1800 avg train loss = 4.3376\n",
            "Step 1900 avg train loss = 4.3567\n",
            "Step 2000 avg train loss = 4.3515\n",
            "Step 2100 avg train loss = 4.3368\n",
            "Step 2200 avg train loss = 4.3526\n",
            "Step 2300 avg train loss = 4.3681\n",
            "Step 2400 avg train loss = 4.3282\n",
            "Validation loss after 16 epoch = 5.4343\n",
            "Step 0 avg train loss = 4.1605\n",
            "Step 100 avg train loss = 4.2267\n",
            "Step 200 avg train loss = 4.2250\n",
            "Step 300 avg train loss = 4.2411\n",
            "Step 400 avg train loss = 4.2660\n",
            "Step 500 avg train loss = 4.2262\n",
            "Step 600 avg train loss = 4.2590\n",
            "Step 700 avg train loss = 4.2621\n",
            "Step 800 avg train loss = 4.2726\n",
            "Step 900 avg train loss = 4.2636\n",
            "Step 1000 avg train loss = 4.2787\n",
            "Step 1100 avg train loss = 4.2690\n",
            "Step 1200 avg train loss = 4.2664\n",
            "Step 1300 avg train loss = 4.2884\n",
            "Step 1400 avg train loss = 4.3142\n",
            "Step 1500 avg train loss = 4.2918\n",
            "Step 1600 avg train loss = 4.2721\n",
            "Step 1700 avg train loss = 4.2999\n",
            "Step 1800 avg train loss = 4.2989\n",
            "Step 1900 avg train loss = 4.3143\n",
            "Step 2000 avg train loss = 4.2947\n",
            "Step 2100 avg train loss = 4.3045\n",
            "Step 2200 avg train loss = 4.3129\n",
            "Step 2300 avg train loss = 4.3625\n",
            "Step 2400 avg train loss = 4.3251\n",
            "Validation loss after 17 epoch = 5.4610\n",
            "Step 0 avg train loss = 4.2360\n",
            "Step 100 avg train loss = 4.1870\n",
            "Step 200 avg train loss = 4.1929\n",
            "Step 300 avg train loss = 4.2190\n",
            "Step 400 avg train loss = 4.1917\n",
            "Step 500 avg train loss = 4.2137\n",
            "Step 600 avg train loss = 4.2025\n",
            "Step 700 avg train loss = 4.2336\n",
            "Step 800 avg train loss = 4.2289\n",
            "Step 900 avg train loss = 4.2205\n",
            "Step 1000 avg train loss = 4.2273\n",
            "Step 1100 avg train loss = 4.2746\n",
            "Step 1200 avg train loss = 4.2316\n",
            "Step 1300 avg train loss = 4.2392\n",
            "Step 1400 avg train loss = 4.2620\n",
            "Step 1500 avg train loss = 4.2703\n",
            "Step 1600 avg train loss = 4.2682\n",
            "Step 1700 avg train loss = 4.2784\n",
            "Step 1800 avg train loss = 4.2766\n",
            "Step 1900 avg train loss = 4.2842\n",
            "Step 2000 avg train loss = 4.2974\n",
            "Step 2100 avg train loss = 4.2884\n",
            "Step 2200 avg train loss = 4.2990\n",
            "Step 2300 avg train loss = 4.2737\n",
            "Step 2400 avg train loss = 4.3108\n",
            "Validation loss after 18 epoch = 5.4789\n",
            "Step 0 avg train loss = 4.2661\n",
            "Step 100 avg train loss = 4.1634\n",
            "Step 200 avg train loss = 4.1750\n",
            "Step 300 avg train loss = 4.1774\n",
            "Step 400 avg train loss = 4.1695\n",
            "Step 500 avg train loss = 4.1819\n",
            "Step 600 avg train loss = 4.1751\n",
            "Step 700 avg train loss = 4.1987\n",
            "Step 800 avg train loss = 4.2052\n",
            "Step 900 avg train loss = 4.2081\n",
            "Step 1000 avg train loss = 4.1995\n",
            "Step 1100 avg train loss = 4.2407\n",
            "Step 1200 avg train loss = 4.2215\n",
            "Step 1300 avg train loss = 4.2203\n",
            "Step 1400 avg train loss = 4.2317\n",
            "Step 1500 avg train loss = 4.2465\n",
            "Step 1600 avg train loss = 4.2369\n",
            "Step 1700 avg train loss = 4.2419\n",
            "Step 1800 avg train loss = 4.2411\n",
            "Step 1900 avg train loss = 4.2564\n",
            "Step 2000 avg train loss = 4.2516\n",
            "Step 2100 avg train loss = 4.2519\n",
            "Step 2200 avg train loss = 4.2472\n",
            "Step 2300 avg train loss = 4.2614\n",
            "Step 2400 avg train loss = 4.2573\n",
            "Validation loss after 19 epoch = 5.5068\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 200, padding_idx=2)\n",
            "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4072\n",
            "Step 100 avg train loss = 7.8734\n",
            "Step 200 avg train loss = 7.1471\n",
            "Step 300 avg train loss = 6.9900\n",
            "Step 400 avg train loss = 6.8690\n",
            "Step 500 avg train loss = 6.7549\n",
            "Step 600 avg train loss = 6.6119\n",
            "Step 700 avg train loss = 6.5381\n",
            "Step 800 avg train loss = 6.4657\n",
            "Step 900 avg train loss = 6.4130\n",
            "Step 1000 avg train loss = 6.3581\n",
            "Step 1100 avg train loss = 6.3145\n",
            "Step 1200 avg train loss = 6.2397\n",
            "Step 1300 avg train loss = 6.2099\n",
            "Step 1400 avg train loss = 6.1619\n",
            "Step 1500 avg train loss = 6.1458\n",
            "Step 1600 avg train loss = 6.1025\n",
            "Step 1700 avg train loss = 6.0803\n",
            "Step 1800 avg train loss = 6.0359\n",
            "Step 1900 avg train loss = 6.0127\n",
            "Step 2000 avg train loss = 5.9815\n",
            "Step 2100 avg train loss = 5.9464\n",
            "Step 2200 avg train loss = 5.9454\n",
            "Step 2300 avg train loss = 5.9100\n",
            "Step 2400 avg train loss = 5.9083\n",
            "Validation loss after 0 epoch = 5.7213\n",
            "Step 0 avg train loss = 5.7127\n",
            "Step 100 avg train loss = 5.7983\n",
            "Step 200 avg train loss = 5.7793\n",
            "Step 300 avg train loss = 5.7620\n",
            "Step 400 avg train loss = 5.7586\n",
            "Step 500 avg train loss = 5.7486\n",
            "Step 600 avg train loss = 5.7258\n",
            "Step 700 avg train loss = 5.7270\n",
            "Step 800 avg train loss = 5.7068\n",
            "Step 900 avg train loss = 5.6857\n",
            "Step 1000 avg train loss = 5.6785\n",
            "Step 1100 avg train loss = 5.6672\n",
            "Step 1200 avg train loss = 5.6578\n",
            "Step 1300 avg train loss = 5.6541\n",
            "Step 1400 avg train loss = 5.6405\n",
            "Step 1500 avg train loss = 5.6194\n",
            "Step 1600 avg train loss = 5.6212\n",
            "Step 1700 avg train loss = 5.5919\n",
            "Step 1800 avg train loss = 5.5986\n",
            "Step 1900 avg train loss = 5.5820\n",
            "Step 2000 avg train loss = 5.5825\n",
            "Step 2100 avg train loss = 5.5540\n",
            "Step 2200 avg train loss = 5.5547\n",
            "Step 2300 avg train loss = 5.5617\n",
            "Step 2400 avg train loss = 5.5197\n",
            "Validation loss after 1 epoch = 5.4519\n",
            "Step 0 avg train loss = 5.3700\n",
            "Step 100 avg train loss = 5.4337\n",
            "Step 200 avg train loss = 5.3993\n",
            "Step 300 avg train loss = 5.4125\n",
            "Step 400 avg train loss = 5.4128\n",
            "Step 500 avg train loss = 5.4004\n",
            "Step 600 avg train loss = 5.4129\n",
            "Step 700 avg train loss = 5.3832\n",
            "Step 800 avg train loss = 5.3888\n",
            "Step 900 avg train loss = 5.3802\n",
            "Step 1000 avg train loss = 5.4008\n",
            "Step 1100 avg train loss = 5.3652\n",
            "Step 1200 avg train loss = 5.3819\n",
            "Step 1300 avg train loss = 5.3645\n",
            "Step 1400 avg train loss = 5.3651\n",
            "Step 1500 avg train loss = 5.3479\n",
            "Step 1600 avg train loss = 5.3501\n",
            "Step 1700 avg train loss = 5.3341\n",
            "Step 1800 avg train loss = 5.3439\n",
            "Step 1900 avg train loss = 5.3335\n",
            "Step 2000 avg train loss = 5.3543\n",
            "Step 2100 avg train loss = 5.3197\n",
            "Step 2200 avg train loss = 5.3092\n",
            "Step 2300 avg train loss = 5.3198\n",
            "Step 2400 avg train loss = 5.3211\n",
            "Validation loss after 2 epoch = 5.3333\n",
            "Step 0 avg train loss = 5.1641\n",
            "Step 100 avg train loss = 5.1876\n",
            "Step 200 avg train loss = 5.1706\n",
            "Step 300 avg train loss = 5.1738\n",
            "Step 400 avg train loss = 5.1707\n",
            "Step 500 avg train loss = 5.1782\n",
            "Step 600 avg train loss = 5.1721\n",
            "Step 700 avg train loss = 5.1858\n",
            "Step 800 avg train loss = 5.1804\n",
            "Step 900 avg train loss = 5.1827\n",
            "Step 1000 avg train loss = 5.1771\n",
            "Step 1100 avg train loss = 5.1885\n",
            "Step 1200 avg train loss = 5.1812\n",
            "Step 1300 avg train loss = 5.1638\n",
            "Step 1400 avg train loss = 5.1602\n",
            "Step 1500 avg train loss = 5.1755\n",
            "Step 1600 avg train loss = 5.1627\n",
            "Step 1700 avg train loss = 5.1379\n",
            "Step 1800 avg train loss = 5.1525\n",
            "Step 1900 avg train loss = 5.1664\n",
            "Step 2000 avg train loss = 5.1349\n",
            "Step 2100 avg train loss = 5.1540\n",
            "Step 2200 avg train loss = 5.1496\n",
            "Step 2300 avg train loss = 5.1238\n",
            "Step 2400 avg train loss = 5.1435\n",
            "Validation loss after 3 epoch = 5.2613\n",
            "Step 0 avg train loss = 5.0862\n",
            "Step 100 avg train loss = 5.0027\n",
            "Step 200 avg train loss = 5.0167\n",
            "Step 300 avg train loss = 5.0061\n",
            "Step 400 avg train loss = 5.0147\n",
            "Step 500 avg train loss = 5.0007\n",
            "Step 600 avg train loss = 4.9987\n",
            "Step 700 avg train loss = 5.0205\n",
            "Step 800 avg train loss = 4.9979\n",
            "Step 900 avg train loss = 5.0138\n",
            "Step 1000 avg train loss = 5.0195\n",
            "Step 1100 avg train loss = 5.0022\n",
            "Step 1200 avg train loss = 5.0169\n",
            "Step 1300 avg train loss = 5.0220\n",
            "Step 1400 avg train loss = 5.0102\n",
            "Step 1500 avg train loss = 5.0159\n",
            "Step 1600 avg train loss = 5.0053\n",
            "Step 1700 avg train loss = 5.0121\n",
            "Step 1800 avg train loss = 5.0187\n",
            "Step 1900 avg train loss = 5.0028\n",
            "Step 2000 avg train loss = 5.0128\n",
            "Step 2100 avg train loss = 4.9970\n",
            "Step 2200 avg train loss = 5.0086\n",
            "Step 2300 avg train loss = 5.0139\n",
            "Step 2400 avg train loss = 4.9951\n",
            "Validation loss after 4 epoch = 5.2234\n",
            "Step 0 avg train loss = 4.7910\n",
            "Step 100 avg train loss = 4.8562\n",
            "Step 200 avg train loss = 4.8613\n",
            "Step 300 avg train loss = 4.8863\n",
            "Step 400 avg train loss = 4.8549\n",
            "Step 500 avg train loss = 4.8889\n",
            "Step 600 avg train loss = 4.8718\n",
            "Step 700 avg train loss = 4.8905\n",
            "Step 800 avg train loss = 4.8810\n",
            "Step 900 avg train loss = 4.8913\n",
            "Step 1000 avg train loss = 4.8911\n",
            "Step 1100 avg train loss = 4.8873\n",
            "Step 1200 avg train loss = 4.8806\n",
            "Step 1300 avg train loss = 4.8800\n",
            "Step 1400 avg train loss = 4.8919\n",
            "Step 1500 avg train loss = 4.8815\n",
            "Step 1600 avg train loss = 4.8875\n",
            "Step 1700 avg train loss = 4.8996\n",
            "Step 1800 avg train loss = 4.8842\n",
            "Step 1900 avg train loss = 4.8843\n",
            "Step 2000 avg train loss = 4.8781\n",
            "Step 2100 avg train loss = 4.8721\n",
            "Step 2200 avg train loss = 4.9032\n",
            "Step 2300 avg train loss = 4.8716\n",
            "Step 2400 avg train loss = 4.9014\n",
            "Validation loss after 5 epoch = 5.2085\n",
            "Step 0 avg train loss = 4.5154\n",
            "Step 100 avg train loss = 4.7395\n",
            "Step 200 avg train loss = 4.7420\n",
            "Step 300 avg train loss = 4.7591\n",
            "Step 400 avg train loss = 4.7462\n",
            "Step 500 avg train loss = 4.7480\n",
            "Step 600 avg train loss = 4.7729\n",
            "Step 700 avg train loss = 4.7864\n",
            "Step 800 avg train loss = 4.7813\n",
            "Step 900 avg train loss = 4.7982\n",
            "Step 1000 avg train loss = 4.7523\n",
            "Step 1100 avg train loss = 4.7426\n",
            "Step 1200 avg train loss = 4.7852\n",
            "Step 1300 avg train loss = 4.7630\n",
            "Step 1400 avg train loss = 4.7863\n",
            "Step 1500 avg train loss = 4.7631\n",
            "Step 1600 avg train loss = 4.7773\n",
            "Step 1700 avg train loss = 4.8014\n",
            "Step 1800 avg train loss = 4.7995\n",
            "Step 1900 avg train loss = 4.7921\n",
            "Step 2000 avg train loss = 4.7768\n",
            "Step 2100 avg train loss = 4.7744\n",
            "Step 2200 avg train loss = 4.8140\n",
            "Step 2300 avg train loss = 4.8047\n",
            "Step 2400 avg train loss = 4.8054\n",
            "Validation loss after 6 epoch = 5.2093\n",
            "Step 0 avg train loss = 4.5736\n",
            "Step 100 avg train loss = 4.6334\n",
            "Step 200 avg train loss = 4.6583\n",
            "Step 300 avg train loss = 4.6586\n",
            "Step 400 avg train loss = 4.6652\n",
            "Step 500 avg train loss = 4.6765\n",
            "Step 600 avg train loss = 4.6583\n",
            "Step 700 avg train loss = 4.6679\n",
            "Step 800 avg train loss = 4.6738\n",
            "Step 900 avg train loss = 4.6697\n",
            "Step 1000 avg train loss = 4.6871\n",
            "Step 1100 avg train loss = 4.6736\n",
            "Step 1200 avg train loss = 4.6892\n",
            "Step 1300 avg train loss = 4.6855\n",
            "Step 1400 avg train loss = 4.6756\n",
            "Step 1500 avg train loss = 4.7132\n",
            "Step 1600 avg train loss = 4.6891\n",
            "Step 1700 avg train loss = 4.7001\n",
            "Step 1800 avg train loss = 4.6795\n",
            "Step 1900 avg train loss = 4.7140\n",
            "Step 2000 avg train loss = 4.6911\n",
            "Step 2100 avg train loss = 4.6988\n",
            "Step 2200 avg train loss = 4.7135\n",
            "Step 2300 avg train loss = 4.7172\n",
            "Step 2400 avg train loss = 4.7030\n",
            "Validation loss after 7 epoch = 5.2206\n",
            "Step 0 avg train loss = 4.7209\n",
            "Step 100 avg train loss = 4.5570\n",
            "Step 200 avg train loss = 4.5865\n",
            "Step 300 avg train loss = 4.5709\n",
            "Step 400 avg train loss = 4.5668\n",
            "Step 500 avg train loss = 4.5801\n",
            "Step 600 avg train loss = 4.5937\n",
            "Step 700 avg train loss = 4.5678\n",
            "Step 800 avg train loss = 4.5914\n",
            "Step 900 avg train loss = 4.6029\n",
            "Step 1000 avg train loss = 4.5958\n",
            "Step 1100 avg train loss = 4.6013\n",
            "Step 1200 avg train loss = 4.6071\n",
            "Step 1300 avg train loss = 4.6439\n",
            "Step 1400 avg train loss = 4.6006\n",
            "Step 1500 avg train loss = 4.5940\n",
            "Step 1600 avg train loss = 4.6271\n",
            "Step 1700 avg train loss = 4.6006\n",
            "Step 1800 avg train loss = 4.6212\n",
            "Step 1900 avg train loss = 4.6221\n",
            "Step 2000 avg train loss = 4.6258\n",
            "Step 2100 avg train loss = 4.6108\n",
            "Step 2200 avg train loss = 4.6259\n",
            "Step 2300 avg train loss = 4.6351\n",
            "Step 2400 avg train loss = 4.6376\n",
            "Validation loss after 8 epoch = 5.2339\n",
            "Step 0 avg train loss = 4.5395\n",
            "Step 100 avg train loss = 4.4844\n",
            "Step 200 avg train loss = 4.4784\n",
            "Step 300 avg train loss = 4.5089\n",
            "Step 400 avg train loss = 4.4969\n",
            "Step 500 avg train loss = 4.5090\n",
            "Step 600 avg train loss = 4.5387\n",
            "Step 700 avg train loss = 4.5052\n",
            "Step 800 avg train loss = 4.5192\n",
            "Step 900 avg train loss = 4.5294\n",
            "Step 1000 avg train loss = 4.5147\n",
            "Step 1100 avg train loss = 4.5183\n",
            "Step 1200 avg train loss = 4.5238\n",
            "Step 1300 avg train loss = 4.5396\n",
            "Step 1400 avg train loss = 4.5302\n",
            "Step 1500 avg train loss = 4.5614\n",
            "Step 1600 avg train loss = 4.5436\n",
            "Step 1700 avg train loss = 4.5283\n",
            "Step 1800 avg train loss = 4.5534\n",
            "Step 1900 avg train loss = 4.5506\n",
            "Step 2000 avg train loss = 4.5634\n",
            "Step 2100 avg train loss = 4.5676\n",
            "Step 2200 avg train loss = 4.5539\n",
            "Step 2300 avg train loss = 4.5558\n",
            "Step 2400 avg train loss = 4.5609\n",
            "Validation loss after 9 epoch = 5.2535\n",
            "Step 0 avg train loss = 4.4260\n",
            "Step 100 avg train loss = 4.4039\n",
            "Step 200 avg train loss = 4.4179\n",
            "Step 300 avg train loss = 4.4186\n",
            "Step 400 avg train loss = 4.4270\n",
            "Step 500 avg train loss = 4.4493\n",
            "Step 600 avg train loss = 4.4389\n",
            "Step 700 avg train loss = 4.4444\n",
            "Step 800 avg train loss = 4.4455\n",
            "Step 900 avg train loss = 4.4573\n",
            "Step 1000 avg train loss = 4.4721\n",
            "Step 1100 avg train loss = 4.4426\n",
            "Step 1200 avg train loss = 4.4768\n",
            "Step 1300 avg train loss = 4.4696\n",
            "Step 1400 avg train loss = 4.4682\n",
            "Step 1500 avg train loss = 4.4698\n",
            "Step 1600 avg train loss = 4.5027\n",
            "Step 1700 avg train loss = 4.4927\n",
            "Step 1800 avg train loss = 4.4908\n",
            "Step 1900 avg train loss = 4.4720\n",
            "Step 2000 avg train loss = 4.4912\n",
            "Step 2100 avg train loss = 4.4981\n",
            "Step 2200 avg train loss = 4.4861\n",
            "Step 2300 avg train loss = 4.5120\n",
            "Step 2400 avg train loss = 4.5051\n",
            "Validation loss after 10 epoch = 5.2752\n",
            "Step 0 avg train loss = 4.1379\n",
            "Step 100 avg train loss = 4.3538\n",
            "Step 200 avg train loss = 4.3465\n",
            "Step 300 avg train loss = 4.3662\n",
            "Step 400 avg train loss = 4.3811\n",
            "Step 500 avg train loss = 4.3694\n",
            "Step 600 avg train loss = 4.3720\n",
            "Step 700 avg train loss = 4.3686\n",
            "Step 800 avg train loss = 4.3761\n",
            "Step 900 avg train loss = 4.3973\n",
            "Step 1000 avg train loss = 4.4346\n",
            "Step 1100 avg train loss = 4.4071\n",
            "Step 1200 avg train loss = 4.4056\n",
            "Step 1300 avg train loss = 4.4084\n",
            "Step 1400 avg train loss = 4.4134\n",
            "Step 1500 avg train loss = 4.4248\n",
            "Step 1600 avg train loss = 4.4239\n",
            "Step 1700 avg train loss = 4.4405\n",
            "Step 1800 avg train loss = 4.4399\n",
            "Step 1900 avg train loss = 4.4291\n",
            "Step 2000 avg train loss = 4.4387\n",
            "Step 2100 avg train loss = 4.4464\n",
            "Step 2200 avg train loss = 4.4417\n",
            "Step 2300 avg train loss = 4.4337\n",
            "Step 2400 avg train loss = 4.4449\n",
            "Validation loss after 11 epoch = 5.2981\n",
            "Step 0 avg train loss = 4.4015\n",
            "Step 100 avg train loss = 4.3116\n",
            "Step 200 avg train loss = 4.2860\n",
            "Step 300 avg train loss = 4.3145\n",
            "Step 400 avg train loss = 4.3321\n",
            "Step 500 avg train loss = 4.3110\n",
            "Step 600 avg train loss = 4.3394\n",
            "Step 700 avg train loss = 4.3310\n",
            "Step 800 avg train loss = 4.3234\n",
            "Step 900 avg train loss = 4.3438\n",
            "Step 1000 avg train loss = 4.3542\n",
            "Step 1100 avg train loss = 4.3503\n",
            "Step 1200 avg train loss = 4.3205\n",
            "Step 1300 avg train loss = 4.3513\n",
            "Step 1400 avg train loss = 4.3764\n",
            "Step 1500 avg train loss = 4.3456\n",
            "Step 1600 avg train loss = 4.3771\n",
            "Step 1700 avg train loss = 4.3615\n",
            "Step 1800 avg train loss = 4.3607\n",
            "Step 1900 avg train loss = 4.3651\n",
            "Step 2000 avg train loss = 4.4010\n",
            "Step 2100 avg train loss = 4.3963\n",
            "Step 2200 avg train loss = 4.4192\n",
            "Step 2300 avg train loss = 4.3878\n",
            "Step 2400 avg train loss = 4.3866\n",
            "Validation loss after 12 epoch = 5.3268\n",
            "Step 0 avg train loss = 4.1954\n",
            "Step 100 avg train loss = 4.2482\n",
            "Step 200 avg train loss = 4.2359\n",
            "Step 300 avg train loss = 4.2367\n",
            "Step 400 avg train loss = 4.2612\n",
            "Step 500 avg train loss = 4.2920\n",
            "Step 600 avg train loss = 4.2888\n",
            "Step 700 avg train loss = 4.2701\n",
            "Step 800 avg train loss = 4.2946\n",
            "Step 900 avg train loss = 4.3153\n",
            "Step 1000 avg train loss = 4.2971\n",
            "Step 1100 avg train loss = 4.2984\n",
            "Step 1200 avg train loss = 4.2932\n",
            "Step 1300 avg train loss = 4.3182\n",
            "Step 1400 avg train loss = 4.3139\n",
            "Step 1500 avg train loss = 4.3306\n",
            "Step 1600 avg train loss = 4.3260\n",
            "Step 1700 avg train loss = 4.3133\n",
            "Step 1800 avg train loss = 4.3404\n",
            "Step 1900 avg train loss = 4.3176\n",
            "Step 2000 avg train loss = 4.3522\n",
            "Step 2100 avg train loss = 4.3210\n",
            "Step 2200 avg train loss = 4.3386\n",
            "Step 2300 avg train loss = 4.3423\n",
            "Step 2400 avg train loss = 4.3232\n",
            "Validation loss after 13 epoch = 5.3515\n",
            "Step 0 avg train loss = 4.1868\n",
            "Step 100 avg train loss = 4.2284\n",
            "Step 200 avg train loss = 4.1772\n",
            "Step 300 avg train loss = 4.2114\n",
            "Step 400 avg train loss = 4.2367\n",
            "Step 500 avg train loss = 4.2289\n",
            "Step 600 avg train loss = 4.2441\n",
            "Step 700 avg train loss = 4.2209\n",
            "Step 800 avg train loss = 4.2631\n",
            "Step 900 avg train loss = 4.2440\n",
            "Step 1000 avg train loss = 4.2469\n",
            "Step 1100 avg train loss = 4.2578\n",
            "Step 1200 avg train loss = 4.2676\n",
            "Step 1300 avg train loss = 4.2573\n",
            "Step 1400 avg train loss = 4.2699\n",
            "Step 1500 avg train loss = 4.2751\n",
            "Step 1600 avg train loss = 4.2915\n",
            "Step 1700 avg train loss = 4.2617\n",
            "Step 1800 avg train loss = 4.2539\n",
            "Step 1900 avg train loss = 4.2760\n",
            "Step 2000 avg train loss = 4.2728\n",
            "Step 2100 avg train loss = 4.2873\n",
            "Step 2200 avg train loss = 4.2856\n",
            "Step 2300 avg train loss = 4.2880\n",
            "Step 2400 avg train loss = 4.2990\n",
            "Validation loss after 14 epoch = 5.3798\n",
            "Step 0 avg train loss = 3.9694\n",
            "Step 100 avg train loss = 4.1541\n",
            "Step 200 avg train loss = 4.1601\n",
            "Step 300 avg train loss = 4.1889\n",
            "Step 400 avg train loss = 4.1788\n",
            "Step 500 avg train loss = 4.1664\n",
            "Step 600 avg train loss = 4.1742\n",
            "Step 700 avg train loss = 4.2005\n",
            "Step 800 avg train loss = 4.2060\n",
            "Step 900 avg train loss = 4.2018\n",
            "Step 1000 avg train loss = 4.2024\n",
            "Step 1100 avg train loss = 4.2122\n",
            "Step 1200 avg train loss = 4.2456\n",
            "Step 1300 avg train loss = 4.2165\n",
            "Step 1400 avg train loss = 4.2209\n",
            "Step 1500 avg train loss = 4.2472\n",
            "Step 1600 avg train loss = 4.2171\n",
            "Step 1700 avg train loss = 4.2426\n",
            "Step 1800 avg train loss = 4.2255\n",
            "Step 1900 avg train loss = 4.2298\n",
            "Step 2000 avg train loss = 4.2223\n",
            "Step 2100 avg train loss = 4.2484\n",
            "Step 2200 avg train loss = 4.2681\n",
            "Step 2300 avg train loss = 4.2514\n",
            "Step 2400 avg train loss = 4.2473\n",
            "Validation loss after 15 epoch = 5.4066\n",
            "Step 0 avg train loss = 4.2376\n",
            "Step 100 avg train loss = 4.1166\n",
            "Step 200 avg train loss = 4.1233\n",
            "Step 300 avg train loss = 4.1392\n",
            "Step 400 avg train loss = 4.1489\n",
            "Step 500 avg train loss = 4.1407\n",
            "Step 600 avg train loss = 4.1554\n",
            "Step 700 avg train loss = 4.1312\n",
            "Step 800 avg train loss = 4.1651\n",
            "Step 900 avg train loss = 4.1784\n",
            "Step 1000 avg train loss = 4.1738\n",
            "Step 1100 avg train loss = 4.1455\n",
            "Step 1200 avg train loss = 4.1770\n",
            "Step 1300 avg train loss = 4.1742\n",
            "Step 1400 avg train loss = 4.1887\n",
            "Step 1500 avg train loss = 4.1778\n",
            "Step 1600 avg train loss = 4.1858\n",
            "Step 1700 avg train loss = 4.1826\n",
            "Step 1800 avg train loss = 4.1982\n",
            "Step 1900 avg train loss = 4.2017\n",
            "Step 2000 avg train loss = 4.2095\n",
            "Step 2100 avg train loss = 4.2138\n",
            "Step 2200 avg train loss = 4.2040\n",
            "Step 2300 avg train loss = 4.2178\n",
            "Step 2400 avg train loss = 4.2090\n",
            "Validation loss after 16 epoch = 5.4379\n",
            "Step 0 avg train loss = 4.0614\n",
            "Step 100 avg train loss = 4.0570\n",
            "Step 200 avg train loss = 4.0855\n",
            "Step 300 avg train loss = 4.0890\n",
            "Step 400 avg train loss = 4.1000\n",
            "Step 500 avg train loss = 4.1185\n",
            "Step 600 avg train loss = 4.0977\n",
            "Step 700 avg train loss = 4.1101\n",
            "Step 800 avg train loss = 4.1214\n",
            "Step 900 avg train loss = 4.1088\n",
            "Step 1000 avg train loss = 4.1242\n",
            "Step 1100 avg train loss = 4.1499\n",
            "Step 1200 avg train loss = 4.1463\n",
            "Step 1300 avg train loss = 4.1285\n",
            "Step 1400 avg train loss = 4.1380\n",
            "Step 1500 avg train loss = 4.1419\n",
            "Step 1600 avg train loss = 4.1527\n",
            "Step 1700 avg train loss = 4.1422\n",
            "Step 1800 avg train loss = 4.1489\n",
            "Step 1900 avg train loss = 4.1748\n",
            "Step 2000 avg train loss = 4.1730\n",
            "Step 2100 avg train loss = 4.1749\n",
            "Step 2200 avg train loss = 4.1967\n",
            "Step 2300 avg train loss = 4.1798\n",
            "Step 2400 avg train loss = 4.1927\n",
            "Validation loss after 17 epoch = 5.4691\n",
            "Step 0 avg train loss = 4.2660\n",
            "Step 100 avg train loss = 4.0254\n",
            "Step 200 avg train loss = 4.0464\n",
            "Step 300 avg train loss = 4.0588\n",
            "Step 400 avg train loss = 4.0721\n",
            "Step 500 avg train loss = 4.0575\n",
            "Step 600 avg train loss = 4.0670\n",
            "Step 700 avg train loss = 4.0978\n",
            "Step 800 avg train loss = 4.0892\n",
            "Step 900 avg train loss = 4.0724\n",
            "Step 1000 avg train loss = 4.1005\n",
            "Step 1100 avg train loss = 4.0947\n",
            "Step 1200 avg train loss = 4.1061\n",
            "Step 1300 avg train loss = 4.0877\n",
            "Step 1400 avg train loss = 4.1069\n",
            "Step 1500 avg train loss = 4.1205\n",
            "Step 1600 avg train loss = 4.1290\n",
            "Step 1700 avg train loss = 4.1129\n",
            "Step 1800 avg train loss = 4.1316\n",
            "Step 1900 avg train loss = 4.1349\n",
            "Step 2000 avg train loss = 4.1324\n",
            "Step 2100 avg train loss = 4.1205\n",
            "Step 2200 avg train loss = 4.1396\n",
            "Step 2300 avg train loss = 4.1578\n",
            "Step 2400 avg train loss = 4.1480\n",
            "Validation loss after 18 epoch = 5.4949\n",
            "Step 0 avg train loss = 4.0512\n",
            "Step 100 avg train loss = 3.9958\n",
            "Step 200 avg train loss = 4.0035\n",
            "Step 300 avg train loss = 4.0203\n",
            "Step 400 avg train loss = 4.0256\n",
            "Step 500 avg train loss = 4.0288\n",
            "Step 600 avg train loss = 4.0257\n",
            "Step 700 avg train loss = 4.0490\n",
            "Step 800 avg train loss = 4.0708\n",
            "Step 900 avg train loss = 4.0587\n",
            "Step 1000 avg train loss = 4.0584\n",
            "Step 1100 avg train loss = 4.0559\n",
            "Step 1200 avg train loss = 4.0658\n",
            "Step 1300 avg train loss = 4.0672\n",
            "Step 1400 avg train loss = 4.0846\n",
            "Step 1500 avg train loss = 4.0657\n",
            "Step 1600 avg train loss = 4.0857\n",
            "Step 1700 avg train loss = 4.1016\n",
            "Step 1800 avg train loss = 4.0952\n",
            "Step 1900 avg train loss = 4.1037\n",
            "Step 2000 avg train loss = 4.1114\n",
            "Step 2100 avg train loss = 4.0961\n",
            "Step 2200 avg train loss = 4.0982\n",
            "Step 2300 avg train loss = 4.0962\n",
            "Step 2400 avg train loss = 4.1322\n",
            "Validation loss after 19 epoch = 5.5245\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4132\n",
            "Step 100 avg train loss = 7.8714\n",
            "Step 200 avg train loss = 7.0718\n",
            "Step 300 avg train loss = 6.9174\n",
            "Step 400 avg train loss = 6.7383\n",
            "Step 500 avg train loss = 6.6138\n",
            "Step 600 avg train loss = 6.5480\n",
            "Step 700 avg train loss = 6.4684\n",
            "Step 800 avg train loss = 6.3962\n",
            "Step 900 avg train loss = 6.3300\n",
            "Step 1000 avg train loss = 6.2845\n",
            "Step 1100 avg train loss = 6.2470\n",
            "Step 1200 avg train loss = 6.1912\n",
            "Step 1300 avg train loss = 6.1466\n",
            "Step 1400 avg train loss = 6.0905\n",
            "Step 1500 avg train loss = 6.0833\n",
            "Step 1600 avg train loss = 6.0545\n",
            "Step 1700 avg train loss = 6.0238\n",
            "Step 1800 avg train loss = 6.0048\n",
            "Step 1900 avg train loss = 5.9594\n",
            "Step 2000 avg train loss = 5.9311\n",
            "Step 2100 avg train loss = 5.9381\n",
            "Step 2200 avg train loss = 5.8777\n",
            "Step 2300 avg train loss = 5.8600\n",
            "Step 2400 avg train loss = 5.8589\n",
            "Validation loss after 0 epoch = 5.6767\n",
            "Step 0 avg train loss = 5.7587\n",
            "Step 100 avg train loss = 5.7528\n",
            "Step 200 avg train loss = 5.7185\n",
            "Step 300 avg train loss = 5.7048\n",
            "Step 400 avg train loss = 5.7110\n",
            "Step 500 avg train loss = 5.7001\n",
            "Step 600 avg train loss = 5.7094\n",
            "Step 700 avg train loss = 5.6934\n",
            "Step 800 avg train loss = 5.6502\n",
            "Step 900 avg train loss = 5.6611\n",
            "Step 1000 avg train loss = 5.6310\n",
            "Step 1100 avg train loss = 5.6267\n",
            "Step 1200 avg train loss = 5.6277\n",
            "Step 1300 avg train loss = 5.5999\n",
            "Step 1400 avg train loss = 5.6039\n",
            "Step 1500 avg train loss = 5.5820\n",
            "Step 1600 avg train loss = 5.5850\n",
            "Step 1700 avg train loss = 5.5689\n",
            "Step 1800 avg train loss = 5.5302\n",
            "Step 1900 avg train loss = 5.5379\n",
            "Step 2000 avg train loss = 5.5269\n",
            "Step 2100 avg train loss = 5.5479\n",
            "Step 2200 avg train loss = 5.5299\n",
            "Step 2300 avg train loss = 5.5011\n",
            "Step 2400 avg train loss = 5.5004\n",
            "Validation loss after 1 epoch = 5.4238\n",
            "Step 0 avg train loss = 5.5506\n",
            "Step 100 avg train loss = 5.3963\n",
            "Step 200 avg train loss = 5.3657\n",
            "Step 300 avg train loss = 5.3674\n",
            "Step 400 avg train loss = 5.3633\n",
            "Step 500 avg train loss = 5.3702\n",
            "Step 600 avg train loss = 5.3783\n",
            "Step 700 avg train loss = 5.3514\n",
            "Step 800 avg train loss = 5.3522\n",
            "Step 900 avg train loss = 5.3666\n",
            "Step 1000 avg train loss = 5.3614\n",
            "Step 1100 avg train loss = 5.3267\n",
            "Step 1200 avg train loss = 5.3230\n",
            "Step 1300 avg train loss = 5.3262\n",
            "Step 1400 avg train loss = 5.3109\n",
            "Step 1500 avg train loss = 5.3222\n",
            "Step 1600 avg train loss = 5.2921\n",
            "Step 1700 avg train loss = 5.2936\n",
            "Step 1800 avg train loss = 5.2926\n",
            "Step 1900 avg train loss = 5.2965\n",
            "Step 2000 avg train loss = 5.2875\n",
            "Step 2100 avg train loss = 5.2914\n",
            "Step 2200 avg train loss = 5.2825\n",
            "Step 2300 avg train loss = 5.2815\n",
            "Step 2400 avg train loss = 5.2795\n",
            "Validation loss after 2 epoch = 5.2959\n",
            "Step 0 avg train loss = 5.0195\n",
            "Step 100 avg train loss = 5.1366\n",
            "Step 200 avg train loss = 5.1382\n",
            "Step 300 avg train loss = 5.1205\n",
            "Step 400 avg train loss = 5.1541\n",
            "Step 500 avg train loss = 5.1322\n",
            "Step 600 avg train loss = 5.1403\n",
            "Step 700 avg train loss = 5.1263\n",
            "Step 800 avg train loss = 5.1309\n",
            "Step 900 avg train loss = 5.1300\n",
            "Step 1000 avg train loss = 5.1390\n",
            "Step 1100 avg train loss = 5.1246\n",
            "Step 1200 avg train loss = 5.1436\n",
            "Step 1300 avg train loss = 5.1297\n",
            "Step 1400 avg train loss = 5.1331\n",
            "Step 1500 avg train loss = 5.1092\n",
            "Step 1600 avg train loss = 5.1036\n",
            "Step 1700 avg train loss = 5.1178\n",
            "Step 1800 avg train loss = 5.1242\n",
            "Step 1900 avg train loss = 5.1057\n",
            "Step 2000 avg train loss = 5.1104\n",
            "Step 2100 avg train loss = 5.0977\n",
            "Step 2200 avg train loss = 5.1044\n",
            "Step 2300 avg train loss = 5.1035\n",
            "Step 2400 avg train loss = 5.0999\n",
            "Validation loss after 3 epoch = 5.2366\n",
            "Step 0 avg train loss = 4.8921\n",
            "Step 100 avg train loss = 4.9432\n",
            "Step 200 avg train loss = 4.9525\n",
            "Step 300 avg train loss = 4.9740\n",
            "Step 400 avg train loss = 4.9508\n",
            "Step 500 avg train loss = 4.9661\n",
            "Step 600 avg train loss = 4.9629\n",
            "Step 700 avg train loss = 4.9784\n",
            "Step 800 avg train loss = 4.9607\n",
            "Step 900 avg train loss = 4.9889\n",
            "Step 1000 avg train loss = 4.9693\n",
            "Step 1100 avg train loss = 4.9704\n",
            "Step 1200 avg train loss = 4.9815\n",
            "Step 1300 avg train loss = 4.9598\n",
            "Step 1400 avg train loss = 4.9858\n",
            "Step 1500 avg train loss = 4.9625\n",
            "Step 1600 avg train loss = 4.9674\n",
            "Step 1700 avg train loss = 4.9663\n",
            "Step 1800 avg train loss = 4.9582\n",
            "Step 1900 avg train loss = 4.9630\n",
            "Step 2000 avg train loss = 4.9627\n",
            "Step 2100 avg train loss = 4.9805\n",
            "Step 2200 avg train loss = 4.9635\n",
            "Step 2300 avg train loss = 4.9540\n",
            "Step 2400 avg train loss = 4.9612\n",
            "Validation loss after 4 epoch = 5.2103\n",
            "Step 0 avg train loss = 4.6798\n",
            "Step 100 avg train loss = 4.8180\n",
            "Step 200 avg train loss = 4.8346\n",
            "Step 300 avg train loss = 4.8313\n",
            "Step 400 avg train loss = 4.8162\n",
            "Step 500 avg train loss = 4.8379\n",
            "Step 600 avg train loss = 4.8344\n",
            "Step 700 avg train loss = 4.8493\n",
            "Step 800 avg train loss = 4.8353\n",
            "Step 900 avg train loss = 4.8277\n",
            "Step 1000 avg train loss = 4.8371\n",
            "Step 1100 avg train loss = 4.8447\n",
            "Step 1200 avg train loss = 4.8419\n",
            "Step 1300 avg train loss = 4.8302\n",
            "Step 1400 avg train loss = 4.8531\n",
            "Step 1500 avg train loss = 4.8365\n",
            "Step 1600 avg train loss = 4.8542\n",
            "Step 1700 avg train loss = 4.8545\n",
            "Step 1800 avg train loss = 4.8356\n",
            "Step 1900 avg train loss = 4.8338\n",
            "Step 2000 avg train loss = 4.8203\n",
            "Step 2100 avg train loss = 4.8353\n",
            "Step 2200 avg train loss = 4.8453\n",
            "Step 2300 avg train loss = 4.8232\n",
            "Step 2400 avg train loss = 4.8604\n",
            "Validation loss after 5 epoch = 5.2010\n",
            "Step 0 avg train loss = 4.6998\n",
            "Step 100 avg train loss = 4.7235\n",
            "Step 200 avg train loss = 4.6999\n",
            "Step 300 avg train loss = 4.6949\n",
            "Step 400 avg train loss = 4.7107\n",
            "Step 500 avg train loss = 4.7160\n",
            "Step 600 avg train loss = 4.7304\n",
            "Step 700 avg train loss = 4.7101\n",
            "Step 800 avg train loss = 4.7077\n",
            "Step 900 avg train loss = 4.7209\n",
            "Step 1000 avg train loss = 4.7556\n",
            "Step 1100 avg train loss = 4.7351\n",
            "Step 1200 avg train loss = 4.7228\n",
            "Step 1300 avg train loss = 4.7430\n",
            "Step 1400 avg train loss = 4.7261\n",
            "Step 1500 avg train loss = 4.7591\n",
            "Step 1600 avg train loss = 4.7589\n",
            "Step 1700 avg train loss = 4.7402\n",
            "Step 1800 avg train loss = 4.7493\n",
            "Step 1900 avg train loss = 4.7287\n",
            "Step 2000 avg train loss = 4.7516\n",
            "Step 2100 avg train loss = 4.7284\n",
            "Step 2200 avg train loss = 4.7357\n",
            "Step 2300 avg train loss = 4.7516\n",
            "Step 2400 avg train loss = 4.7478\n",
            "Validation loss after 6 epoch = 5.2127\n",
            "Step 0 avg train loss = 4.4473\n",
            "Step 100 avg train loss = 4.6054\n",
            "Step 200 avg train loss = 4.6297\n",
            "Step 300 avg train loss = 4.6115\n",
            "Step 400 avg train loss = 4.6327\n",
            "Step 500 avg train loss = 4.6328\n",
            "Step 600 avg train loss = 4.6112\n",
            "Step 700 avg train loss = 4.6301\n",
            "Step 800 avg train loss = 4.6088\n",
            "Step 900 avg train loss = 4.6406\n",
            "Step 1000 avg train loss = 4.6494\n",
            "Step 1100 avg train loss = 4.6394\n",
            "Step 1200 avg train loss = 4.6366\n",
            "Step 1300 avg train loss = 4.6285\n",
            "Step 1400 avg train loss = 4.6434\n",
            "Step 1500 avg train loss = 4.6441\n",
            "Step 1600 avg train loss = 4.6587\n",
            "Step 1700 avg train loss = 4.6592\n",
            "Step 1800 avg train loss = 4.6515\n",
            "Step 1900 avg train loss = 4.6606\n",
            "Step 2000 avg train loss = 4.6479\n",
            "Step 2100 avg train loss = 4.6624\n",
            "Step 2200 avg train loss = 4.6377\n",
            "Step 2300 avg train loss = 4.6708\n",
            "Step 2400 avg train loss = 4.6581\n",
            "Validation loss after 7 epoch = 5.2190\n",
            "Step 0 avg train loss = 4.2927\n",
            "Step 100 avg train loss = 4.4935\n",
            "Step 200 avg train loss = 4.5162\n",
            "Step 300 avg train loss = 4.5264\n",
            "Step 400 avg train loss = 4.5315\n",
            "Step 500 avg train loss = 4.5391\n",
            "Step 600 avg train loss = 4.5384\n",
            "Step 700 avg train loss = 4.5460\n",
            "Step 800 avg train loss = 4.5488\n",
            "Step 900 avg train loss = 4.5770\n",
            "Step 1000 avg train loss = 4.5476\n",
            "Step 1100 avg train loss = 4.5600\n",
            "Step 1200 avg train loss = 4.5727\n",
            "Step 1300 avg train loss = 4.5778\n",
            "Step 1400 avg train loss = 4.5445\n",
            "Step 1500 avg train loss = 4.5699\n",
            "Step 1600 avg train loss = 4.5815\n",
            "Step 1700 avg train loss = 4.5835\n",
            "Step 1800 avg train loss = 4.5899\n",
            "Step 1900 avg train loss = 4.5956\n",
            "Step 2000 avg train loss = 4.5685\n",
            "Step 2100 avg train loss = 4.5766\n",
            "Step 2200 avg train loss = 4.6023\n",
            "Step 2300 avg train loss = 4.5796\n",
            "Step 2400 avg train loss = 4.5731\n",
            "Validation loss after 8 epoch = 5.2386\n",
            "Step 0 avg train loss = 4.4380\n",
            "Step 100 avg train loss = 4.4550\n",
            "Step 200 avg train loss = 4.4457\n",
            "Step 300 avg train loss = 4.4483\n",
            "Step 400 avg train loss = 4.4734\n",
            "Step 500 avg train loss = 4.4549\n",
            "Step 600 avg train loss = 4.4839\n",
            "Step 700 avg train loss = 4.4780\n",
            "Step 800 avg train loss = 4.4784\n",
            "Step 900 avg train loss = 4.5049\n",
            "Step 1000 avg train loss = 4.4631\n",
            "Step 1100 avg train loss = 4.4889\n",
            "Step 1200 avg train loss = 4.4929\n",
            "Step 1300 avg train loss = 4.4843\n",
            "Step 1400 avg train loss = 4.4992\n",
            "Step 1500 avg train loss = 4.4775\n",
            "Step 1600 avg train loss = 4.5039\n",
            "Step 1700 avg train loss = 4.5227\n",
            "Step 1800 avg train loss = 4.4995\n",
            "Step 1900 avg train loss = 4.5082\n",
            "Step 2000 avg train loss = 4.4815\n",
            "Step 2100 avg train loss = 4.5146\n",
            "Step 2200 avg train loss = 4.4961\n",
            "Step 2300 avg train loss = 4.5280\n",
            "Step 2400 avg train loss = 4.5080\n",
            "Validation loss after 9 epoch = 5.2582\n",
            "Step 0 avg train loss = 4.4055\n",
            "Step 100 avg train loss = 4.3765\n",
            "Step 200 avg train loss = 4.3724\n",
            "Step 300 avg train loss = 4.3788\n",
            "Step 400 avg train loss = 4.4054\n",
            "Step 500 avg train loss = 4.3940\n",
            "Step 600 avg train loss = 4.4031\n",
            "Step 700 avg train loss = 4.3958\n",
            "Step 800 avg train loss = 4.4139\n",
            "Step 900 avg train loss = 4.4374\n",
            "Step 1000 avg train loss = 4.4126\n",
            "Step 1100 avg train loss = 4.4152\n",
            "Step 1200 avg train loss = 4.4322\n",
            "Step 1300 avg train loss = 4.4198\n",
            "Step 1400 avg train loss = 4.4141\n",
            "Step 1500 avg train loss = 4.4266\n",
            "Step 1600 avg train loss = 4.4492\n",
            "Step 1700 avg train loss = 4.4342\n",
            "Step 1800 avg train loss = 4.4374\n",
            "Step 1900 avg train loss = 4.4539\n",
            "Step 2000 avg train loss = 4.4652\n",
            "Step 2100 avg train loss = 4.4614\n",
            "Step 2200 avg train loss = 4.4463\n",
            "Step 2300 avg train loss = 4.4552\n",
            "Step 2400 avg train loss = 4.4479\n",
            "Validation loss after 10 epoch = 5.2892\n",
            "Step 0 avg train loss = 4.3618\n",
            "Step 100 avg train loss = 4.3037\n",
            "Step 200 avg train loss = 4.3026\n",
            "Step 300 avg train loss = 4.3128\n",
            "Step 400 avg train loss = 4.3145\n",
            "Step 500 avg train loss = 4.3393\n",
            "Step 600 avg train loss = 4.3507\n",
            "Step 700 avg train loss = 4.3289\n",
            "Step 800 avg train loss = 4.3566\n",
            "Step 900 avg train loss = 4.3549\n",
            "Step 1000 avg train loss = 4.3586\n",
            "Step 1100 avg train loss = 4.3561\n",
            "Step 1200 avg train loss = 4.3660\n",
            "Step 1300 avg train loss = 4.3641\n",
            "Step 1400 avg train loss = 4.3784\n",
            "Step 1500 avg train loss = 4.3786\n",
            "Step 1600 avg train loss = 4.3619\n",
            "Step 1700 avg train loss = 4.3822\n",
            "Step 1800 avg train loss = 4.4005\n",
            "Step 1900 avg train loss = 4.4051\n",
            "Step 2000 avg train loss = 4.3930\n",
            "Step 2100 avg train loss = 4.3909\n",
            "Step 2200 avg train loss = 4.4177\n",
            "Step 2300 avg train loss = 4.4144\n",
            "Step 2400 avg train loss = 4.4052\n",
            "Validation loss after 11 epoch = 5.3104\n",
            "Step 0 avg train loss = 4.2222\n",
            "Step 100 avg train loss = 4.2431\n",
            "Step 200 avg train loss = 4.2607\n",
            "Step 300 avg train loss = 4.2780\n",
            "Step 400 avg train loss = 4.2829\n",
            "Step 500 avg train loss = 4.2874\n",
            "Step 600 avg train loss = 4.2693\n",
            "Step 700 avg train loss = 4.2963\n",
            "Step 800 avg train loss = 4.2916\n",
            "Step 900 avg train loss = 4.3065\n",
            "Step 1000 avg train loss = 4.3078\n",
            "Step 1100 avg train loss = 4.3065\n",
            "Step 1200 avg train loss = 4.3225\n",
            "Step 1300 avg train loss = 4.3066\n",
            "Step 1400 avg train loss = 4.3129\n",
            "Step 1500 avg train loss = 4.3258\n",
            "Step 1600 avg train loss = 4.3162\n",
            "Step 1700 avg train loss = 4.3184\n",
            "Step 1800 avg train loss = 4.3419\n",
            "Step 1900 avg train loss = 4.3227\n",
            "Step 2000 avg train loss = 4.3440\n",
            "Step 2100 avg train loss = 4.3308\n",
            "Step 2200 avg train loss = 4.3603\n",
            "Step 2300 avg train loss = 4.3454\n",
            "Step 2400 avg train loss = 4.3487\n",
            "Validation loss after 12 epoch = 5.3362\n",
            "Step 0 avg train loss = 4.2120\n",
            "Step 100 avg train loss = 4.1886\n",
            "Step 200 avg train loss = 4.1989\n",
            "Step 300 avg train loss = 4.2311\n",
            "Step 400 avg train loss = 4.2388\n",
            "Step 500 avg train loss = 4.2294\n",
            "Step 600 avg train loss = 4.2298\n",
            "Step 700 avg train loss = 4.2463\n",
            "Step 800 avg train loss = 4.2391\n",
            "Step 900 avg train loss = 4.2502\n",
            "Step 1000 avg train loss = 4.2688\n",
            "Step 1100 avg train loss = 4.2590\n",
            "Step 1200 avg train loss = 4.2649\n",
            "Step 1300 avg train loss = 4.2649\n",
            "Step 1400 avg train loss = 4.2731\n",
            "Step 1500 avg train loss = 4.2836\n",
            "Step 1600 avg train loss = 4.2629\n",
            "Step 1700 avg train loss = 4.2678\n",
            "Step 1800 avg train loss = 4.2923\n",
            "Step 1900 avg train loss = 4.2716\n",
            "Step 2000 avg train loss = 4.2761\n",
            "Step 2100 avg train loss = 4.3148\n",
            "Step 2200 avg train loss = 4.2881\n",
            "Step 2300 avg train loss = 4.2699\n",
            "Step 2400 avg train loss = 4.3139\n",
            "Validation loss after 13 epoch = 5.3644\n",
            "Step 0 avg train loss = 4.0577\n",
            "Step 100 avg train loss = 4.1405\n",
            "Step 200 avg train loss = 4.1497\n",
            "Step 300 avg train loss = 4.1604\n",
            "Step 400 avg train loss = 4.1758\n",
            "Step 500 avg train loss = 4.1736\n",
            "Step 600 avg train loss = 4.1823\n",
            "Step 700 avg train loss = 4.1856\n",
            "Step 800 avg train loss = 4.2179\n",
            "Step 900 avg train loss = 4.1874\n",
            "Step 1000 avg train loss = 4.1968\n",
            "Step 1100 avg train loss = 4.2222\n",
            "Step 1200 avg train loss = 4.2174\n",
            "Step 1300 avg train loss = 4.2175\n",
            "Step 1400 avg train loss = 4.2315\n",
            "Step 1500 avg train loss = 4.2371\n",
            "Step 1600 avg train loss = 4.2383\n",
            "Step 1700 avg train loss = 4.2266\n",
            "Step 1800 avg train loss = 4.2499\n",
            "Step 1900 avg train loss = 4.2366\n",
            "Step 2000 avg train loss = 4.2536\n",
            "Step 2100 avg train loss = 4.2439\n",
            "Step 2200 avg train loss = 4.2432\n",
            "Step 2300 avg train loss = 4.2592\n",
            "Step 2400 avg train loss = 4.2655\n",
            "Validation loss after 14 epoch = 5.3933\n",
            "Step 0 avg train loss = 4.0131\n",
            "Step 100 avg train loss = 4.1013\n",
            "Step 200 avg train loss = 4.0988\n",
            "Step 300 avg train loss = 4.1272\n",
            "Step 400 avg train loss = 4.1209\n",
            "Step 500 avg train loss = 4.1275\n",
            "Step 600 avg train loss = 4.1455\n",
            "Step 700 avg train loss = 4.1450\n",
            "Step 800 avg train loss = 4.1420\n",
            "Step 900 avg train loss = 4.1523\n",
            "Step 1000 avg train loss = 4.1660\n",
            "Step 1100 avg train loss = 4.1722\n",
            "Step 1200 avg train loss = 4.1657\n",
            "Step 1300 avg train loss = 4.1743\n",
            "Step 1400 avg train loss = 4.1944\n",
            "Step 1500 avg train loss = 4.2043\n",
            "Step 1600 avg train loss = 4.1986\n",
            "Step 1700 avg train loss = 4.1996\n",
            "Step 1800 avg train loss = 4.1933\n",
            "Step 1900 avg train loss = 4.1965\n",
            "Step 2000 avg train loss = 4.2143\n",
            "Step 2100 avg train loss = 4.1789\n",
            "Step 2200 avg train loss = 4.1946\n",
            "Step 2300 avg train loss = 4.2069\n",
            "Step 2400 avg train loss = 4.2358\n",
            "Validation loss after 15 epoch = 5.4246\n",
            "Step 0 avg train loss = 4.1673\n",
            "Step 100 avg train loss = 4.0547\n",
            "Step 200 avg train loss = 4.0693\n",
            "Step 300 avg train loss = 4.0738\n",
            "Step 400 avg train loss = 4.0841\n",
            "Step 500 avg train loss = 4.0999\n",
            "Step 600 avg train loss = 4.1131\n",
            "Step 700 avg train loss = 4.0992\n",
            "Step 800 avg train loss = 4.0871\n",
            "Step 900 avg train loss = 4.1142\n",
            "Step 1000 avg train loss = 4.1206\n",
            "Step 1100 avg train loss = 4.1252\n",
            "Step 1200 avg train loss = 4.1220\n",
            "Step 1300 avg train loss = 4.1445\n",
            "Step 1400 avg train loss = 4.1459\n",
            "Step 1500 avg train loss = 4.1337\n",
            "Step 1600 avg train loss = 4.1752\n",
            "Step 1700 avg train loss = 4.1666\n",
            "Step 1800 avg train loss = 4.1629\n",
            "Step 1900 avg train loss = 4.1651\n",
            "Step 2000 avg train loss = 4.1737\n",
            "Step 2100 avg train loss = 4.1539\n",
            "Step 2200 avg train loss = 4.1735\n",
            "Step 2300 avg train loss = 4.1523\n",
            "Step 2400 avg train loss = 4.1757\n",
            "Validation loss after 16 epoch = 5.4519\n",
            "Step 0 avg train loss = 3.9297\n",
            "Step 100 avg train loss = 4.0126\n",
            "Step 200 avg train loss = 4.0253\n",
            "Step 300 avg train loss = 4.0391\n",
            "Step 400 avg train loss = 4.0439\n",
            "Step 500 avg train loss = 4.0645\n",
            "Step 600 avg train loss = 4.0549\n",
            "Step 700 avg train loss = 4.0900\n",
            "Step 800 avg train loss = 4.0793\n",
            "Step 900 avg train loss = 4.0775\n",
            "Step 1000 avg train loss = 4.0889\n",
            "Step 1100 avg train loss = 4.0630\n",
            "Step 1200 avg train loss = 4.0886\n",
            "Step 1300 avg train loss = 4.1105\n",
            "Step 1400 avg train loss = 4.1256\n",
            "Step 1500 avg train loss = 4.1118\n",
            "Step 1600 avg train loss = 4.1088\n",
            "Step 1700 avg train loss = 4.1054\n",
            "Step 1800 avg train loss = 4.1225\n",
            "Step 1900 avg train loss = 4.1061\n",
            "Step 2000 avg train loss = 4.1228\n",
            "Step 2100 avg train loss = 4.1372\n",
            "Step 2200 avg train loss = 4.1323\n",
            "Step 2300 avg train loss = 4.1270\n",
            "Step 2400 avg train loss = 4.1334\n",
            "Validation loss after 17 epoch = 5.4837\n",
            "Step 0 avg train loss = 4.0440\n",
            "Step 100 avg train loss = 3.9772\n",
            "Step 200 avg train loss = 3.9905\n",
            "Step 300 avg train loss = 4.0043\n",
            "Step 400 avg train loss = 4.0068\n",
            "Step 500 avg train loss = 4.0165\n",
            "Step 600 avg train loss = 4.0387\n",
            "Step 700 avg train loss = 4.0457\n",
            "Step 800 avg train loss = 4.0179\n",
            "Step 900 avg train loss = 4.0531\n",
            "Step 1000 avg train loss = 4.0661\n",
            "Step 1100 avg train loss = 4.0614\n",
            "Step 1200 avg train loss = 4.0640\n",
            "Step 1300 avg train loss = 4.0802\n",
            "Step 1400 avg train loss = 4.0703\n",
            "Step 1500 avg train loss = 4.0460\n",
            "Step 1600 avg train loss = 4.0697\n",
            "Step 1700 avg train loss = 4.0769\n",
            "Step 1800 avg train loss = 4.0839\n",
            "Step 1900 avg train loss = 4.0770\n",
            "Step 2000 avg train loss = 4.0966\n",
            "Step 2100 avg train loss = 4.0775\n",
            "Step 2200 avg train loss = 4.0849\n",
            "Step 2300 avg train loss = 4.1133\n",
            "Step 2400 avg train loss = 4.0894\n",
            "Validation loss after 18 epoch = 5.5082\n",
            "Step 0 avg train loss = 3.9998\n",
            "Step 100 avg train loss = 3.9558\n",
            "Step 200 avg train loss = 3.9769\n",
            "Step 300 avg train loss = 3.9715\n",
            "Step 400 avg train loss = 3.9490\n",
            "Step 500 avg train loss = 3.9980\n",
            "Step 600 avg train loss = 4.0008\n",
            "Step 700 avg train loss = 3.9940\n",
            "Step 800 avg train loss = 4.0109\n",
            "Step 900 avg train loss = 4.0060\n",
            "Step 1000 avg train loss = 4.0087\n",
            "Step 1100 avg train loss = 4.0202\n",
            "Step 1200 avg train loss = 4.0125\n",
            "Step 1300 avg train loss = 4.0295\n",
            "Step 1400 avg train loss = 4.0254\n",
            "Step 1500 avg train loss = 4.0366\n",
            "Step 1600 avg train loss = 4.0455\n",
            "Step 1700 avg train loss = 4.0576\n",
            "Step 1800 avg train loss = 4.0540\n",
            "Step 1900 avg train loss = 4.0624\n",
            "Step 2000 avg train loss = 4.0473\n",
            "Step 2100 avg train loss = 4.0440\n",
            "Step 2200 avg train loss = 4.0534\n",
            "Step 2300 avg train loss = 4.0765\n",
            "Step 2400 avg train loss = 4.0659\n",
            "Validation loss after 19 epoch = 5.5362\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 400, padding_idx=2)\n",
            "  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4023\n",
            "Step 100 avg train loss = 7.8250\n",
            "Step 200 avg train loss = 7.0826\n",
            "Step 300 avg train loss = 6.8590\n",
            "Step 400 avg train loss = 6.7079\n",
            "Step 500 avg train loss = 6.6157\n",
            "Step 600 avg train loss = 6.5115\n",
            "Step 700 avg train loss = 6.4588\n",
            "Step 800 avg train loss = 6.3831\n",
            "Step 900 avg train loss = 6.3402\n",
            "Step 1000 avg train loss = 6.2948\n",
            "Step 1100 avg train loss = 6.2216\n",
            "Step 1200 avg train loss = 6.1758\n",
            "Step 1300 avg train loss = 6.1454\n",
            "Step 1400 avg train loss = 6.1283\n",
            "Step 1500 avg train loss = 6.0499\n",
            "Step 1600 avg train loss = 6.0247\n",
            "Step 1700 avg train loss = 6.0261\n",
            "Step 1800 avg train loss = 5.9836\n",
            "Step 1900 avg train loss = 5.9606\n",
            "Step 2000 avg train loss = 5.9391\n",
            "Step 2100 avg train loss = 5.9152\n",
            "Step 2200 avg train loss = 5.9022\n",
            "Step 2300 avg train loss = 5.8728\n",
            "Step 2400 avg train loss = 5.8519\n",
            "Validation loss after 0 epoch = 5.6733\n",
            "Step 0 avg train loss = 5.7023\n",
            "Step 100 avg train loss = 5.7328\n",
            "Step 200 avg train loss = 5.7244\n",
            "Step 300 avg train loss = 5.7013\n",
            "Step 400 avg train loss = 5.6960\n",
            "Step 500 avg train loss = 5.7002\n",
            "Step 600 avg train loss = 5.6611\n",
            "Step 700 avg train loss = 5.6550\n",
            "Step 800 avg train loss = 5.6492\n",
            "Step 900 avg train loss = 5.6442\n",
            "Step 1000 avg train loss = 5.6368\n",
            "Step 1100 avg train loss = 5.6299\n",
            "Step 1200 avg train loss = 5.5984\n",
            "Step 1300 avg train loss = 5.5849\n",
            "Step 1400 avg train loss = 5.5605\n",
            "Step 1500 avg train loss = 5.5807\n",
            "Step 1600 avg train loss = 5.5840\n",
            "Step 1700 avg train loss = 5.5296\n",
            "Step 1800 avg train loss = 5.5472\n",
            "Step 1900 avg train loss = 5.5346\n",
            "Step 2000 avg train loss = 5.5158\n",
            "Step 2100 avg train loss = 5.5128\n",
            "Step 2200 avg train loss = 5.5088\n",
            "Step 2300 avg train loss = 5.4913\n",
            "Step 2400 avg train loss = 5.4905\n",
            "Validation loss after 1 epoch = 5.4120\n",
            "Step 0 avg train loss = 5.3730\n",
            "Step 100 avg train loss = 5.3563\n",
            "Step 200 avg train loss = 5.3515\n",
            "Step 300 avg train loss = 5.3500\n",
            "Step 400 avg train loss = 5.3400\n",
            "Step 500 avg train loss = 5.3386\n",
            "Step 600 avg train loss = 5.3290\n",
            "Step 700 avg train loss = 5.3296\n",
            "Step 800 avg train loss = 5.3612\n",
            "Step 900 avg train loss = 5.3523\n",
            "Step 1000 avg train loss = 5.3082\n",
            "Step 1100 avg train loss = 5.3089\n",
            "Step 1200 avg train loss = 5.3216\n",
            "Step 1300 avg train loss = 5.2978\n",
            "Step 1400 avg train loss = 5.3079\n",
            "Step 1500 avg train loss = 5.3146\n",
            "Step 1600 avg train loss = 5.2861\n",
            "Step 1700 avg train loss = 5.2960\n",
            "Step 1800 avg train loss = 5.2927\n",
            "Step 1900 avg train loss = 5.2708\n",
            "Step 2000 avg train loss = 5.2605\n",
            "Step 2100 avg train loss = 5.2534\n",
            "Step 2200 avg train loss = 5.2604\n",
            "Step 2300 avg train loss = 5.2743\n",
            "Step 2400 avg train loss = 5.2619\n",
            "Validation loss after 2 epoch = 5.2892\n",
            "Step 0 avg train loss = 5.1294\n",
            "Step 100 avg train loss = 5.1191\n",
            "Step 200 avg train loss = 5.1321\n",
            "Step 300 avg train loss = 5.1095\n",
            "Step 400 avg train loss = 5.1091\n",
            "Step 500 avg train loss = 5.1229\n",
            "Step 600 avg train loss = 5.1146\n",
            "Step 700 avg train loss = 5.1116\n",
            "Step 800 avg train loss = 5.1080\n",
            "Step 900 avg train loss = 5.1183\n",
            "Step 1000 avg train loss = 5.1070\n",
            "Step 1100 avg train loss = 5.0849\n",
            "Step 1200 avg train loss = 5.1083\n",
            "Step 1300 avg train loss = 5.0937\n",
            "Step 1400 avg train loss = 5.1064\n",
            "Step 1500 avg train loss = 5.0944\n",
            "Step 1600 avg train loss = 5.0899\n",
            "Step 1700 avg train loss = 5.1022\n",
            "Step 1800 avg train loss = 5.0878\n",
            "Step 1900 avg train loss = 5.0824\n",
            "Step 2000 avg train loss = 5.0657\n",
            "Step 2100 avg train loss = 5.1331\n",
            "Step 2200 avg train loss = 5.1054\n",
            "Step 2300 avg train loss = 5.0902\n",
            "Step 2400 avg train loss = 5.0703\n",
            "Validation loss after 3 epoch = 5.2324\n",
            "Step 0 avg train loss = 5.0786\n",
            "Step 100 avg train loss = 4.9287\n",
            "Step 200 avg train loss = 4.9257\n",
            "Step 300 avg train loss = 4.9590\n",
            "Step 400 avg train loss = 4.9518\n",
            "Step 500 avg train loss = 4.9382\n",
            "Step 600 avg train loss = 4.9572\n",
            "Step 700 avg train loss = 4.9459\n",
            "Step 800 avg train loss = 4.9467\n",
            "Step 900 avg train loss = 4.9565\n",
            "Step 1000 avg train loss = 4.9423\n",
            "Step 1100 avg train loss = 4.9230\n",
            "Step 1200 avg train loss = 4.9397\n",
            "Step 1300 avg train loss = 4.9609\n",
            "Step 1400 avg train loss = 4.9469\n",
            "Step 1500 avg train loss = 4.9456\n",
            "Step 1600 avg train loss = 4.9634\n",
            "Step 1700 avg train loss = 4.9259\n",
            "Step 1800 avg train loss = 4.9511\n",
            "Step 1900 avg train loss = 4.9285\n",
            "Step 2000 avg train loss = 4.9521\n",
            "Step 2100 avg train loss = 4.9684\n",
            "Step 2200 avg train loss = 4.9520\n",
            "Step 2300 avg train loss = 4.9504\n",
            "Step 2400 avg train loss = 4.9484\n",
            "Validation loss after 4 epoch = 5.2095\n",
            "Step 0 avg train loss = 4.7277\n",
            "Step 100 avg train loss = 4.8169\n",
            "Step 200 avg train loss = 4.8071\n",
            "Step 300 avg train loss = 4.8053\n",
            "Step 400 avg train loss = 4.8019\n",
            "Step 500 avg train loss = 4.8195\n",
            "Step 600 avg train loss = 4.8038\n",
            "Step 700 avg train loss = 4.8275\n",
            "Step 800 avg train loss = 4.8186\n",
            "Step 900 avg train loss = 4.8242\n",
            "Step 1000 avg train loss = 4.8007\n",
            "Step 1100 avg train loss = 4.8289\n",
            "Step 1200 avg train loss = 4.8057\n",
            "Step 1300 avg train loss = 4.8150\n",
            "Step 1400 avg train loss = 4.8164\n",
            "Step 1500 avg train loss = 4.8503\n",
            "Step 1600 avg train loss = 4.8219\n",
            "Step 1700 avg train loss = 4.8282\n",
            "Step 1800 avg train loss = 4.8472\n",
            "Step 1900 avg train loss = 4.8101\n",
            "Step 2000 avg train loss = 4.8301\n",
            "Step 2100 avg train loss = 4.8470\n",
            "Step 2200 avg train loss = 4.8083\n",
            "Step 2300 avg train loss = 4.8183\n",
            "Step 2400 avg train loss = 4.8298\n",
            "Validation loss after 5 epoch = 5.2055\n",
            "Step 0 avg train loss = 4.6173\n",
            "Step 100 avg train loss = 4.6568\n",
            "Step 200 avg train loss = 4.6956\n",
            "Step 300 avg train loss = 4.6825\n",
            "Step 400 avg train loss = 4.6839\n",
            "Step 500 avg train loss = 4.6862\n",
            "Step 600 avg train loss = 4.6935\n",
            "Step 700 avg train loss = 4.7037\n",
            "Step 800 avg train loss = 4.7234\n",
            "Step 900 avg train loss = 4.7086\n",
            "Step 1000 avg train loss = 4.7280\n",
            "Step 1100 avg train loss = 4.7211\n",
            "Step 1200 avg train loss = 4.7215\n",
            "Step 1300 avg train loss = 4.7231\n",
            "Step 1400 avg train loss = 4.7352\n",
            "Step 1500 avg train loss = 4.7258\n",
            "Step 1600 avg train loss = 4.7247\n",
            "Step 1700 avg train loss = 4.7210\n",
            "Step 1800 avg train loss = 4.7167\n",
            "Step 1900 avg train loss = 4.7216\n",
            "Step 2000 avg train loss = 4.7386\n",
            "Step 2100 avg train loss = 4.7191\n",
            "Step 2200 avg train loss = 4.7308\n",
            "Step 2300 avg train loss = 4.7362\n",
            "Step 2400 avg train loss = 4.7295\n",
            "Validation loss after 6 epoch = 5.2143\n",
            "Step 0 avg train loss = 4.4967\n",
            "Step 100 avg train loss = 4.5824\n",
            "Step 200 avg train loss = 4.5770\n",
            "Step 300 avg train loss = 4.6073\n",
            "Step 400 avg train loss = 4.6023\n",
            "Step 500 avg train loss = 4.5821\n",
            "Step 600 avg train loss = 4.6027\n",
            "Step 700 avg train loss = 4.6088\n",
            "Step 800 avg train loss = 4.6071\n",
            "Step 900 avg train loss = 4.6067\n",
            "Step 1000 avg train loss = 4.6221\n",
            "Step 1100 avg train loss = 4.6301\n",
            "Step 1200 avg train loss = 4.6302\n",
            "Step 1300 avg train loss = 4.6181\n",
            "Step 1400 avg train loss = 4.6172\n",
            "Step 1500 avg train loss = 4.6505\n",
            "Step 1600 avg train loss = 4.6231\n",
            "Step 1700 avg train loss = 4.6233\n",
            "Step 1800 avg train loss = 4.6405\n",
            "Step 1900 avg train loss = 4.6459\n",
            "Step 2000 avg train loss = 4.6493\n",
            "Step 2100 avg train loss = 4.6637\n",
            "Step 2200 avg train loss = 4.6425\n",
            "Step 2300 avg train loss = 4.6438\n",
            "Step 2400 avg train loss = 4.6461\n",
            "Validation loss after 7 epoch = 5.2345\n",
            "Step 0 avg train loss = 4.5057\n",
            "Step 100 avg train loss = 4.4964\n",
            "Step 200 avg train loss = 4.5077\n",
            "Step 300 avg train loss = 4.5161\n",
            "Step 400 avg train loss = 4.5153\n",
            "Step 500 avg train loss = 4.5260\n",
            "Step 600 avg train loss = 4.5185\n",
            "Step 700 avg train loss = 4.5190\n",
            "Step 800 avg train loss = 4.5210\n",
            "Step 900 avg train loss = 4.5432\n",
            "Step 1000 avg train loss = 4.5360\n",
            "Step 1100 avg train loss = 4.5590\n",
            "Step 1200 avg train loss = 4.5447\n",
            "Step 1300 avg train loss = 4.5484\n",
            "Step 1400 avg train loss = 4.5494\n",
            "Step 1500 avg train loss = 4.5325\n",
            "Step 1600 avg train loss = 4.5551\n",
            "Step 1700 avg train loss = 4.5559\n",
            "Step 1800 avg train loss = 4.5546\n",
            "Step 1900 avg train loss = 4.5681\n",
            "Step 2000 avg train loss = 4.5703\n",
            "Step 2100 avg train loss = 4.5479\n",
            "Step 2200 avg train loss = 4.5708\n",
            "Step 2300 avg train loss = 4.5673\n",
            "Step 2400 avg train loss = 4.5588\n",
            "Validation loss after 8 epoch = 5.2575\n",
            "Step 0 avg train loss = 4.4800\n",
            "Step 100 avg train loss = 4.4264\n",
            "Step 200 avg train loss = 4.4187\n",
            "Step 300 avg train loss = 4.4199\n",
            "Step 400 avg train loss = 4.4340\n",
            "Step 500 avg train loss = 4.4527\n",
            "Step 600 avg train loss = 4.4404\n",
            "Step 700 avg train loss = 4.4569\n",
            "Step 800 avg train loss = 4.4562\n",
            "Step 900 avg train loss = 4.4733\n",
            "Step 1000 avg train loss = 4.4654\n",
            "Step 1100 avg train loss = 4.4655\n",
            "Step 1200 avg train loss = 4.5027\n",
            "Step 1300 avg train loss = 4.4709\n",
            "Step 1400 avg train loss = 4.4676\n",
            "Step 1500 avg train loss = 4.4754\n",
            "Step 1600 avg train loss = 4.4837\n",
            "Step 1700 avg train loss = 4.4964\n",
            "Step 1800 avg train loss = 4.4928\n",
            "Step 1900 avg train loss = 4.4742\n",
            "Step 2000 avg train loss = 4.4999\n",
            "Step 2100 avg train loss = 4.4681\n",
            "Step 2200 avg train loss = 4.4924\n",
            "Step 2300 avg train loss = 4.4783\n",
            "Step 2400 avg train loss = 4.4943\n",
            "Validation loss after 9 epoch = 5.2754\n",
            "Step 0 avg train loss = 4.2336\n",
            "Step 100 avg train loss = 4.3511\n",
            "Step 200 avg train loss = 4.3681\n",
            "Step 300 avg train loss = 4.3397\n",
            "Step 400 avg train loss = 4.3713\n",
            "Step 500 avg train loss = 4.3796\n",
            "Step 600 avg train loss = 4.3920\n",
            "Step 700 avg train loss = 4.3778\n",
            "Step 800 avg train loss = 4.3816\n",
            "Step 900 avg train loss = 4.3859\n",
            "Step 1000 avg train loss = 4.4069\n",
            "Step 1100 avg train loss = 4.4107\n",
            "Step 1200 avg train loss = 4.4034\n",
            "Step 1300 avg train loss = 4.3945\n",
            "Step 1400 avg train loss = 4.4096\n",
            "Step 1500 avg train loss = 4.4292\n",
            "Step 1600 avg train loss = 4.4149\n",
            "Step 1700 avg train loss = 4.4073\n",
            "Step 1800 avg train loss = 4.4219\n",
            "Step 1900 avg train loss = 4.4168\n",
            "Step 2000 avg train loss = 4.4408\n",
            "Step 2100 avg train loss = 4.4232\n",
            "Step 2200 avg train loss = 4.4429\n",
            "Step 2300 avg train loss = 4.4264\n",
            "Step 2400 avg train loss = 4.4314\n",
            "Validation loss after 10 epoch = 5.3052\n",
            "Step 0 avg train loss = 4.0679\n",
            "Step 100 avg train loss = 4.2737\n",
            "Step 200 avg train loss = 4.2835\n",
            "Step 300 avg train loss = 4.3065\n",
            "Step 400 avg train loss = 4.3031\n",
            "Step 500 avg train loss = 4.3113\n",
            "Step 600 avg train loss = 4.3217\n",
            "Step 700 avg train loss = 4.3066\n",
            "Step 800 avg train loss = 4.3148\n",
            "Step 900 avg train loss = 4.3189\n",
            "Step 1000 avg train loss = 4.3407\n",
            "Step 1100 avg train loss = 4.3562\n",
            "Step 1200 avg train loss = 4.3371\n",
            "Step 1300 avg train loss = 4.3485\n",
            "Step 1400 avg train loss = 4.3647\n",
            "Step 1500 avg train loss = 4.3523\n",
            "Step 1600 avg train loss = 4.3602\n",
            "Step 1700 avg train loss = 4.3741\n",
            "Step 1800 avg train loss = 4.3640\n",
            "Step 1900 avg train loss = 4.3903\n",
            "Step 2000 avg train loss = 4.3647\n",
            "Step 2100 avg train loss = 4.3828\n",
            "Step 2200 avg train loss = 4.3678\n",
            "Step 2300 avg train loss = 4.3636\n",
            "Step 2400 avg train loss = 4.3780\n",
            "Validation loss after 11 epoch = 5.3374\n",
            "Step 0 avg train loss = 4.1806\n",
            "Step 100 avg train loss = 4.2266\n",
            "Step 200 avg train loss = 4.2401\n",
            "Step 300 avg train loss = 4.2223\n",
            "Step 400 avg train loss = 4.2504\n",
            "Step 500 avg train loss = 4.2576\n",
            "Step 600 avg train loss = 4.2517\n",
            "Step 700 avg train loss = 4.2779\n",
            "Step 800 avg train loss = 4.2727\n",
            "Step 900 avg train loss = 4.2747\n",
            "Step 1000 avg train loss = 4.2642\n",
            "Step 1100 avg train loss = 4.2626\n",
            "Step 1200 avg train loss = 4.2924\n",
            "Step 1300 avg train loss = 4.2941\n",
            "Step 1400 avg train loss = 4.2995\n",
            "Step 1500 avg train loss = 4.2985\n",
            "Step 1600 avg train loss = 4.3283\n",
            "Step 1700 avg train loss = 4.3317\n",
            "Step 1800 avg train loss = 4.3120\n",
            "Step 1900 avg train loss = 4.3245\n",
            "Step 2000 avg train loss = 4.3050\n",
            "Step 2100 avg train loss = 4.3261\n",
            "Step 2200 avg train loss = 4.3321\n",
            "Step 2300 avg train loss = 4.3333\n",
            "Step 2400 avg train loss = 4.2995\n",
            "Validation loss after 12 epoch = 5.3689\n",
            "Step 0 avg train loss = 3.9983\n",
            "Step 100 avg train loss = 4.1634\n",
            "Step 200 avg train loss = 4.1947\n",
            "Step 300 avg train loss = 4.2063\n",
            "Step 400 avg train loss = 4.2014\n",
            "Step 500 avg train loss = 4.1926\n",
            "Step 600 avg train loss = 4.2148\n",
            "Step 700 avg train loss = 4.2078\n",
            "Step 800 avg train loss = 4.2368\n",
            "Step 900 avg train loss = 4.2238\n",
            "Step 1000 avg train loss = 4.2245\n",
            "Step 1100 avg train loss = 4.2284\n",
            "Step 1200 avg train loss = 4.2186\n",
            "Step 1300 avg train loss = 4.2408\n",
            "Step 1400 avg train loss = 4.2398\n",
            "Step 1500 avg train loss = 4.2612\n",
            "Step 1600 avg train loss = 4.2374\n",
            "Step 1700 avg train loss = 4.2797\n",
            "Step 1800 avg train loss = 4.2634\n",
            "Step 1900 avg train loss = 4.2838\n",
            "Step 2000 avg train loss = 4.2659\n",
            "Step 2100 avg train loss = 4.2516\n",
            "Step 2200 avg train loss = 4.2725\n",
            "Step 2300 avg train loss = 4.2687\n",
            "Step 2400 avg train loss = 4.2819\n",
            "Validation loss after 13 epoch = 5.3945\n",
            "Step 0 avg train loss = 4.0700\n",
            "Step 100 avg train loss = 4.1193\n",
            "Step 200 avg train loss = 4.1229\n",
            "Step 300 avg train loss = 4.1187\n",
            "Step 400 avg train loss = 4.1553\n",
            "Step 500 avg train loss = 4.1485\n",
            "Step 600 avg train loss = 4.1475\n",
            "Step 700 avg train loss = 4.1741\n",
            "Step 800 avg train loss = 4.1715\n",
            "Step 900 avg train loss = 4.1588\n",
            "Step 1000 avg train loss = 4.1828\n",
            "Step 1100 avg train loss = 4.2007\n",
            "Step 1200 avg train loss = 4.1766\n",
            "Step 1300 avg train loss = 4.2048\n",
            "Step 1400 avg train loss = 4.2149\n",
            "Step 1500 avg train loss = 4.2264\n",
            "Step 1600 avg train loss = 4.2042\n",
            "Step 1700 avg train loss = 4.2115\n",
            "Step 1800 avg train loss = 4.2200\n",
            "Step 1900 avg train loss = 4.1981\n",
            "Step 2000 avg train loss = 4.2197\n",
            "Step 2100 avg train loss = 4.2273\n",
            "Step 2200 avg train loss = 4.2347\n",
            "Step 2300 avg train loss = 4.2447\n",
            "Step 2400 avg train loss = 4.2305\n",
            "Validation loss after 14 epoch = 5.4292\n",
            "Step 0 avg train loss = 4.1498\n",
            "Step 100 avg train loss = 4.0650\n",
            "Step 200 avg train loss = 4.0763\n",
            "Step 300 avg train loss = 4.0947\n",
            "Step 400 avg train loss = 4.0923\n",
            "Step 500 avg train loss = 4.0890\n",
            "Step 600 avg train loss = 4.1183\n",
            "Step 700 avg train loss = 4.1046\n",
            "Step 800 avg train loss = 4.1245\n",
            "Step 900 avg train loss = 4.1294\n",
            "Step 1000 avg train loss = 4.1524\n",
            "Step 1100 avg train loss = 4.1669\n",
            "Step 1200 avg train loss = 4.1534\n",
            "Step 1300 avg train loss = 4.1338\n",
            "Step 1400 avg train loss = 4.1487\n",
            "Step 1500 avg train loss = 4.1673\n",
            "Step 1600 avg train loss = 4.1707\n",
            "Step 1700 avg train loss = 4.1834\n",
            "Step 1800 avg train loss = 4.1754\n",
            "Step 1900 avg train loss = 4.1873\n",
            "Step 2000 avg train loss = 4.1936\n",
            "Step 2100 avg train loss = 4.1599\n",
            "Step 2200 avg train loss = 4.1868\n",
            "Step 2300 avg train loss = 4.2023\n",
            "Step 2400 avg train loss = 4.1757\n",
            "Validation loss after 15 epoch = 5.4660\n",
            "Step 0 avg train loss = 4.0273\n",
            "Step 100 avg train loss = 4.0220\n",
            "Step 200 avg train loss = 4.0549\n",
            "Step 300 avg train loss = 4.0614\n",
            "Step 400 avg train loss = 4.0537\n",
            "Step 500 avg train loss = 4.0514\n",
            "Step 600 avg train loss = 4.0865\n",
            "Step 700 avg train loss = 4.0546\n",
            "Step 800 avg train loss = 4.0985\n",
            "Step 900 avg train loss = 4.1136\n",
            "Step 1000 avg train loss = 4.0781\n",
            "Step 1100 avg train loss = 4.1131\n",
            "Step 1200 avg train loss = 4.1265\n",
            "Step 1300 avg train loss = 4.1093\n",
            "Step 1400 avg train loss = 4.0922\n",
            "Step 1500 avg train loss = 4.1161\n",
            "Step 1600 avg train loss = 4.1181\n",
            "Step 1700 avg train loss = 4.1335\n",
            "Step 1800 avg train loss = 4.1290\n",
            "Step 1900 avg train loss = 4.1249\n",
            "Step 2000 avg train loss = 4.1470\n",
            "Step 2100 avg train loss = 4.1427\n",
            "Step 2200 avg train loss = 4.1465\n",
            "Step 2300 avg train loss = 4.1532\n",
            "Step 2400 avg train loss = 4.1267\n",
            "Validation loss after 16 epoch = 5.4926\n",
            "Step 0 avg train loss = 4.0648\n",
            "Step 100 avg train loss = 3.9927\n",
            "Step 200 avg train loss = 3.9955\n",
            "Step 300 avg train loss = 3.9995\n",
            "Step 400 avg train loss = 4.0166\n",
            "Step 500 avg train loss = 4.0329\n",
            "Step 600 avg train loss = 4.0125\n",
            "Step 700 avg train loss = 4.0438\n",
            "Step 800 avg train loss = 4.0544\n",
            "Step 900 avg train loss = 4.0643\n",
            "Step 1000 avg train loss = 4.0626\n",
            "Step 1100 avg train loss = 4.0750\n",
            "Step 1200 avg train loss = 4.0636\n",
            "Step 1300 avg train loss = 4.0804\n",
            "Step 1400 avg train loss = 4.0882\n",
            "Step 1500 avg train loss = 4.0944\n",
            "Step 1600 avg train loss = 4.0834\n",
            "Step 1700 avg train loss = 4.1077\n",
            "Step 1800 avg train loss = 4.0756\n",
            "Step 1900 avg train loss = 4.0918\n",
            "Step 2000 avg train loss = 4.0872\n",
            "Step 2100 avg train loss = 4.0858\n",
            "Step 2200 avg train loss = 4.0997\n",
            "Step 2300 avg train loss = 4.1257\n",
            "Step 2400 avg train loss = 4.1121\n",
            "Validation loss after 17 epoch = 5.5328\n",
            "Step 0 avg train loss = 3.9079\n",
            "Step 100 avg train loss = 3.9593\n",
            "Step 200 avg train loss = 3.9547\n",
            "Step 300 avg train loss = 3.9700\n",
            "Step 400 avg train loss = 4.0013\n",
            "Step 500 avg train loss = 3.9737\n",
            "Step 600 avg train loss = 3.9953\n",
            "Step 700 avg train loss = 4.0029\n",
            "Step 800 avg train loss = 4.0087\n",
            "Step 900 avg train loss = 4.0131\n",
            "Step 1000 avg train loss = 4.0181\n",
            "Step 1100 avg train loss = 4.0232\n",
            "Step 1200 avg train loss = 4.0233\n",
            "Step 1300 avg train loss = 4.0317\n",
            "Step 1400 avg train loss = 4.0616\n",
            "Step 1500 avg train loss = 4.0395\n",
            "Step 1600 avg train loss = 4.0304\n",
            "Step 1700 avg train loss = 4.0634\n",
            "Step 1800 avg train loss = 4.0594\n",
            "Step 1900 avg train loss = 4.0613\n",
            "Step 2000 avg train loss = 4.0799\n",
            "Step 2100 avg train loss = 4.0716\n",
            "Step 2200 avg train loss = 4.0788\n",
            "Step 2300 avg train loss = 4.0840\n",
            "Step 2400 avg train loss = 4.0640\n",
            "Validation loss after 18 epoch = 5.5573\n",
            "Step 0 avg train loss = 3.9781\n",
            "Step 100 avg train loss = 3.9200\n",
            "Step 200 avg train loss = 3.9366\n",
            "Step 300 avg train loss = 3.9145\n",
            "Step 400 avg train loss = 3.9230\n",
            "Step 500 avg train loss = 3.9579\n",
            "Step 600 avg train loss = 3.9589\n",
            "Step 700 avg train loss = 3.9782\n",
            "Step 800 avg train loss = 3.9825\n",
            "Step 900 avg train loss = 4.0048\n",
            "Step 1000 avg train loss = 3.9992\n",
            "Step 1100 avg train loss = 3.9936\n",
            "Step 1200 avg train loss = 4.0119\n",
            "Step 1300 avg train loss = 4.0024\n",
            "Step 1400 avg train loss = 4.0293\n",
            "Step 1500 avg train loss = 4.0182\n",
            "Step 1600 avg train loss = 4.0272\n",
            "Step 1700 avg train loss = 4.0067\n",
            "Step 1800 avg train loss = 4.0090\n",
            "Step 1900 avg train loss = 4.0225\n",
            "Step 2000 avg train loss = 4.0266\n",
            "Step 2100 avg train loss = 4.0301\n",
            "Step 2200 avg train loss = 4.0415\n",
            "Step 2300 avg train loss = 4.0565\n",
            "Step 2400 avg train loss = 4.0321\n",
            "Validation loss after 19 epoch = 5.5895\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 500, padding_idx=2)\n",
            "  (lstm): LSTM(500, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=128, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4104\n",
            "Step 100 avg train loss = 7.8140\n",
            "Step 200 avg train loss = 7.1067\n",
            "Step 300 avg train loss = 6.9097\n",
            "Step 400 avg train loss = 6.7177\n",
            "Step 500 avg train loss = 6.5911\n",
            "Step 600 avg train loss = 6.5139\n",
            "Step 700 avg train loss = 6.4147\n",
            "Step 800 avg train loss = 6.3649\n",
            "Step 900 avg train loss = 6.3038\n",
            "Step 1000 avg train loss = 6.2399\n",
            "Step 1100 avg train loss = 6.1846\n",
            "Step 1200 avg train loss = 6.1361\n",
            "Step 1300 avg train loss = 6.1052\n",
            "Step 1400 avg train loss = 6.0749\n",
            "Step 1500 avg train loss = 6.0425\n",
            "Step 1600 avg train loss = 6.0194\n",
            "Step 1700 avg train loss = 5.9650\n",
            "Step 1800 avg train loss = 5.9335\n",
            "Step 1900 avg train loss = 5.9212\n",
            "Step 2000 avg train loss = 5.8957\n",
            "Step 2100 avg train loss = 5.8850\n",
            "Step 2200 avg train loss = 5.8551\n",
            "Step 2300 avg train loss = 5.8543\n",
            "Step 2400 avg train loss = 5.8184\n",
            "Validation loss after 0 epoch = 5.6485\n",
            "Step 0 avg train loss = 5.8281\n",
            "Step 100 avg train loss = 5.7175\n",
            "Step 200 avg train loss = 5.7072\n",
            "Step 300 avg train loss = 5.6913\n",
            "Step 400 avg train loss = 5.6363\n",
            "Step 500 avg train loss = 5.6574\n",
            "Step 600 avg train loss = 5.6538\n",
            "Step 700 avg train loss = 5.6532\n",
            "Step 800 avg train loss = 5.6453\n",
            "Step 900 avg train loss = 5.6162\n",
            "Step 1000 avg train loss = 5.6160\n",
            "Step 1100 avg train loss = 5.5937\n",
            "Step 1200 avg train loss = 5.5852\n",
            "Step 1300 avg train loss = 5.5843\n",
            "Step 1400 avg train loss = 5.5713\n",
            "Step 1500 avg train loss = 5.5722\n",
            "Step 1600 avg train loss = 5.5399\n",
            "Step 1700 avg train loss = 5.5537\n",
            "Step 1800 avg train loss = 5.5514\n",
            "Step 1900 avg train loss = 5.5409\n",
            "Step 2000 avg train loss = 5.5255\n",
            "Step 2100 avg train loss = 5.4920\n",
            "Step 2200 avg train loss = 5.5099\n",
            "Step 2300 avg train loss = 5.4721\n",
            "Step 2400 avg train loss = 5.4798\n",
            "Validation loss after 1 epoch = 5.4089\n",
            "Step 0 avg train loss = 5.4125\n",
            "Step 100 avg train loss = 5.3591\n",
            "Step 200 avg train loss = 5.3388\n",
            "Step 300 avg train loss = 5.3616\n",
            "Step 400 avg train loss = 5.3527\n",
            "Step 500 avg train loss = 5.3481\n",
            "Step 600 avg train loss = 5.3444\n",
            "Step 700 avg train loss = 5.3326\n",
            "Step 800 avg train loss = 5.3339\n",
            "Step 900 avg train loss = 5.3152\n",
            "Step 1000 avg train loss = 5.3282\n",
            "Step 1100 avg train loss = 5.3124\n",
            "Step 1200 avg train loss = 5.3176\n",
            "Step 1300 avg train loss = 5.2889\n",
            "Step 1400 avg train loss = 5.3018\n",
            "Step 1500 avg train loss = 5.3203\n",
            "Step 1600 avg train loss = 5.2899\n",
            "Step 1700 avg train loss = 5.2979\n",
            "Step 1800 avg train loss = 5.3036\n",
            "Step 1900 avg train loss = 5.3054\n",
            "Step 2000 avg train loss = 5.2615\n",
            "Step 2100 avg train loss = 5.2804\n",
            "Step 2200 avg train loss = 5.2600\n",
            "Step 2300 avg train loss = 5.2708\n",
            "Step 2400 avg train loss = 5.2673\n",
            "Validation loss after 2 epoch = 5.3074\n",
            "Step 0 avg train loss = 5.0767\n",
            "Step 100 avg train loss = 5.1266\n",
            "Step 200 avg train loss = 5.1151\n",
            "Step 300 avg train loss = 5.1240\n",
            "Step 400 avg train loss = 5.1216\n",
            "Step 500 avg train loss = 5.1241\n",
            "Step 600 avg train loss = 5.1294\n",
            "Step 700 avg train loss = 5.0884\n",
            "Step 800 avg train loss = 5.1153\n",
            "Step 900 avg train loss = 5.1383\n",
            "Step 1000 avg train loss = 5.1243\n",
            "Step 1100 avg train loss = 5.1352\n",
            "Step 1200 avg train loss = 5.1264\n",
            "Step 1300 avg train loss = 5.1205\n",
            "Step 1400 avg train loss = 5.1058\n",
            "Step 1500 avg train loss = 5.1228\n",
            "Step 1600 avg train loss = 5.1138\n",
            "Step 1700 avg train loss = 5.0996\n",
            "Step 1800 avg train loss = 5.0850\n",
            "Step 1900 avg train loss = 5.1024\n",
            "Step 2000 avg train loss = 5.1001\n",
            "Step 2100 avg train loss = 5.0977\n",
            "Step 2200 avg train loss = 5.1067\n",
            "Step 2300 avg train loss = 5.1118\n",
            "Step 2400 avg train loss = 5.0855\n",
            "Validation loss after 3 epoch = 5.2516\n",
            "Step 0 avg train loss = 4.8916\n",
            "Step 100 avg train loss = 4.9340\n",
            "Step 200 avg train loss = 4.9251\n",
            "Step 300 avg train loss = 4.9643\n",
            "Step 400 avg train loss = 4.9438\n",
            "Step 500 avg train loss = 4.9536\n",
            "Step 600 avg train loss = 4.9610\n",
            "Step 700 avg train loss = 4.9635\n",
            "Step 800 avg train loss = 4.9538\n",
            "Step 900 avg train loss = 4.9599\n",
            "Step 1000 avg train loss = 4.9603\n",
            "Step 1100 avg train loss = 4.9615\n",
            "Step 1200 avg train loss = 4.9598\n",
            "Step 1300 avg train loss = 4.9628\n",
            "Step 1400 avg train loss = 4.9625\n",
            "Step 1500 avg train loss = 4.9733\n",
            "Step 1600 avg train loss = 4.9727\n",
            "Step 1700 avg train loss = 4.9474\n",
            "Step 1800 avg train loss = 4.9597\n",
            "Step 1900 avg train loss = 4.9616\n",
            "Step 2000 avg train loss = 4.9618\n",
            "Step 2100 avg train loss = 4.9740\n",
            "Step 2200 avg train loss = 4.9539\n",
            "Step 2300 avg train loss = 4.9563\n",
            "Step 2400 avg train loss = 4.9462\n",
            "Validation loss after 4 epoch = 5.2255\n",
            "Step 0 avg train loss = 4.8789\n",
            "Step 100 avg train loss = 4.8030\n",
            "Step 200 avg train loss = 4.7898\n",
            "Step 300 avg train loss = 4.8188\n",
            "Step 400 avg train loss = 4.7895\n",
            "Step 500 avg train loss = 4.8342\n",
            "Step 600 avg train loss = 4.8247\n",
            "Step 700 avg train loss = 4.8340\n",
            "Step 800 avg train loss = 4.8207\n",
            "Step 900 avg train loss = 4.8372\n",
            "Step 1000 avg train loss = 4.8188\n",
            "Step 1100 avg train loss = 4.8174\n",
            "Step 1200 avg train loss = 4.8070\n",
            "Step 1300 avg train loss = 4.8299\n",
            "Step 1400 avg train loss = 4.8296\n",
            "Step 1500 avg train loss = 4.8325\n",
            "Step 1600 avg train loss = 4.8128\n",
            "Step 1700 avg train loss = 4.8312\n",
            "Step 1800 avg train loss = 4.8536\n",
            "Step 1900 avg train loss = 4.8203\n",
            "Step 2000 avg train loss = 4.8576\n",
            "Step 2100 avg train loss = 4.8289\n",
            "Step 2200 avg train loss = 4.8614\n",
            "Step 2300 avg train loss = 4.8477\n",
            "Step 2400 avg train loss = 4.8676\n",
            "Validation loss after 5 epoch = 5.2284\n",
            "Step 0 avg train loss = 4.5773\n",
            "Step 100 avg train loss = 4.6798\n",
            "Step 200 avg train loss = 4.7082\n",
            "Step 300 avg train loss = 4.6748\n",
            "Step 400 avg train loss = 4.7208\n",
            "Step 500 avg train loss = 4.6965\n",
            "Step 600 avg train loss = 4.7072\n",
            "Step 700 avg train loss = 4.7018\n",
            "Step 800 avg train loss = 4.7185\n",
            "Step 900 avg train loss = 4.7290\n",
            "Step 1000 avg train loss = 4.7098\n",
            "Step 1100 avg train loss = 4.7224\n",
            "Step 1200 avg train loss = 4.7319\n",
            "Step 1300 avg train loss = 4.7295\n",
            "Step 1400 avg train loss = 4.7316\n",
            "Step 1500 avg train loss = 4.7341\n",
            "Step 1600 avg train loss = 4.7190\n",
            "Step 1700 avg train loss = 4.7110\n",
            "Step 1800 avg train loss = 4.7179\n",
            "Step 1900 avg train loss = 4.7443\n",
            "Step 2000 avg train loss = 4.7150\n",
            "Step 2100 avg train loss = 4.7446\n",
            "Step 2200 avg train loss = 4.7489\n",
            "Step 2300 avg train loss = 4.7345\n",
            "Step 2400 avg train loss = 4.7348\n",
            "Validation loss after 6 epoch = 5.2331\n",
            "Step 0 avg train loss = 4.8415\n",
            "Step 100 avg train loss = 4.5721\n",
            "Step 200 avg train loss = 4.5975\n",
            "Step 300 avg train loss = 4.6016\n",
            "Step 400 avg train loss = 4.5927\n",
            "Step 500 avg train loss = 4.6195\n",
            "Step 600 avg train loss = 4.6020\n",
            "Step 700 avg train loss = 4.6284\n",
            "Step 800 avg train loss = 4.6068\n",
            "Step 900 avg train loss = 4.6244\n",
            "Step 1000 avg train loss = 4.6261\n",
            "Step 1100 avg train loss = 4.6268\n",
            "Step 1200 avg train loss = 4.6069\n",
            "Step 1300 avg train loss = 4.6294\n",
            "Step 1400 avg train loss = 4.6227\n",
            "Step 1500 avg train loss = 4.6318\n",
            "Step 1600 avg train loss = 4.6364\n",
            "Step 1700 avg train loss = 4.6440\n",
            "Step 1800 avg train loss = 4.6583\n",
            "Step 1900 avg train loss = 4.6565\n",
            "Step 2000 avg train loss = 4.6325\n",
            "Step 2100 avg train loss = 4.6490\n",
            "Step 2200 avg train loss = 4.6406\n",
            "Step 2300 avg train loss = 4.6380\n",
            "Step 2400 avg train loss = 4.6417\n",
            "Validation loss after 7 epoch = 5.2567\n",
            "Step 0 avg train loss = 4.5436\n",
            "Step 100 avg train loss = 4.4922\n",
            "Step 200 avg train loss = 4.4902\n",
            "Step 300 avg train loss = 4.5199\n",
            "Step 400 avg train loss = 4.5199\n",
            "Step 500 avg train loss = 4.5139\n",
            "Step 600 avg train loss = 4.5100\n",
            "Step 700 avg train loss = 4.5091\n",
            "Step 800 avg train loss = 4.5325\n",
            "Step 900 avg train loss = 4.5307\n",
            "Step 1000 avg train loss = 4.5282\n",
            "Step 1100 avg train loss = 4.5397\n",
            "Step 1200 avg train loss = 4.5475\n",
            "Step 1300 avg train loss = 4.5560\n",
            "Step 1400 avg train loss = 4.5652\n",
            "Step 1500 avg train loss = 4.5537\n",
            "Step 1600 avg train loss = 4.5599\n",
            "Step 1700 avg train loss = 4.5678\n",
            "Step 1800 avg train loss = 4.5613\n",
            "Step 1900 avg train loss = 4.5457\n",
            "Step 2000 avg train loss = 4.5818\n",
            "Step 2100 avg train loss = 4.5654\n",
            "Step 2200 avg train loss = 4.5745\n",
            "Step 2300 avg train loss = 4.5649\n",
            "Step 2400 avg train loss = 4.5682\n",
            "Validation loss after 8 epoch = 5.2793\n",
            "Step 0 avg train loss = 4.1581\n",
            "Step 100 avg train loss = 4.3905\n",
            "Step 200 avg train loss = 4.4177\n",
            "Step 300 avg train loss = 4.4165\n",
            "Step 400 avg train loss = 4.4440\n",
            "Step 500 avg train loss = 4.4392\n",
            "Step 600 avg train loss = 4.4577\n",
            "Step 700 avg train loss = 4.4636\n",
            "Step 800 avg train loss = 4.4497\n",
            "Step 900 avg train loss = 4.4558\n",
            "Step 1000 avg train loss = 4.4725\n",
            "Step 1100 avg train loss = 4.4647\n",
            "Step 1200 avg train loss = 4.4746\n",
            "Step 1300 avg train loss = 4.4898\n",
            "Step 1400 avg train loss = 4.4802\n",
            "Step 1500 avg train loss = 4.4724\n",
            "Step 1600 avg train loss = 4.4944\n",
            "Step 1700 avg train loss = 4.4806\n",
            "Step 1800 avg train loss = 4.4810\n",
            "Step 1900 avg train loss = 4.5078\n",
            "Step 2000 avg train loss = 4.4798\n",
            "Step 2100 avg train loss = 4.4784\n",
            "Step 2200 avg train loss = 4.4975\n",
            "Step 2300 avg train loss = 4.5086\n",
            "Step 2400 avg train loss = 4.5145\n",
            "Validation loss after 9 epoch = 5.3026\n",
            "Step 0 avg train loss = 4.3879\n",
            "Step 100 avg train loss = 4.3463\n",
            "Step 200 avg train loss = 4.3374\n",
            "Step 300 avg train loss = 4.3608\n",
            "Step 400 avg train loss = 4.3780\n",
            "Step 500 avg train loss = 4.3618\n",
            "Step 600 avg train loss = 4.3782\n",
            "Step 700 avg train loss = 4.3804\n",
            "Step 800 avg train loss = 4.3948\n",
            "Step 900 avg train loss = 4.3831\n",
            "Step 1000 avg train loss = 4.3858\n",
            "Step 1100 avg train loss = 4.3975\n",
            "Step 1200 avg train loss = 4.4239\n",
            "Step 1300 avg train loss = 4.4028\n",
            "Step 1400 avg train loss = 4.3959\n",
            "Step 1500 avg train loss = 4.4076\n",
            "Step 1600 avg train loss = 4.4203\n",
            "Step 1700 avg train loss = 4.4108\n",
            "Step 1800 avg train loss = 4.4262\n",
            "Step 1900 avg train loss = 4.4142\n",
            "Step 2000 avg train loss = 4.4230\n",
            "Step 2100 avg train loss = 4.4381\n",
            "Step 2200 avg train loss = 4.4578\n",
            "Step 2300 avg train loss = 4.4329\n",
            "Step 2400 avg train loss = 4.4334\n",
            "Validation loss after 10 epoch = 5.3355\n",
            "Step 0 avg train loss = 4.1833\n",
            "Step 100 avg train loss = 4.2862\n",
            "Step 200 avg train loss = 4.2857\n",
            "Step 300 avg train loss = 4.2926\n",
            "Step 400 avg train loss = 4.2991\n",
            "Step 500 avg train loss = 4.3355\n",
            "Step 600 avg train loss = 4.3363\n",
            "Step 700 avg train loss = 4.3314\n",
            "Step 800 avg train loss = 4.3262\n",
            "Step 900 avg train loss = 4.3328\n",
            "Step 1000 avg train loss = 4.3232\n",
            "Step 1100 avg train loss = 4.3425\n",
            "Step 1200 avg train loss = 4.3417\n",
            "Step 1300 avg train loss = 4.3405\n",
            "Step 1400 avg train loss = 4.3359\n",
            "Step 1500 avg train loss = 4.3534\n",
            "Step 1600 avg train loss = 4.3499\n",
            "Step 1700 avg train loss = 4.3575\n",
            "Step 1800 avg train loss = 4.3692\n",
            "Step 1900 avg train loss = 4.3625\n",
            "Step 2000 avg train loss = 4.3554\n",
            "Step 2100 avg train loss = 4.3652\n",
            "Step 2200 avg train loss = 4.3823\n",
            "Step 2300 avg train loss = 4.3568\n",
            "Step 2400 avg train loss = 4.3828\n",
            "Validation loss after 11 epoch = 5.3643\n",
            "Step 0 avg train loss = 3.9015\n",
            "Step 100 avg train loss = 4.2156\n",
            "Step 200 avg train loss = 4.2325\n",
            "Step 300 avg train loss = 4.2363\n",
            "Step 400 avg train loss = 4.2456\n",
            "Step 500 avg train loss = 4.2602\n",
            "Step 600 avg train loss = 4.2600\n",
            "Step 700 avg train loss = 4.2783\n",
            "Step 800 avg train loss = 4.2670\n",
            "Step 900 avg train loss = 4.2730\n",
            "Step 1000 avg train loss = 4.2804\n",
            "Step 1100 avg train loss = 4.2945\n",
            "Step 1200 avg train loss = 4.2802\n",
            "Step 1300 avg train loss = 4.2655\n",
            "Step 1400 avg train loss = 4.2836\n",
            "Step 1500 avg train loss = 4.3148\n",
            "Step 1600 avg train loss = 4.3065\n",
            "Step 1700 avg train loss = 4.3043\n",
            "Step 1800 avg train loss = 4.3097\n",
            "Step 1900 avg train loss = 4.3220\n",
            "Step 2000 avg train loss = 4.2960\n",
            "Step 2100 avg train loss = 4.3156\n",
            "Step 2200 avg train loss = 4.3082\n",
            "Step 2300 avg train loss = 4.3340\n",
            "Step 2400 avg train loss = 4.3314\n",
            "Validation loss after 12 epoch = 5.4011\n",
            "Step 0 avg train loss = 4.2097\n",
            "Step 100 avg train loss = 4.1546\n",
            "Step 200 avg train loss = 4.1888\n",
            "Step 300 avg train loss = 4.2055\n",
            "Step 400 avg train loss = 4.1842\n",
            "Step 500 avg train loss = 4.1929\n",
            "Step 600 avg train loss = 4.1959\n",
            "Step 700 avg train loss = 4.2058\n",
            "Step 800 avg train loss = 4.2134\n",
            "Step 900 avg train loss = 4.2351\n",
            "Step 1000 avg train loss = 4.2454\n",
            "Step 1100 avg train loss = 4.2207\n",
            "Step 1200 avg train loss = 4.2419\n",
            "Step 1300 avg train loss = 4.2376\n",
            "Step 1400 avg train loss = 4.2236\n",
            "Step 1500 avg train loss = 4.2341\n",
            "Step 1600 avg train loss = 4.2361\n",
            "Step 1700 avg train loss = 4.2438\n",
            "Step 1800 avg train loss = 4.2593\n",
            "Step 1900 avg train loss = 4.2627\n",
            "Step 2000 avg train loss = 4.2648\n",
            "Step 2100 avg train loss = 4.2647\n",
            "Step 2200 avg train loss = 4.2852\n",
            "Step 2300 avg train loss = 4.2904\n",
            "Step 2400 avg train loss = 4.2588\n",
            "Validation loss after 13 epoch = 5.4329\n",
            "Step 0 avg train loss = 3.9968\n",
            "Step 100 avg train loss = 4.1199\n",
            "Step 200 avg train loss = 4.1053\n",
            "Step 300 avg train loss = 4.1341\n",
            "Step 400 avg train loss = 4.1318\n",
            "Step 500 avg train loss = 4.1580\n",
            "Step 600 avg train loss = 4.1598\n",
            "Step 700 avg train loss = 4.1517\n",
            "Step 800 avg train loss = 4.1523\n",
            "Step 900 avg train loss = 4.1582\n",
            "Step 1000 avg train loss = 4.1723\n",
            "Step 1100 avg train loss = 4.1901\n",
            "Step 1200 avg train loss = 4.1927\n",
            "Step 1300 avg train loss = 4.1922\n",
            "Step 1400 avg train loss = 4.2144\n",
            "Step 1500 avg train loss = 4.2117\n",
            "Step 1600 avg train loss = 4.2004\n",
            "Step 1700 avg train loss = 4.2220\n",
            "Step 1800 avg train loss = 4.2221\n",
            "Step 1900 avg train loss = 4.2202\n",
            "Step 2000 avg train loss = 4.1999\n",
            "Step 2100 avg train loss = 4.2317\n",
            "Step 2200 avg train loss = 4.2206\n",
            "Step 2300 avg train loss = 4.2274\n",
            "Step 2400 avg train loss = 4.2367\n",
            "Validation loss after 14 epoch = 5.4609\n",
            "Step 0 avg train loss = 4.1878\n",
            "Step 100 avg train loss = 4.0696\n",
            "Step 200 avg train loss = 4.0643\n",
            "Step 300 avg train loss = 4.1074\n",
            "Step 400 avg train loss = 4.1168\n",
            "Step 500 avg train loss = 4.1210\n",
            "Step 600 avg train loss = 4.1072\n",
            "Step 700 avg train loss = 4.1094\n",
            "Step 800 avg train loss = 4.1289\n",
            "Step 900 avg train loss = 4.1048\n",
            "Step 1000 avg train loss = 4.1479\n",
            "Step 1100 avg train loss = 4.1435\n",
            "Step 1200 avg train loss = 4.1333\n",
            "Step 1300 avg train loss = 4.1374\n",
            "Step 1400 avg train loss = 4.1694\n",
            "Step 1500 avg train loss = 4.1767\n",
            "Step 1600 avg train loss = 4.1574\n",
            "Step 1700 avg train loss = 4.1465\n",
            "Step 1800 avg train loss = 4.1458\n",
            "Step 1900 avg train loss = 4.1754\n",
            "Step 2000 avg train loss = 4.1502\n",
            "Step 2100 avg train loss = 4.1709\n",
            "Step 2200 avg train loss = 4.1770\n",
            "Step 2300 avg train loss = 4.1867\n",
            "Step 2400 avg train loss = 4.1882\n",
            "Validation loss after 15 epoch = 5.4983\n",
            "Step 0 avg train loss = 4.1218\n",
            "Step 100 avg train loss = 4.0318\n",
            "Step 200 avg train loss = 4.0299\n",
            "Step 300 avg train loss = 4.0406\n",
            "Step 400 avg train loss = 4.0497\n",
            "Step 500 avg train loss = 4.0483\n",
            "Step 600 avg train loss = 4.0781\n",
            "Step 700 avg train loss = 4.0655\n",
            "Step 800 avg train loss = 4.0720\n",
            "Step 900 avg train loss = 4.0732\n",
            "Step 1000 avg train loss = 4.0911\n",
            "Step 1100 avg train loss = 4.1002\n",
            "Step 1200 avg train loss = 4.0899\n",
            "Step 1300 avg train loss = 4.0931\n",
            "Step 1400 avg train loss = 4.1244\n",
            "Step 1500 avg train loss = 4.1223\n",
            "Step 1600 avg train loss = 4.1194\n",
            "Step 1700 avg train loss = 4.1170\n",
            "Step 1800 avg train loss = 4.1561\n",
            "Step 1900 avg train loss = 4.1252\n",
            "Step 2000 avg train loss = 4.1402\n",
            "Step 2100 avg train loss = 4.1437\n",
            "Step 2200 avg train loss = 4.1330\n",
            "Step 2300 avg train loss = 4.1470\n",
            "Step 2400 avg train loss = 4.1463\n",
            "Validation loss after 16 epoch = 5.5334\n",
            "Step 0 avg train loss = 3.7316\n",
            "Step 100 avg train loss = 4.0013\n",
            "Step 200 avg train loss = 3.9938\n",
            "Step 300 avg train loss = 4.0058\n",
            "Step 400 avg train loss = 4.0110\n",
            "Step 500 avg train loss = 4.0128\n",
            "Step 600 avg train loss = 4.0476\n",
            "Step 700 avg train loss = 4.0376\n",
            "Step 800 avg train loss = 4.0620\n",
            "Step 900 avg train loss = 4.0469\n",
            "Step 1000 avg train loss = 4.0585\n",
            "Step 1100 avg train loss = 4.0540\n",
            "Step 1200 avg train loss = 4.0533\n",
            "Step 1300 avg train loss = 4.0616\n",
            "Step 1400 avg train loss = 4.0754\n",
            "Step 1500 avg train loss = 4.0883\n",
            "Step 1600 avg train loss = 4.0759\n",
            "Step 1700 avg train loss = 4.0898\n",
            "Step 1800 avg train loss = 4.0750\n",
            "Step 1900 avg train loss = 4.0744\n",
            "Step 2000 avg train loss = 4.1101\n",
            "Step 2100 avg train loss = 4.0871\n",
            "Step 2200 avg train loss = 4.0985\n",
            "Step 2300 avg train loss = 4.0938\n",
            "Step 2400 avg train loss = 4.1244\n",
            "Validation loss after 17 epoch = 5.5626\n",
            "Step 0 avg train loss = 4.0586\n",
            "Step 100 avg train loss = 3.9622\n",
            "Step 200 avg train loss = 3.9616\n",
            "Step 300 avg train loss = 3.9661\n",
            "Step 400 avg train loss = 3.9781\n",
            "Step 500 avg train loss = 3.9821\n",
            "Step 600 avg train loss = 3.9901\n",
            "Step 700 avg train loss = 3.9958\n",
            "Step 800 avg train loss = 3.9904\n",
            "Step 900 avg train loss = 4.0086\n",
            "Step 1000 avg train loss = 4.0141\n",
            "Step 1100 avg train loss = 4.0295\n",
            "Step 1200 avg train loss = 4.0179\n",
            "Step 1300 avg train loss = 4.0372\n",
            "Step 1400 avg train loss = 4.0217\n",
            "Step 1500 avg train loss = 4.0407\n",
            "Step 1600 avg train loss = 4.0298\n",
            "Step 1700 avg train loss = 4.0544\n",
            "Step 1800 avg train loss = 4.0632\n",
            "Step 1900 avg train loss = 4.0674\n",
            "Step 2000 avg train loss = 4.0524\n",
            "Step 2100 avg train loss = 4.0640\n",
            "Step 2200 avg train loss = 4.0670\n",
            "Step 2300 avg train loss = 4.0740\n",
            "Step 2400 avg train loss = 4.0513\n",
            "Validation loss after 18 epoch = 5.6016\n",
            "Step 0 avg train loss = 3.9841\n",
            "Step 100 avg train loss = 3.9164\n",
            "Step 200 avg train loss = 3.9314\n",
            "Step 300 avg train loss = 3.9329\n",
            "Step 400 avg train loss = 3.9385\n",
            "Step 500 avg train loss = 3.9649\n",
            "Step 600 avg train loss = 3.9697\n",
            "Step 700 avg train loss = 3.9678\n",
            "Step 800 avg train loss = 3.9723\n",
            "Step 900 avg train loss = 3.9918\n",
            "Step 1000 avg train loss = 3.9726\n",
            "Step 1100 avg train loss = 3.9727\n",
            "Step 1200 avg train loss = 3.9829\n",
            "Step 1300 avg train loss = 3.9852\n",
            "Step 1400 avg train loss = 3.9998\n",
            "Step 1500 avg train loss = 4.0064\n",
            "Step 1600 avg train loss = 4.0153\n",
            "Step 1700 avg train loss = 4.0046\n",
            "Step 1800 avg train loss = 4.0104\n",
            "Step 1900 avg train loss = 3.9953\n",
            "Step 2000 avg train loss = 4.0229\n",
            "Step 2100 avg train loss = 4.0129\n",
            "Step 2200 avg train loss = 4.0377\n",
            "Step 2300 avg train loss = 4.0442\n",
            "Step 2400 avg train loss = 4.0593\n",
            "Validation loss after 19 epoch = 5.6320\n",
            "Saving best model with best embedding dimension...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVH3KCO9_UW",
        "colab_type": "code",
        "outputId": "7cfe354d-4565-49d0-9837-5976f53a364d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# second hyperparameter tuning: hidden size: 100- 500 \n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "# single grid search for the hyperparameter tuning\n",
        "param_grid_hid_size = {\"hidden_size\": [300,400,500]}    # tried 100, 200 before.\n",
        "\n",
        "# list of parameters=========== \n",
        "epoch_num = 20\n",
        "embedding_size = 300 # find the best embedding size 5.2010 after 5 epoch\n",
        "learning_rate=0.001\n",
        "overall_min_val_loss = 20\n",
        "# Loop over embedding size:\n",
        "for hidden_size in param_grid_hid_size[\"hidden_size\"]:\n",
        "  \n",
        "  embedding_size = embedding_size\n",
        "  hidden_size = hidden_size # output of dimension \n",
        "  num_layers = 2\n",
        "  lstm_dropout = 0.1\n",
        "# input_size = lookup.weight.size(1)\n",
        "  vocab_size = len(train_dict)\n",
        "  \n",
        "  options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "  \n",
        "  model = LSTMModel(options).to(current_device)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=train_dict.get_id('<pad>'))\n",
        "  model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = optim.Adam(model_parameters, lr=learning_rate)\n",
        "  \n",
        "  print(model)\n",
        "  \n",
        "  plot_cache = []\n",
        "  min_val_loss = 20   \n",
        "\n",
        "  for epoch_number in range(epoch_num):\n",
        "    \n",
        "    # do train \n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        best = avg_val_loss < min_val_loss\n",
        "        if best:\n",
        "            min_val_loss = avg_val_loss\n",
        "            best_model = model\n",
        "            print(\"update best model with this parameter to :\")\n",
        "            print(best_model)\n",
        "    \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "    \n",
        "    \n",
        "    if load_pretrained:\n",
        "        break\n",
        "  \n",
        "  if (min_val_loss < overall_min_val_loss):\n",
        "    best_model_overall = best_model\n",
        "    overall_min_val_loss = min_val_loss\n",
        "    print(\"update overall best model to :\")\n",
        "    print(best_model_overall)\n",
        "    \n",
        "# save the best model in this single grid search:         \n",
        "print('Saving best model with best hidden size...')\n",
        "torch.save({\n",
        "'options': options,\n",
        "'loss_cache': plot_cache,\n",
        "'model_dict': best_model_overall.state_dict()\n",
        "        }, './hid_tune_best_LSTM.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4099\n",
            "Step 100 avg train loss = 7.6503\n",
            "Step 200 avg train loss = 7.1205\n",
            "Step 300 avg train loss = 6.9481\n",
            "Step 400 avg train loss = 6.7299\n",
            "Step 500 avg train loss = 6.5959\n",
            "Step 600 avg train loss = 6.4814\n",
            "Step 700 avg train loss = 6.3755\n",
            "Step 800 avg train loss = 6.3012\n",
            "Step 900 avg train loss = 6.2247\n",
            "Step 1000 avg train loss = 6.1656\n",
            "Step 1100 avg train loss = 6.1052\n",
            "Step 1200 avg train loss = 6.0663\n",
            "Step 1300 avg train loss = 6.0285\n",
            "Step 1400 avg train loss = 5.9922\n",
            "Step 1500 avg train loss = 5.9431\n",
            "Step 1600 avg train loss = 5.9030\n",
            "Step 1700 avg train loss = 5.8825\n",
            "Step 1800 avg train loss = 5.8530\n",
            "Step 1900 avg train loss = 5.8168\n",
            "Step 2000 avg train loss = 5.7784\n",
            "Step 2100 avg train loss = 5.7565\n",
            "Step 2200 avg train loss = 5.7259\n",
            "Step 2300 avg train loss = 5.7185\n",
            "Step 2400 avg train loss = 5.7055\n",
            "Validation loss after 0 epoch = 5.5290\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.4427\n",
            "Step 100 avg train loss = 5.5335\n",
            "Step 200 avg train loss = 5.5159\n",
            "Step 300 avg train loss = 5.5119\n",
            "Step 400 avg train loss = 5.5175\n",
            "Step 500 avg train loss = 5.5026\n",
            "Step 600 avg train loss = 5.4787\n",
            "Step 700 avg train loss = 5.4554\n",
            "Step 800 avg train loss = 5.4593\n",
            "Step 900 avg train loss = 5.4301\n",
            "Step 1000 avg train loss = 5.4231\n",
            "Step 1100 avg train loss = 5.4054\n",
            "Step 1200 avg train loss = 5.3733\n",
            "Step 1300 avg train loss = 5.3645\n",
            "Step 1400 avg train loss = 5.3550\n",
            "Step 1500 avg train loss = 5.3633\n",
            "Step 1600 avg train loss = 5.3374\n",
            "Step 1700 avg train loss = 5.3263\n",
            "Step 1800 avg train loss = 5.3229\n",
            "Step 1900 avg train loss = 5.3222\n",
            "Step 2000 avg train loss = 5.2895\n",
            "Step 2100 avg train loss = 5.2699\n",
            "Step 2200 avg train loss = 5.2679\n",
            "Step 2300 avg train loss = 5.2731\n",
            "Step 2400 avg train loss = 5.2633\n",
            "Validation loss after 1 epoch = 5.2606\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.1397\n",
            "Step 100 avg train loss = 5.0791\n",
            "Step 200 avg train loss = 5.0573\n",
            "Step 300 avg train loss = 5.0570\n",
            "Step 400 avg train loss = 5.0327\n",
            "Step 500 avg train loss = 5.0358\n",
            "Step 600 avg train loss = 5.0562\n",
            "Step 700 avg train loss = 5.0531\n",
            "Step 800 avg train loss = 5.0421\n",
            "Step 900 avg train loss = 5.0305\n",
            "Step 1000 avg train loss = 5.0268\n",
            "Step 1100 avg train loss = 5.0174\n",
            "Step 1200 avg train loss = 5.0098\n",
            "Step 1300 avg train loss = 4.9993\n",
            "Step 1400 avg train loss = 5.0077\n",
            "Step 1500 avg train loss = 5.0213\n",
            "Step 1600 avg train loss = 4.9785\n",
            "Step 1700 avg train loss = 5.0014\n",
            "Step 1800 avg train loss = 4.9807\n",
            "Step 1900 avg train loss = 5.0027\n",
            "Step 2000 avg train loss = 4.9883\n",
            "Step 2100 avg train loss = 4.9746\n",
            "Step 2200 avg train loss = 4.9813\n",
            "Step 2300 avg train loss = 4.9706\n",
            "Step 2400 avg train loss = 4.9184\n",
            "Validation loss after 2 epoch = 5.1541\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.7694\n",
            "Step 100 avg train loss = 4.7595\n",
            "Step 200 avg train loss = 4.7478\n",
            "Step 300 avg train loss = 4.7492\n",
            "Step 400 avg train loss = 4.7560\n",
            "Step 500 avg train loss = 4.7574\n",
            "Step 600 avg train loss = 4.7632\n",
            "Step 700 avg train loss = 4.7486\n",
            "Step 800 avg train loss = 4.7542\n",
            "Step 900 avg train loss = 4.7281\n",
            "Step 1000 avg train loss = 4.7470\n",
            "Step 1100 avg train loss = 4.7408\n",
            "Step 1200 avg train loss = 4.7567\n",
            "Step 1300 avg train loss = 4.7272\n",
            "Step 1400 avg train loss = 4.7395\n",
            "Step 1500 avg train loss = 4.7376\n",
            "Step 1600 avg train loss = 4.7607\n",
            "Step 1700 avg train loss = 4.7169\n",
            "Step 1800 avg train loss = 4.7630\n",
            "Step 1900 avg train loss = 4.7081\n",
            "Step 2000 avg train loss = 4.7493\n",
            "Step 2100 avg train loss = 4.7253\n",
            "Step 2200 avg train loss = 4.7502\n",
            "Step 2300 avg train loss = 4.7248\n",
            "Step 2400 avg train loss = 4.7259\n",
            "Validation loss after 3 epoch = 5.1226\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.4741\n",
            "Step 100 avg train loss = 4.4803\n",
            "Step 200 avg train loss = 4.4952\n",
            "Step 300 avg train loss = 4.5141\n",
            "Step 400 avg train loss = 4.5000\n",
            "Step 500 avg train loss = 4.5007\n",
            "Step 600 avg train loss = 4.5440\n",
            "Step 700 avg train loss = 4.5412\n",
            "Step 800 avg train loss = 4.4976\n",
            "Step 900 avg train loss = 4.5229\n",
            "Step 1000 avg train loss = 4.5425\n",
            "Step 1100 avg train loss = 4.5192\n",
            "Step 1200 avg train loss = 4.5280\n",
            "Step 1300 avg train loss = 4.5341\n",
            "Step 1400 avg train loss = 4.5197\n",
            "Step 1500 avg train loss = 4.5343\n",
            "Step 1600 avg train loss = 4.5381\n",
            "Step 1700 avg train loss = 4.5367\n",
            "Step 1800 avg train loss = 4.5439\n",
            "Step 1900 avg train loss = 4.5416\n",
            "Step 2000 avg train loss = 4.5388\n",
            "Step 2100 avg train loss = 4.5345\n",
            "Step 2200 avg train loss = 4.5394\n",
            "Step 2300 avg train loss = 4.5295\n",
            "Step 2400 avg train loss = 4.5709\n",
            "Validation loss after 4 epoch = 5.1462\n",
            "Step 0 avg train loss = 4.3837\n",
            "Step 100 avg train loss = 4.3057\n",
            "Step 200 avg train loss = 4.3158\n",
            "Step 300 avg train loss = 4.2948\n",
            "Step 400 avg train loss = 4.3173\n",
            "Step 500 avg train loss = 4.3366\n",
            "Step 600 avg train loss = 4.3305\n",
            "Step 700 avg train loss = 4.3099\n",
            "Step 800 avg train loss = 4.3415\n",
            "Step 900 avg train loss = 4.3177\n",
            "Step 1000 avg train loss = 4.3623\n",
            "Step 1100 avg train loss = 4.3501\n",
            "Step 1200 avg train loss = 4.3291\n",
            "Step 1300 avg train loss = 4.3666\n",
            "Step 1400 avg train loss = 4.3728\n",
            "Step 1500 avg train loss = 4.3527\n",
            "Step 1600 avg train loss = 4.3682\n",
            "Step 1700 avg train loss = 4.3785\n",
            "Step 1800 avg train loss = 4.3579\n",
            "Step 1900 avg train loss = 4.3822\n",
            "Step 2000 avg train loss = 4.3779\n",
            "Step 2100 avg train loss = 4.3613\n",
            "Step 2200 avg train loss = 4.3698\n",
            "Step 2300 avg train loss = 4.3592\n",
            "Step 2400 avg train loss = 4.3895\n",
            "Validation loss after 5 epoch = 5.2028\n",
            "Step 0 avg train loss = 4.0526\n",
            "Step 100 avg train loss = 4.1325\n",
            "Step 200 avg train loss = 4.1482\n",
            "Step 300 avg train loss = 4.1528\n",
            "Step 400 avg train loss = 4.1402\n",
            "Step 500 avg train loss = 4.1841\n",
            "Step 600 avg train loss = 4.1917\n",
            "Step 700 avg train loss = 4.1770\n",
            "Step 800 avg train loss = 4.1738\n",
            "Step 900 avg train loss = 4.1912\n",
            "Step 1000 avg train loss = 4.1658\n",
            "Step 1100 avg train loss = 4.1895\n",
            "Step 1200 avg train loss = 4.1859\n",
            "Step 1300 avg train loss = 4.2089\n",
            "Step 1400 avg train loss = 4.2021\n",
            "Step 1500 avg train loss = 4.2086\n",
            "Step 1600 avg train loss = 4.2127\n",
            "Step 1700 avg train loss = 4.2212\n",
            "Step 1800 avg train loss = 4.2368\n",
            "Step 1900 avg train loss = 4.2285\n",
            "Step 2000 avg train loss = 4.2108\n",
            "Step 2100 avg train loss = 4.2291\n",
            "Step 2200 avg train loss = 4.2241\n",
            "Step 2300 avg train loss = 4.2448\n",
            "Step 2400 avg train loss = 4.2500\n",
            "Validation loss after 6 epoch = 5.2643\n",
            "Step 0 avg train loss = 4.1608\n",
            "Step 100 avg train loss = 3.9926\n",
            "Step 200 avg train loss = 3.9853\n",
            "Step 300 avg train loss = 4.0101\n",
            "Step 400 avg train loss = 4.0226\n",
            "Step 500 avg train loss = 4.0400\n",
            "Step 600 avg train loss = 4.0514\n",
            "Step 700 avg train loss = 4.0365\n",
            "Step 800 avg train loss = 4.0351\n",
            "Step 900 avg train loss = 4.0683\n",
            "Step 1000 avg train loss = 4.0576\n",
            "Step 1100 avg train loss = 4.0519\n",
            "Step 1200 avg train loss = 4.0592\n",
            "Step 1300 avg train loss = 4.0794\n",
            "Step 1400 avg train loss = 4.0862\n",
            "Step 1500 avg train loss = 4.0734\n",
            "Step 1600 avg train loss = 4.0884\n",
            "Step 1700 avg train loss = 4.0832\n",
            "Step 1800 avg train loss = 4.0857\n",
            "Step 1900 avg train loss = 4.1060\n",
            "Step 2000 avg train loss = 4.1110\n",
            "Step 2100 avg train loss = 4.1013\n",
            "Step 2200 avg train loss = 4.0906\n",
            "Step 2300 avg train loss = 4.1083\n",
            "Step 2400 avg train loss = 4.1312\n",
            "Validation loss after 7 epoch = 5.3327\n",
            "Step 0 avg train loss = 3.7683\n",
            "Step 100 avg train loss = 3.8570\n",
            "Step 200 avg train loss = 3.8763\n",
            "Step 300 avg train loss = 3.8780\n",
            "Step 400 avg train loss = 3.8904\n",
            "Step 500 avg train loss = 3.9068\n",
            "Step 600 avg train loss = 3.9278\n",
            "Step 700 avg train loss = 3.9140\n",
            "Step 800 avg train loss = 3.9463\n",
            "Step 900 avg train loss = 3.9398\n",
            "Step 1000 avg train loss = 3.9402\n",
            "Step 1100 avg train loss = 3.9359\n",
            "Step 1200 avg train loss = 3.9579\n",
            "Step 1300 avg train loss = 3.9682\n",
            "Step 1400 avg train loss = 3.9625\n",
            "Step 1500 avg train loss = 3.9634\n",
            "Step 1600 avg train loss = 3.9992\n",
            "Step 1700 avg train loss = 3.9688\n",
            "Step 1800 avg train loss = 3.9685\n",
            "Step 1900 avg train loss = 4.0123\n",
            "Step 2000 avg train loss = 4.0018\n",
            "Step 2100 avg train loss = 3.9943\n",
            "Step 2200 avg train loss = 3.9933\n",
            "Step 2300 avg train loss = 3.9897\n",
            "Step 2400 avg train loss = 3.9900\n",
            "Validation loss after 8 epoch = 5.3975\n",
            "Step 0 avg train loss = 3.7222\n",
            "Step 100 avg train loss = 3.7571\n",
            "Step 200 avg train loss = 3.7694\n",
            "Step 300 avg train loss = 3.7698\n",
            "Step 400 avg train loss = 3.7991\n",
            "Step 500 avg train loss = 3.8114\n",
            "Step 600 avg train loss = 3.7970\n",
            "Step 700 avg train loss = 3.8228\n",
            "Step 800 avg train loss = 3.8290\n",
            "Step 900 avg train loss = 3.8350\n",
            "Step 1000 avg train loss = 3.8275\n",
            "Step 1100 avg train loss = 3.8465\n",
            "Step 1200 avg train loss = 3.8479\n",
            "Step 1300 avg train loss = 3.8568\n",
            "Step 1400 avg train loss = 3.8392\n",
            "Step 1500 avg train loss = 3.8794\n",
            "Step 1600 avg train loss = 3.8874\n",
            "Step 1700 avg train loss = 3.8756\n",
            "Step 1800 avg train loss = 3.8972\n",
            "Step 1900 avg train loss = 3.8759\n",
            "Step 2000 avg train loss = 3.8924\n",
            "Step 2100 avg train loss = 3.8987\n",
            "Step 2200 avg train loss = 3.8825\n",
            "Step 2300 avg train loss = 3.9162\n",
            "Step 2400 avg train loss = 3.8946\n",
            "Validation loss after 9 epoch = 5.4741\n",
            "Step 0 avg train loss = 3.7291\n",
            "Step 100 avg train loss = 3.6551\n",
            "Step 200 avg train loss = 3.6756\n",
            "Step 300 avg train loss = 3.6913\n",
            "Step 400 avg train loss = 3.6820\n",
            "Step 500 avg train loss = 3.7193\n",
            "Step 600 avg train loss = 3.6884\n",
            "Step 700 avg train loss = 3.7278\n",
            "Step 800 avg train loss = 3.7154\n",
            "Step 900 avg train loss = 3.7263\n",
            "Step 1000 avg train loss = 3.7529\n",
            "Step 1100 avg train loss = 3.7575\n",
            "Step 1200 avg train loss = 3.7585\n",
            "Step 1300 avg train loss = 3.7535\n",
            "Step 1400 avg train loss = 3.7745\n",
            "Step 1500 avg train loss = 3.7748\n",
            "Step 1600 avg train loss = 3.7892\n",
            "Step 1700 avg train loss = 3.7992\n",
            "Step 1800 avg train loss = 3.7850\n",
            "Step 1900 avg train loss = 3.7759\n",
            "Step 2000 avg train loss = 3.7892\n",
            "Step 2100 avg train loss = 3.7872\n",
            "Step 2200 avg train loss = 3.8131\n",
            "Step 2300 avg train loss = 3.8323\n",
            "Step 2400 avg train loss = 3.8356\n",
            "Validation loss after 10 epoch = 5.5322\n",
            "Step 0 avg train loss = 3.8113\n",
            "Step 100 avg train loss = 3.5682\n",
            "Step 200 avg train loss = 3.5750\n",
            "Step 300 avg train loss = 3.5833\n",
            "Step 400 avg train loss = 3.6055\n",
            "Step 500 avg train loss = 3.6171\n",
            "Step 600 avg train loss = 3.6419\n",
            "Step 700 avg train loss = 3.6146\n",
            "Step 800 avg train loss = 3.6351\n",
            "Step 900 avg train loss = 3.6484\n",
            "Step 1000 avg train loss = 3.6540\n",
            "Step 1100 avg train loss = 3.6774\n",
            "Step 1200 avg train loss = 3.6741\n",
            "Step 1300 avg train loss = 3.6976\n",
            "Step 1400 avg train loss = 3.6839\n",
            "Step 1500 avg train loss = 3.6895\n",
            "Step 1600 avg train loss = 3.7053\n",
            "Step 1700 avg train loss = 3.6926\n",
            "Step 1800 avg train loss = 3.7059\n",
            "Step 1900 avg train loss = 3.7257\n",
            "Step 2000 avg train loss = 3.7028\n",
            "Step 2100 avg train loss = 3.7292\n",
            "Step 2200 avg train loss = 3.7338\n",
            "Step 2300 avg train loss = 3.7201\n",
            "Step 2400 avg train loss = 3.7461\n",
            "Validation loss after 11 epoch = 5.6184\n",
            "Step 0 avg train loss = 3.3071\n",
            "Step 100 avg train loss = 3.4697\n",
            "Step 200 avg train loss = 3.4999\n",
            "Step 300 avg train loss = 3.5252\n",
            "Step 400 avg train loss = 3.5207\n",
            "Step 500 avg train loss = 3.5351\n",
            "Step 600 avg train loss = 3.5686\n",
            "Step 700 avg train loss = 3.5591\n",
            "Step 800 avg train loss = 3.5615\n",
            "Step 900 avg train loss = 3.5776\n",
            "Step 1000 avg train loss = 3.5719\n",
            "Step 1100 avg train loss = 3.5880\n",
            "Step 1200 avg train loss = 3.6035\n",
            "Step 1300 avg train loss = 3.5975\n",
            "Step 1400 avg train loss = 3.6062\n",
            "Step 1500 avg train loss = 3.6110\n",
            "Step 1600 avg train loss = 3.6164\n",
            "Step 1700 avg train loss = 3.6234\n",
            "Step 1800 avg train loss = 3.6323\n",
            "Step 1900 avg train loss = 3.6218\n",
            "Step 2000 avg train loss = 3.6716\n",
            "Step 2100 avg train loss = 3.6399\n",
            "Step 2200 avg train loss = 3.6409\n",
            "Step 2300 avg train loss = 3.6468\n",
            "Step 2400 avg train loss = 3.6594\n",
            "Validation loss after 12 epoch = 5.6740\n",
            "Step 0 avg train loss = 3.4354\n",
            "Step 100 avg train loss = 3.4094\n",
            "Step 200 avg train loss = 3.4397\n",
            "Step 300 avg train loss = 3.4415\n",
            "Step 400 avg train loss = 3.4573\n",
            "Step 500 avg train loss = 3.4640\n",
            "Step 600 avg train loss = 3.4705\n",
            "Step 700 avg train loss = 3.4894\n",
            "Step 800 avg train loss = 3.4910\n",
            "Step 900 avg train loss = 3.5019\n",
            "Step 1000 avg train loss = 3.5078\n",
            "Step 1100 avg train loss = 3.5457\n",
            "Step 1200 avg train loss = 3.5079\n",
            "Step 1300 avg train loss = 3.5296\n",
            "Step 1400 avg train loss = 3.5421\n",
            "Step 1500 avg train loss = 3.5358\n",
            "Step 1600 avg train loss = 3.5525\n",
            "Step 1700 avg train loss = 3.5517\n",
            "Step 1800 avg train loss = 3.5501\n",
            "Step 1900 avg train loss = 3.5790\n",
            "Step 2000 avg train loss = 3.5600\n",
            "Step 2100 avg train loss = 3.5644\n",
            "Step 2200 avg train loss = 3.5797\n",
            "Step 2300 avg train loss = 3.5731\n",
            "Step 2400 avg train loss = 3.5812\n",
            "Validation loss after 13 epoch = 5.7475\n",
            "Step 0 avg train loss = 3.2863\n",
            "Step 100 avg train loss = 3.3622\n",
            "Step 200 avg train loss = 3.3642\n",
            "Step 300 avg train loss = 3.3426\n",
            "Step 400 avg train loss = 3.3910\n",
            "Step 500 avg train loss = 3.4220\n",
            "Step 600 avg train loss = 3.4062\n",
            "Step 700 avg train loss = 3.4011\n",
            "Step 800 avg train loss = 3.4230\n",
            "Step 900 avg train loss = 3.4452\n",
            "Step 1000 avg train loss = 3.4464\n",
            "Step 1100 avg train loss = 3.4487\n",
            "Step 1200 avg train loss = 3.4500\n",
            "Step 1300 avg train loss = 3.4658\n",
            "Step 1400 avg train loss = 3.4717\n",
            "Step 1500 avg train loss = 3.4795\n",
            "Step 1600 avg train loss = 3.4666\n",
            "Step 1700 avg train loss = 3.4916\n",
            "Step 1800 avg train loss = 3.4915\n",
            "Step 1900 avg train loss = 3.5139\n",
            "Step 2000 avg train loss = 3.5037\n",
            "Step 2100 avg train loss = 3.4985\n",
            "Step 2200 avg train loss = 3.5241\n",
            "Step 2300 avg train loss = 3.5092\n",
            "Step 2400 avg train loss = 3.5168\n",
            "Validation loss after 14 epoch = 5.8012\n",
            "Step 0 avg train loss = 3.1319\n",
            "Step 100 avg train loss = 3.2800\n",
            "Step 200 avg train loss = 3.3174\n",
            "Step 300 avg train loss = 3.3142\n",
            "Step 400 avg train loss = 3.2987\n",
            "Step 500 avg train loss = 3.3539\n",
            "Step 600 avg train loss = 3.3492\n",
            "Step 700 avg train loss = 3.3694\n",
            "Step 800 avg train loss = 3.3564\n",
            "Step 900 avg train loss = 3.3689\n",
            "Step 1000 avg train loss = 3.3779\n",
            "Step 1100 avg train loss = 3.3759\n",
            "Step 1200 avg train loss = 3.3902\n",
            "Step 1300 avg train loss = 3.3965\n",
            "Step 1400 avg train loss = 3.4062\n",
            "Step 1500 avg train loss = 3.3951\n",
            "Step 1600 avg train loss = 3.4263\n",
            "Step 1700 avg train loss = 3.4413\n",
            "Step 1800 avg train loss = 3.4143\n",
            "Step 1900 avg train loss = 3.4398\n",
            "Step 2000 avg train loss = 3.4500\n",
            "Step 2100 avg train loss = 3.4363\n",
            "Step 2200 avg train loss = 3.4307\n",
            "Step 2300 avg train loss = 3.4724\n",
            "Step 2400 avg train loss = 3.4862\n",
            "Validation loss after 15 epoch = 5.8743\n",
            "Step 0 avg train loss = 3.2559\n",
            "Step 100 avg train loss = 3.2319\n",
            "Step 200 avg train loss = 3.2329\n",
            "Step 300 avg train loss = 3.2552\n",
            "Step 400 avg train loss = 3.2595\n",
            "Step 500 avg train loss = 3.2810\n",
            "Step 600 avg train loss = 3.2685\n",
            "Step 700 avg train loss = 3.3044\n",
            "Step 800 avg train loss = 3.3060\n",
            "Step 900 avg train loss = 3.3145\n",
            "Step 1000 avg train loss = 3.3244\n",
            "Step 1100 avg train loss = 3.3450\n",
            "Step 1200 avg train loss = 3.3607\n",
            "Step 1300 avg train loss = 3.3391\n",
            "Step 1400 avg train loss = 3.3500\n",
            "Step 1500 avg train loss = 3.3405\n",
            "Step 1600 avg train loss = 3.3485\n",
            "Step 1700 avg train loss = 3.3663\n",
            "Step 1800 avg train loss = 3.3645\n",
            "Step 1900 avg train loss = 3.3857\n",
            "Step 2000 avg train loss = 3.4084\n",
            "Step 2100 avg train loss = 3.3891\n",
            "Step 2200 avg train loss = 3.3910\n",
            "Step 2300 avg train loss = 3.3972\n",
            "Step 2400 avg train loss = 3.4180\n",
            "Validation loss after 16 epoch = 5.9382\n",
            "Step 0 avg train loss = 3.1351\n",
            "Step 100 avg train loss = 3.1690\n",
            "Step 200 avg train loss = 3.1953\n",
            "Step 300 avg train loss = 3.2114\n",
            "Step 400 avg train loss = 3.2002\n",
            "Step 500 avg train loss = 3.2336\n",
            "Step 600 avg train loss = 3.2433\n",
            "Step 700 avg train loss = 3.2336\n",
            "Step 800 avg train loss = 3.2417\n",
            "Step 900 avg train loss = 3.2495\n",
            "Step 1000 avg train loss = 3.2415\n",
            "Step 1100 avg train loss = 3.2660\n",
            "Step 1200 avg train loss = 3.2834\n",
            "Step 1300 avg train loss = 3.2966\n",
            "Step 1400 avg train loss = 3.3176\n",
            "Step 1500 avg train loss = 3.3167\n",
            "Step 1600 avg train loss = 3.3291\n",
            "Step 1700 avg train loss = 3.3274\n",
            "Step 1800 avg train loss = 3.3178\n",
            "Step 1900 avg train loss = 3.3335\n",
            "Step 2000 avg train loss = 3.3340\n",
            "Step 2100 avg train loss = 3.3309\n",
            "Step 2200 avg train loss = 3.3299\n",
            "Step 2300 avg train loss = 3.3410\n",
            "Step 2400 avg train loss = 3.3604\n",
            "Validation loss after 17 epoch = 6.0043\n",
            "Step 0 avg train loss = 3.2853\n",
            "Step 100 avg train loss = 3.1190\n",
            "Step 200 avg train loss = 3.1357\n",
            "Step 300 avg train loss = 3.1676\n",
            "Step 400 avg train loss = 3.1814\n",
            "Step 500 avg train loss = 3.1903\n",
            "Step 600 avg train loss = 3.1763\n",
            "Step 700 avg train loss = 3.1932\n",
            "Step 800 avg train loss = 3.1999\n",
            "Step 900 avg train loss = 3.1999\n",
            "Step 1000 avg train loss = 3.2263\n",
            "Step 1100 avg train loss = 3.2124\n",
            "Step 1200 avg train loss = 3.2228\n",
            "Step 1300 avg train loss = 3.2570\n",
            "Step 1400 avg train loss = 3.2456\n",
            "Step 1500 avg train loss = 3.2563\n",
            "Step 1600 avg train loss = 3.2409\n",
            "Step 1700 avg train loss = 3.2554\n",
            "Step 1800 avg train loss = 3.2732\n",
            "Step 1900 avg train loss = 3.2781\n",
            "Step 2000 avg train loss = 3.2878\n",
            "Step 2100 avg train loss = 3.2644\n",
            "Step 2200 avg train loss = 3.2989\n",
            "Step 2300 avg train loss = 3.2963\n",
            "Step 2400 avg train loss = 3.3218\n",
            "Validation loss after 18 epoch = 6.0666\n",
            "Step 0 avg train loss = 3.0786\n",
            "Step 100 avg train loss = 3.0703\n",
            "Step 200 avg train loss = 3.1080\n",
            "Step 300 avg train loss = 3.0987\n",
            "Step 400 avg train loss = 3.1110\n",
            "Step 500 avg train loss = 3.1162\n",
            "Step 600 avg train loss = 3.1442\n",
            "Step 700 avg train loss = 3.1542\n",
            "Step 800 avg train loss = 3.1599\n",
            "Step 900 avg train loss = 3.1698\n",
            "Step 1000 avg train loss = 3.1614\n",
            "Step 1100 avg train loss = 3.1674\n",
            "Step 1200 avg train loss = 3.1715\n",
            "Step 1300 avg train loss = 3.1744\n",
            "Step 1400 avg train loss = 3.1836\n",
            "Step 1500 avg train loss = 3.1990\n",
            "Step 1600 avg train loss = 3.1874\n",
            "Step 1700 avg train loss = 3.2311\n",
            "Step 1800 avg train loss = 3.2313\n",
            "Step 1900 avg train loss = 3.2201\n",
            "Step 2000 avg train loss = 3.2436\n",
            "Step 2100 avg train loss = 3.2652\n",
            "Step 2200 avg train loss = 3.2567\n",
            "Step 2300 avg train loss = 3.2652\n",
            "Step 2400 avg train loss = 3.2681\n",
            "Validation loss after 19 epoch = 6.1196\n",
            "update overall best model to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
            ")\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4098\n",
            "Step 100 avg train loss = 7.6492\n",
            "Step 200 avg train loss = 7.0818\n",
            "Step 300 avg train loss = 6.8396\n",
            "Step 400 avg train loss = 6.6375\n",
            "Step 500 avg train loss = 6.4900\n",
            "Step 600 avg train loss = 6.4086\n",
            "Step 700 avg train loss = 6.2855\n",
            "Step 800 avg train loss = 6.2186\n",
            "Step 900 avg train loss = 6.1665\n",
            "Step 1000 avg train loss = 6.1015\n",
            "Step 1100 avg train loss = 6.0557\n",
            "Step 1200 avg train loss = 6.0004\n",
            "Step 1300 avg train loss = 5.9636\n",
            "Step 1400 avg train loss = 5.9022\n",
            "Step 1500 avg train loss = 5.8735\n",
            "Step 1600 avg train loss = 5.8570\n",
            "Step 1700 avg train loss = 5.8130\n",
            "Step 1800 avg train loss = 5.7860\n",
            "Step 1900 avg train loss = 5.7499\n",
            "Step 2000 avg train loss = 5.7413\n",
            "Step 2100 avg train loss = 5.7078\n",
            "Step 2200 avg train loss = 5.6742\n",
            "Step 2300 avg train loss = 5.6488\n",
            "Step 2400 avg train loss = 5.6270\n",
            "Validation loss after 0 epoch = 5.4853\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.4978\n",
            "Step 100 avg train loss = 5.4801\n",
            "Step 200 avg train loss = 5.4537\n",
            "Step 300 avg train loss = 5.4182\n",
            "Step 400 avg train loss = 5.4334\n",
            "Step 500 avg train loss = 5.4009\n",
            "Step 600 avg train loss = 5.4067\n",
            "Step 700 avg train loss = 5.3802\n",
            "Step 800 avg train loss = 5.3669\n",
            "Step 900 avg train loss = 5.3725\n",
            "Step 1000 avg train loss = 5.3187\n",
            "Step 1100 avg train loss = 5.3181\n",
            "Step 1200 avg train loss = 5.3102\n",
            "Step 1300 avg train loss = 5.2929\n",
            "Step 1400 avg train loss = 5.2853\n",
            "Step 1500 avg train loss = 5.2878\n",
            "Step 1600 avg train loss = 5.2364\n",
            "Step 1700 avg train loss = 5.2652\n",
            "Step 1800 avg train loss = 5.2310\n",
            "Step 1900 avg train loss = 5.2376\n",
            "Step 2000 avg train loss = 5.2195\n",
            "Step 2100 avg train loss = 5.1981\n",
            "Step 2200 avg train loss = 5.1933\n",
            "Step 2300 avg train loss = 5.2065\n",
            "Step 2400 avg train loss = 5.1798\n",
            "Validation loss after 1 epoch = 5.2252\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.9563\n",
            "Step 100 avg train loss = 4.9419\n",
            "Step 200 avg train loss = 4.9624\n",
            "Step 300 avg train loss = 4.9590\n",
            "Step 400 avg train loss = 4.9417\n",
            "Step 500 avg train loss = 4.9319\n",
            "Step 600 avg train loss = 4.9172\n",
            "Step 700 avg train loss = 4.9420\n",
            "Step 800 avg train loss = 4.9540\n",
            "Step 900 avg train loss = 4.9483\n",
            "Step 1000 avg train loss = 4.9140\n",
            "Step 1100 avg train loss = 4.9100\n",
            "Step 1200 avg train loss = 4.9191\n",
            "Step 1300 avg train loss = 4.9099\n",
            "Step 1400 avg train loss = 4.9314\n",
            "Step 1500 avg train loss = 4.9077\n",
            "Step 1600 avg train loss = 4.9266\n",
            "Step 1700 avg train loss = 4.9105\n",
            "Step 1800 avg train loss = 4.9041\n",
            "Step 1900 avg train loss = 4.9077\n",
            "Step 2000 avg train loss = 4.8442\n",
            "Step 2100 avg train loss = 4.8740\n",
            "Step 2200 avg train loss = 4.8757\n",
            "Step 2300 avg train loss = 4.8494\n",
            "Step 2400 avg train loss = 4.8564\n",
            "Validation loss after 2 epoch = 5.1418\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.6708\n",
            "Step 100 avg train loss = 4.6182\n",
            "Step 200 avg train loss = 4.6018\n",
            "Step 300 avg train loss = 4.6213\n",
            "Step 400 avg train loss = 4.5899\n",
            "Step 500 avg train loss = 4.6245\n",
            "Step 600 avg train loss = 4.6162\n",
            "Step 700 avg train loss = 4.6388\n",
            "Step 800 avg train loss = 4.6055\n",
            "Step 900 avg train loss = 4.6350\n",
            "Step 1000 avg train loss = 4.5953\n",
            "Step 1100 avg train loss = 4.6177\n",
            "Step 1200 avg train loss = 4.6018\n",
            "Step 1300 avg train loss = 4.6222\n",
            "Step 1400 avg train loss = 4.6504\n",
            "Step 1500 avg train loss = 4.6123\n",
            "Step 1600 avg train loss = 4.6034\n",
            "Step 1700 avg train loss = 4.6200\n",
            "Step 1800 avg train loss = 4.6032\n",
            "Step 1900 avg train loss = 4.6230\n",
            "Step 2000 avg train loss = 4.6191\n",
            "Step 2100 avg train loss = 4.6030\n",
            "Step 2200 avg train loss = 4.6272\n",
            "Step 2300 avg train loss = 4.6126\n",
            "Step 2400 avg train loss = 4.6244\n",
            "Validation loss after 3 epoch = 5.1319\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=400, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.1984\n",
            "Step 100 avg train loss = 4.3257\n",
            "Step 200 avg train loss = 4.3248\n",
            "Step 300 avg train loss = 4.3606\n",
            "Step 400 avg train loss = 4.3569\n",
            "Step 500 avg train loss = 4.3419\n",
            "Step 600 avg train loss = 4.3553\n",
            "Step 700 avg train loss = 4.3596\n",
            "Step 800 avg train loss = 4.3687\n",
            "Step 900 avg train loss = 4.3636\n",
            "Step 1000 avg train loss = 4.3722\n",
            "Step 1100 avg train loss = 4.3623\n",
            "Step 1200 avg train loss = 4.3746\n",
            "Step 1300 avg train loss = 4.3710\n",
            "Step 1400 avg train loss = 4.4135\n",
            "Step 1500 avg train loss = 4.3609\n",
            "Step 1600 avg train loss = 4.3793\n",
            "Step 1700 avg train loss = 4.3827\n",
            "Step 1800 avg train loss = 4.3834\n",
            "Step 1900 avg train loss = 4.3835\n",
            "Step 2000 avg train loss = 4.3726\n",
            "Step 2100 avg train loss = 4.3558\n",
            "Step 2200 avg train loss = 4.3982\n",
            "Step 2300 avg train loss = 4.3978\n",
            "Step 2400 avg train loss = 4.3891\n",
            "Validation loss after 4 epoch = 5.1783\n",
            "Step 0 avg train loss = 4.3267\n",
            "Step 100 avg train loss = 4.0815\n",
            "Step 200 avg train loss = 4.1013\n",
            "Step 300 avg train loss = 4.1208\n",
            "Step 400 avg train loss = 4.1366\n",
            "Step 500 avg train loss = 4.1387\n",
            "Step 600 avg train loss = 4.1273\n",
            "Step 700 avg train loss = 4.1667\n",
            "Step 800 avg train loss = 4.1553\n",
            "Step 900 avg train loss = 4.1509\n",
            "Step 1000 avg train loss = 4.1352\n",
            "Step 1100 avg train loss = 4.1615\n",
            "Step 1200 avg train loss = 4.1634\n",
            "Step 1300 avg train loss = 4.1712\n",
            "Step 1400 avg train loss = 4.1358\n",
            "Step 1500 avg train loss = 4.1743\n",
            "Step 1600 avg train loss = 4.1838\n",
            "Step 1700 avg train loss = 4.1937\n",
            "Step 1800 avg train loss = 4.1910\n",
            "Step 1900 avg train loss = 4.2073\n",
            "Step 2000 avg train loss = 4.1915\n",
            "Step 2100 avg train loss = 4.2064\n",
            "Step 2200 avg train loss = 4.2135\n",
            "Step 2300 avg train loss = 4.2095\n",
            "Step 2400 avg train loss = 4.1995\n",
            "Validation loss after 5 epoch = 5.2598\n",
            "Step 0 avg train loss = 4.0770\n",
            "Step 100 avg train loss = 3.8953\n",
            "Step 200 avg train loss = 3.9058\n",
            "Step 300 avg train loss = 3.9239\n",
            "Step 400 avg train loss = 3.9259\n",
            "Step 500 avg train loss = 3.9415\n",
            "Step 600 avg train loss = 3.9482\n",
            "Step 700 avg train loss = 3.9858\n",
            "Step 800 avg train loss = 3.9854\n",
            "Step 900 avg train loss = 3.9786\n",
            "Step 1000 avg train loss = 3.9942\n",
            "Step 1100 avg train loss = 3.9807\n",
            "Step 1200 avg train loss = 3.9844\n",
            "Step 1300 avg train loss = 3.9762\n",
            "Step 1400 avg train loss = 4.0142\n",
            "Step 1500 avg train loss = 4.0333\n",
            "Step 1600 avg train loss = 3.9999\n",
            "Step 1700 avg train loss = 4.0310\n",
            "Step 1800 avg train loss = 4.0176\n",
            "Step 1900 avg train loss = 4.0273\n",
            "Step 2000 avg train loss = 4.0134\n",
            "Step 2100 avg train loss = 4.0241\n",
            "Step 2200 avg train loss = 4.0345\n",
            "Step 2300 avg train loss = 4.0351\n",
            "Step 2400 avg train loss = 4.0457\n",
            "Validation loss after 6 epoch = 5.3545\n",
            "Step 0 avg train loss = 3.6888\n",
            "Step 100 avg train loss = 3.7567\n",
            "Step 200 avg train loss = 3.7618\n",
            "Step 300 avg train loss = 3.7711\n",
            "Step 400 avg train loss = 3.7734\n",
            "Step 500 avg train loss = 3.7867\n",
            "Step 600 avg train loss = 3.8039\n",
            "Step 700 avg train loss = 3.8021\n",
            "Step 800 avg train loss = 3.8081\n",
            "Step 900 avg train loss = 3.8029\n",
            "Step 1000 avg train loss = 3.8465\n",
            "Step 1100 avg train loss = 3.8210\n",
            "Step 1200 avg train loss = 3.8431\n",
            "Step 1300 avg train loss = 3.8501\n",
            "Step 1400 avg train loss = 3.8391\n",
            "Step 1500 avg train loss = 3.8666\n",
            "Step 1600 avg train loss = 3.8492\n",
            "Step 1700 avg train loss = 3.8766\n",
            "Step 1800 avg train loss = 3.8715\n",
            "Step 1900 avg train loss = 3.8747\n",
            "Step 2000 avg train loss = 3.9033\n",
            "Step 2100 avg train loss = 3.8794\n",
            "Step 2200 avg train loss = 3.8816\n",
            "Step 2300 avg train loss = 3.8944\n",
            "Step 2400 avg train loss = 3.8788\n",
            "Validation loss after 7 epoch = 5.4317\n",
            "Step 0 avg train loss = 3.5293\n",
            "Step 100 avg train loss = 3.5914\n",
            "Step 200 avg train loss = 3.6155\n",
            "Step 300 avg train loss = 3.6266\n",
            "Step 400 avg train loss = 3.6419\n",
            "Step 500 avg train loss = 3.6547\n",
            "Step 600 avg train loss = 3.6716\n",
            "Step 700 avg train loss = 3.6686\n",
            "Step 800 avg train loss = 3.6737\n",
            "Step 900 avg train loss = 3.6960\n",
            "Step 1000 avg train loss = 3.6889\n",
            "Step 1100 avg train loss = 3.6950\n",
            "Step 1200 avg train loss = 3.7026\n",
            "Step 1300 avg train loss = 3.7058\n",
            "Step 1400 avg train loss = 3.7133\n",
            "Step 1500 avg train loss = 3.7117\n",
            "Step 1600 avg train loss = 3.7329\n",
            "Step 1700 avg train loss = 3.7520\n",
            "Step 1800 avg train loss = 3.7565\n",
            "Step 1900 avg train loss = 3.7602\n",
            "Step 2000 avg train loss = 3.7511\n",
            "Step 2100 avg train loss = 3.7588\n",
            "Step 2200 avg train loss = 3.7471\n",
            "Step 2300 avg train loss = 3.7601\n",
            "Step 2400 avg train loss = 3.7692\n",
            "Validation loss after 8 epoch = 5.5335\n",
            "Step 0 avg train loss = 3.3417\n",
            "Step 100 avg train loss = 3.4848\n",
            "Step 200 avg train loss = 3.4885\n",
            "Step 300 avg train loss = 3.4967\n",
            "Step 400 avg train loss = 3.5262\n",
            "Step 500 avg train loss = 3.5265\n",
            "Step 600 avg train loss = 3.5474\n",
            "Step 700 avg train loss = 3.5607\n",
            "Step 800 avg train loss = 3.5524\n",
            "Step 900 avg train loss = 3.5694\n",
            "Step 1000 avg train loss = 3.5705\n",
            "Step 1100 avg train loss = 3.5858\n",
            "Step 1200 avg train loss = 3.5875\n",
            "Step 1300 avg train loss = 3.5775\n",
            "Step 1400 avg train loss = 3.6043\n",
            "Step 1500 avg train loss = 3.6175\n",
            "Step 1600 avg train loss = 3.6140\n",
            "Step 1700 avg train loss = 3.6320\n",
            "Step 1800 avg train loss = 3.6224\n",
            "Step 1900 avg train loss = 3.6364\n",
            "Step 2000 avg train loss = 3.6624\n",
            "Step 2100 avg train loss = 3.6389\n",
            "Step 2200 avg train loss = 3.6535\n",
            "Step 2300 avg train loss = 3.6378\n",
            "Step 2400 avg train loss = 3.6603\n",
            "Validation loss after 9 epoch = 5.6182\n",
            "Step 0 avg train loss = 3.3034\n",
            "Step 100 avg train loss = 3.3584\n",
            "Step 200 avg train loss = 3.3782\n",
            "Step 300 avg train loss = 3.4030\n",
            "Step 400 avg train loss = 3.4027\n",
            "Step 500 avg train loss = 3.4284\n",
            "Step 600 avg train loss = 3.4111\n",
            "Step 700 avg train loss = 3.4428\n",
            "Step 800 avg train loss = 3.4594\n",
            "Step 900 avg train loss = 3.4710\n",
            "Step 1000 avg train loss = 3.4843\n",
            "Step 1100 avg train loss = 3.4743\n",
            "Step 1200 avg train loss = 3.4900\n",
            "Step 1300 avg train loss = 3.4779\n",
            "Step 1400 avg train loss = 3.5020\n",
            "Step 1500 avg train loss = 3.5063\n",
            "Step 1600 avg train loss = 3.5033\n",
            "Step 1700 avg train loss = 3.5028\n",
            "Step 1800 avg train loss = 3.5285\n",
            "Step 1900 avg train loss = 3.5348\n",
            "Step 2000 avg train loss = 3.5361\n",
            "Step 2100 avg train loss = 3.5453\n",
            "Step 2200 avg train loss = 3.5496\n",
            "Step 2300 avg train loss = 3.5588\n",
            "Step 2400 avg train loss = 3.5547\n",
            "Validation loss after 10 epoch = 5.7020\n",
            "Step 0 avg train loss = 3.3197\n",
            "Step 100 avg train loss = 3.2687\n",
            "Step 200 avg train loss = 3.2940\n",
            "Step 300 avg train loss = 3.3024\n",
            "Step 400 avg train loss = 3.2912\n",
            "Step 500 avg train loss = 3.3248\n",
            "Step 600 avg train loss = 3.3277\n",
            "Step 700 avg train loss = 3.3432\n",
            "Step 800 avg train loss = 3.3481\n",
            "Step 900 avg train loss = 3.3556\n",
            "Step 1000 avg train loss = 3.3667\n",
            "Step 1100 avg train loss = 3.3778\n",
            "Step 1200 avg train loss = 3.3893\n",
            "Step 1300 avg train loss = 3.3955\n",
            "Step 1400 avg train loss = 3.4064\n",
            "Step 1500 avg train loss = 3.4020\n",
            "Step 1600 avg train loss = 3.4201\n",
            "Step 1700 avg train loss = 3.4314\n",
            "Step 1800 avg train loss = 3.4327\n",
            "Step 1900 avg train loss = 3.4203\n",
            "Step 2000 avg train loss = 3.4670\n",
            "Step 2100 avg train loss = 3.4326\n",
            "Step 2200 avg train loss = 3.4685\n",
            "Step 2300 avg train loss = 3.4706\n",
            "Step 2400 avg train loss = 3.4718\n",
            "Validation loss after 11 epoch = 5.7811\n",
            "Step 0 avg train loss = 3.1593\n",
            "Step 100 avg train loss = 3.1740\n",
            "Step 200 avg train loss = 3.1943\n",
            "Step 300 avg train loss = 3.2210\n",
            "Step 400 avg train loss = 3.2295\n",
            "Step 500 avg train loss = 3.2266\n",
            "Step 600 avg train loss = 3.2484\n",
            "Step 700 avg train loss = 3.2586\n",
            "Step 800 avg train loss = 3.2983\n",
            "Step 900 avg train loss = 3.2702\n",
            "Step 1000 avg train loss = 3.2896\n",
            "Step 1100 avg train loss = 3.2868\n",
            "Step 1200 avg train loss = 3.2988\n",
            "Step 1300 avg train loss = 3.2955\n",
            "Step 1400 avg train loss = 3.3128\n",
            "Step 1500 avg train loss = 3.3274\n",
            "Step 1600 avg train loss = 3.3191\n",
            "Step 1700 avg train loss = 3.3373\n",
            "Step 1800 avg train loss = 3.3478\n",
            "Step 1900 avg train loss = 3.3784\n",
            "Step 2000 avg train loss = 3.3317\n",
            "Step 2100 avg train loss = 3.3659\n",
            "Step 2200 avg train loss = 3.3760\n",
            "Step 2300 avg train loss = 3.3654\n",
            "Step 2400 avg train loss = 3.3821\n",
            "Validation loss after 12 epoch = 5.8643\n",
            "Step 0 avg train loss = 3.0546\n",
            "Step 100 avg train loss = 3.0860\n",
            "Step 200 avg train loss = 3.1035\n",
            "Step 300 avg train loss = 3.1314\n",
            "Step 400 avg train loss = 3.1468\n",
            "Step 500 avg train loss = 3.1111\n",
            "Step 600 avg train loss = 3.1740\n",
            "Step 700 avg train loss = 3.1720\n",
            "Step 800 avg train loss = 3.1812\n",
            "Step 900 avg train loss = 3.2001\n",
            "Step 1000 avg train loss = 3.2121\n",
            "Step 1100 avg train loss = 3.2036\n",
            "Step 1200 avg train loss = 3.2392\n",
            "Step 1300 avg train loss = 3.2119\n",
            "Step 1400 avg train loss = 3.2444\n",
            "Step 1500 avg train loss = 3.2563\n",
            "Step 1600 avg train loss = 3.2646\n",
            "Step 1700 avg train loss = 3.2761\n",
            "Step 1800 avg train loss = 3.2711\n",
            "Step 1900 avg train loss = 3.2788\n",
            "Step 2000 avg train loss = 3.2643\n",
            "Step 2100 avg train loss = 3.3107\n",
            "Step 2200 avg train loss = 3.2955\n",
            "Step 2300 avg train loss = 3.2918\n",
            "Step 2400 avg train loss = 3.2972\n",
            "Validation loss after 13 epoch = 5.9623\n",
            "Step 0 avg train loss = 2.9659\n",
            "Step 100 avg train loss = 3.0193\n",
            "Step 200 avg train loss = 3.0386\n",
            "Step 300 avg train loss = 3.0568\n",
            "Step 400 avg train loss = 3.0749\n",
            "Step 500 avg train loss = 3.0801\n",
            "Step 600 avg train loss = 3.0783\n",
            "Step 700 avg train loss = 3.0918\n",
            "Step 800 avg train loss = 3.1381\n",
            "Step 900 avg train loss = 3.1109\n",
            "Step 1000 avg train loss = 3.1379\n",
            "Step 1100 avg train loss = 3.1388\n",
            "Step 1200 avg train loss = 3.1538\n",
            "Step 1300 avg train loss = 3.1400\n",
            "Step 1400 avg train loss = 3.1523\n",
            "Step 1500 avg train loss = 3.1817\n",
            "Step 1600 avg train loss = 3.1791\n",
            "Step 1700 avg train loss = 3.1988\n",
            "Step 1800 avg train loss = 3.1831\n",
            "Step 1900 avg train loss = 3.2086\n",
            "Step 2000 avg train loss = 3.2190\n",
            "Step 2100 avg train loss = 3.1977\n",
            "Step 2200 avg train loss = 3.2158\n",
            "Step 2300 avg train loss = 3.2463\n",
            "Step 2400 avg train loss = 3.2277\n",
            "Validation loss after 14 epoch = 6.0368\n",
            "Step 0 avg train loss = 2.9817\n",
            "Step 100 avg train loss = 2.9552\n",
            "Step 200 avg train loss = 2.9650\n",
            "Step 300 avg train loss = 2.9853\n",
            "Step 400 avg train loss = 2.9887\n",
            "Step 500 avg train loss = 3.0032\n",
            "Step 600 avg train loss = 3.0215\n",
            "Step 700 avg train loss = 3.0294\n",
            "Step 800 avg train loss = 3.0399\n",
            "Step 900 avg train loss = 3.0567\n",
            "Step 1000 avg train loss = 3.0624\n",
            "Step 1100 avg train loss = 3.0673\n",
            "Step 1200 avg train loss = 3.0825\n",
            "Step 1300 avg train loss = 3.0895\n",
            "Step 1400 avg train loss = 3.1259\n",
            "Step 1500 avg train loss = 3.0896\n",
            "Step 1600 avg train loss = 3.1016\n",
            "Step 1700 avg train loss = 3.1299\n",
            "Step 1800 avg train loss = 3.1363\n",
            "Step 1900 avg train loss = 3.1474\n",
            "Step 2000 avg train loss = 3.1495\n",
            "Step 2100 avg train loss = 3.1356\n",
            "Step 2200 avg train loss = 3.1607\n",
            "Step 2300 avg train loss = 3.1621\n",
            "Step 2400 avg train loss = 3.1593\n",
            "Validation loss after 15 epoch = 6.1278\n",
            "Step 0 avg train loss = 2.9548\n",
            "Step 100 avg train loss = 2.8902\n",
            "Step 200 avg train loss = 2.9097\n",
            "Step 300 avg train loss = 2.9183\n",
            "Step 400 avg train loss = 2.9435\n",
            "Step 500 avg train loss = 2.9313\n",
            "Step 600 avg train loss = 2.9565\n",
            "Step 700 avg train loss = 2.9946\n",
            "Step 800 avg train loss = 2.9731\n",
            "Step 900 avg train loss = 2.9919\n",
            "Step 1000 avg train loss = 2.9844\n",
            "Step 1100 avg train loss = 3.0177\n",
            "Step 1200 avg train loss = 3.0334\n",
            "Step 1300 avg train loss = 3.0319\n",
            "Step 1400 avg train loss = 3.0392\n",
            "Step 1500 avg train loss = 3.0507\n",
            "Step 1600 avg train loss = 3.0552\n",
            "Step 1700 avg train loss = 3.0480\n",
            "Step 1800 avg train loss = 3.0520\n",
            "Step 1900 avg train loss = 3.0781\n",
            "Step 2000 avg train loss = 3.0756\n",
            "Step 2100 avg train loss = 3.0740\n",
            "Step 2200 avg train loss = 3.0842\n",
            "Step 2300 avg train loss = 3.1047\n",
            "Step 2400 avg train loss = 3.0946\n",
            "Validation loss after 16 epoch = 6.1878\n",
            "Step 0 avg train loss = 2.9692\n",
            "Step 100 avg train loss = 2.8490\n",
            "Step 200 avg train loss = 2.8594\n",
            "Step 300 avg train loss = 2.8711\n",
            "Step 400 avg train loss = 2.8764\n",
            "Step 500 avg train loss = 2.8777\n",
            "Step 600 avg train loss = 2.8987\n",
            "Step 700 avg train loss = 2.9009\n",
            "Step 800 avg train loss = 2.9115\n",
            "Step 900 avg train loss = 2.9392\n",
            "Step 1000 avg train loss = 2.9389\n",
            "Step 1100 avg train loss = 2.9538\n",
            "Step 1200 avg train loss = 2.9782\n",
            "Step 1300 avg train loss = 2.9729\n",
            "Step 1400 avg train loss = 2.9739\n",
            "Step 1500 avg train loss = 2.9782\n",
            "Step 1600 avg train loss = 2.9799\n",
            "Step 1700 avg train loss = 3.0012\n",
            "Step 1800 avg train loss = 3.0020\n",
            "Step 1900 avg train loss = 3.0152\n",
            "Step 2000 avg train loss = 3.0033\n",
            "Step 2100 avg train loss = 3.0122\n",
            "Step 2200 avg train loss = 3.0447\n",
            "Step 2300 avg train loss = 3.0528\n",
            "Step 2400 avg train loss = 3.0611\n",
            "Validation loss after 17 epoch = 6.2703\n",
            "Step 0 avg train loss = 2.6015\n",
            "Step 100 avg train loss = 2.7929\n",
            "Step 200 avg train loss = 2.8007\n",
            "Step 300 avg train loss = 2.8053\n",
            "Step 400 avg train loss = 2.8242\n",
            "Step 500 avg train loss = 2.8374\n",
            "Step 600 avg train loss = 2.8434\n",
            "Step 700 avg train loss = 2.8758\n",
            "Step 800 avg train loss = 2.8585\n",
            "Step 900 avg train loss = 2.8912\n",
            "Step 1000 avg train loss = 2.8690\n",
            "Step 1100 avg train loss = 2.8939\n",
            "Step 1200 avg train loss = 2.8830\n",
            "Step 1300 avg train loss = 2.9047\n",
            "Step 1400 avg train loss = 2.9210\n",
            "Step 1500 avg train loss = 2.9172\n",
            "Step 1600 avg train loss = 2.9197\n",
            "Step 1700 avg train loss = 2.9678\n",
            "Step 1800 avg train loss = 2.9587\n",
            "Step 1900 avg train loss = 2.9517\n",
            "Step 2000 avg train loss = 2.9617\n",
            "Step 2100 avg train loss = 2.9888\n",
            "Step 2200 avg train loss = 2.9582\n",
            "Step 2300 avg train loss = 3.0080\n",
            "Step 2400 avg train loss = 2.9950\n",
            "Validation loss after 18 epoch = 6.3557\n",
            "Step 0 avg train loss = 2.8078\n",
            "Step 100 avg train loss = 2.7176\n",
            "Step 200 avg train loss = 2.7478\n",
            "Step 300 avg train loss = 2.7561\n",
            "Step 400 avg train loss = 2.7776\n",
            "Step 500 avg train loss = 2.7945\n",
            "Step 600 avg train loss = 2.7926\n",
            "Step 700 avg train loss = 2.8061\n",
            "Step 800 avg train loss = 2.8010\n",
            "Step 900 avg train loss = 2.8376\n",
            "Step 1000 avg train loss = 2.8438\n",
            "Step 1100 avg train loss = 2.8430\n",
            "Step 1200 avg train loss = 2.8473\n",
            "Step 1300 avg train loss = 2.8606\n",
            "Step 1400 avg train loss = 2.8584\n",
            "Step 1500 avg train loss = 2.8948\n",
            "Step 1600 avg train loss = 2.8853\n",
            "Step 1700 avg train loss = 2.8977\n",
            "Step 1800 avg train loss = 2.8820\n",
            "Step 1900 avg train loss = 2.9080\n",
            "Step 2000 avg train loss = 2.9208\n",
            "Step 2100 avg train loss = 2.9194\n",
            "Step 2200 avg train loss = 2.9315\n",
            "Step 2300 avg train loss = 2.9370\n",
            "Step 2400 avg train loss = 2.9421\n",
            "Validation loss after 19 epoch = 6.4216\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 10.4077\n",
            "Step 100 avg train loss = 7.6064\n",
            "Step 200 avg train loss = 7.1188\n",
            "Step 300 avg train loss = 6.8872\n",
            "Step 400 avg train loss = 6.7077\n",
            "Step 500 avg train loss = 6.5587\n",
            "Step 600 avg train loss = 6.4424\n",
            "Step 700 avg train loss = 6.3522\n",
            "Step 800 avg train loss = 6.2640\n",
            "Step 900 avg train loss = 6.1831\n",
            "Step 1000 avg train loss = 6.1223\n",
            "Step 1100 avg train loss = 6.0599\n",
            "Step 1200 avg train loss = 5.9943\n",
            "Step 1300 avg train loss = 5.9851\n",
            "Step 1400 avg train loss = 5.9378\n",
            "Step 1500 avg train loss = 5.9196\n",
            "Step 1600 avg train loss = 5.8563\n",
            "Step 1700 avg train loss = 5.8292\n",
            "Step 1800 avg train loss = 5.8014\n",
            "Step 1900 avg train loss = 5.7638\n",
            "Step 2000 avg train loss = 5.7581\n",
            "Step 2100 avg train loss = 5.7021\n",
            "Step 2200 avg train loss = 5.6820\n",
            "Step 2300 avg train loss = 5.6952\n",
            "Step 2400 avg train loss = 5.6331\n",
            "Validation loss after 0 epoch = 5.4937\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.7384\n",
            "Step 100 avg train loss = 5.4760\n",
            "Step 200 avg train loss = 5.4694\n",
            "Step 300 avg train loss = 5.4564\n",
            "Step 400 avg train loss = 5.4394\n",
            "Step 500 avg train loss = 5.4185\n",
            "Step 600 avg train loss = 5.3846\n",
            "Step 700 avg train loss = 5.3973\n",
            "Step 800 avg train loss = 5.3728\n",
            "Step 900 avg train loss = 5.3445\n",
            "Step 1000 avg train loss = 5.3464\n",
            "Step 1100 avg train loss = 5.3261\n",
            "Step 1200 avg train loss = 5.3276\n",
            "Step 1300 avg train loss = 5.2857\n",
            "Step 1400 avg train loss = 5.3209\n",
            "Step 1500 avg train loss = 5.2787\n",
            "Step 1600 avg train loss = 5.2963\n",
            "Step 1700 avg train loss = 5.2553\n",
            "Step 1800 avg train loss = 5.2516\n",
            "Step 1900 avg train loss = 5.2371\n",
            "Step 2000 avg train loss = 5.2500\n",
            "Step 2100 avg train loss = 5.2220\n",
            "Step 2200 avg train loss = 5.1821\n",
            "Step 2300 avg train loss = 5.1965\n",
            "Step 2400 avg train loss = 5.2011\n",
            "Validation loss after 1 epoch = 5.2352\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 5.1050\n",
            "Step 100 avg train loss = 4.9629\n",
            "Step 200 avg train loss = 4.9514\n",
            "Step 300 avg train loss = 4.9862\n",
            "Step 400 avg train loss = 4.9708\n",
            "Step 500 avg train loss = 4.9591\n",
            "Step 600 avg train loss = 4.9485\n",
            "Step 700 avg train loss = 4.9427\n",
            "Step 800 avg train loss = 4.9337\n",
            "Step 900 avg train loss = 4.9361\n",
            "Step 1000 avg train loss = 4.9320\n",
            "Step 1100 avg train loss = 4.9463\n",
            "Step 1200 avg train loss = 4.9101\n",
            "Step 1300 avg train loss = 4.8964\n",
            "Step 1400 avg train loss = 4.9356\n",
            "Step 1500 avg train loss = 4.9182\n",
            "Step 1600 avg train loss = 4.8843\n",
            "Step 1700 avg train loss = 4.9028\n",
            "Step 1800 avg train loss = 4.8957\n",
            "Step 1900 avg train loss = 4.8639\n",
            "Step 2000 avg train loss = 4.8747\n",
            "Step 2100 avg train loss = 4.8480\n",
            "Step 2200 avg train loss = 4.8580\n",
            "Step 2300 avg train loss = 4.8615\n",
            "Step 2400 avg train loss = 4.8667\n",
            "Validation loss after 2 epoch = 5.1417\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.7328\n",
            "Step 100 avg train loss = 4.6224\n",
            "Step 200 avg train loss = 4.6219\n",
            "Step 300 avg train loss = 4.5910\n",
            "Step 400 avg train loss = 4.6119\n",
            "Step 500 avg train loss = 4.6219\n",
            "Step 600 avg train loss = 4.6087\n",
            "Step 700 avg train loss = 4.5887\n",
            "Step 800 avg train loss = 4.6004\n",
            "Step 900 avg train loss = 4.6072\n",
            "Step 1000 avg train loss = 4.6042\n",
            "Step 1100 avg train loss = 4.5923\n",
            "Step 1200 avg train loss = 4.6117\n",
            "Step 1300 avg train loss = 4.5944\n",
            "Step 1400 avg train loss = 4.6115\n",
            "Step 1500 avg train loss = 4.6043\n",
            "Step 1600 avg train loss = 4.6114\n",
            "Step 1700 avg train loss = 4.5812\n",
            "Step 1800 avg train loss = 4.5925\n",
            "Step 1900 avg train loss = 4.5814\n",
            "Step 2000 avg train loss = 4.5873\n",
            "Step 2100 avg train loss = 4.5977\n",
            "Step 2200 avg train loss = 4.5885\n",
            "Step 2300 avg train loss = 4.5981\n",
            "Step 2400 avg train loss = 4.5930\n",
            "Validation loss after 3 epoch = 5.1347\n",
            "update best model with this parameter to :\n",
            "LSTMModel(\n",
            "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
            "  (lstm): LSTM(300, 500, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (projection): Linear(in_features=500, out_features=33178, bias=True)\n",
            ")\n",
            "Step 0 avg train loss = 4.3048\n",
            "Step 100 avg train loss = 4.3094\n",
            "Step 200 avg train loss = 4.2887\n",
            "Step 300 avg train loss = 4.3226\n",
            "Step 400 avg train loss = 4.3107\n",
            "Step 500 avg train loss = 4.3007\n",
            "Step 600 avg train loss = 4.3311\n",
            "Step 700 avg train loss = 4.3349\n",
            "Step 800 avg train loss = 4.3399\n",
            "Step 900 avg train loss = 4.3247\n",
            "Step 1000 avg train loss = 4.3379\n",
            "Step 1100 avg train loss = 4.3310\n",
            "Step 1200 avg train loss = 4.3613\n",
            "Step 1300 avg train loss = 4.3321\n",
            "Step 1400 avg train loss = 4.3506\n",
            "Step 1500 avg train loss = 4.3295\n",
            "Step 1600 avg train loss = 4.3580\n",
            "Step 1700 avg train loss = 4.3405\n",
            "Step 1800 avg train loss = 4.3327\n",
            "Step 1900 avg train loss = 4.3413\n",
            "Step 2000 avg train loss = 4.3332\n",
            "Step 2100 avg train loss = 4.3441\n",
            "Step 2200 avg train loss = 4.3529\n",
            "Step 2300 avg train loss = 4.3523\n",
            "Step 2400 avg train loss = 4.3593\n",
            "Validation loss after 4 epoch = 5.1925\n",
            "Step 0 avg train loss = 4.0341\n",
            "Step 100 avg train loss = 4.0648\n",
            "Step 200 avg train loss = 4.0773\n",
            "Step 300 avg train loss = 4.0565\n",
            "Step 400 avg train loss = 4.0659\n",
            "Step 500 avg train loss = 4.0704\n",
            "Step 600 avg train loss = 4.0727\n",
            "Step 700 avg train loss = 4.0718\n",
            "Step 800 avg train loss = 4.0730\n",
            "Step 900 avg train loss = 4.1037\n",
            "Step 1000 avg train loss = 4.1036\n",
            "Step 1100 avg train loss = 4.1140\n",
            "Step 1200 avg train loss = 4.1229\n",
            "Step 1300 avg train loss = 4.1157\n",
            "Step 1400 avg train loss = 4.1211\n",
            "Step 1500 avg train loss = 4.1316\n",
            "Step 1600 avg train loss = 4.1158\n",
            "Step 1700 avg train loss = 4.1222\n",
            "Step 1800 avg train loss = 4.1366\n",
            "Step 1900 avg train loss = 4.1232\n",
            "Step 2000 avg train loss = 4.1374\n",
            "Step 2100 avg train loss = 4.1356\n",
            "Step 2200 avg train loss = 4.1401\n",
            "Step 2300 avg train loss = 4.1441\n",
            "Step 2400 avg train loss = 4.1589\n",
            "Validation loss after 5 epoch = 5.2773\n",
            "Step 0 avg train loss = 3.7871\n",
            "Step 100 avg train loss = 3.8495\n",
            "Step 200 avg train loss = 3.8627\n",
            "Step 300 avg train loss = 3.8637\n",
            "Step 400 avg train loss = 3.8582\n",
            "Step 500 avg train loss = 3.8678\n",
            "Step 600 avg train loss = 3.8964\n",
            "Step 700 avg train loss = 3.8671\n",
            "Step 800 avg train loss = 3.9013\n",
            "Step 900 avg train loss = 3.9142\n",
            "Step 1000 avg train loss = 3.8942\n",
            "Step 1100 avg train loss = 3.9272\n",
            "Step 1200 avg train loss = 3.9077\n",
            "Step 1300 avg train loss = 3.9375\n",
            "Step 1400 avg train loss = 3.9307\n",
            "Step 1500 avg train loss = 3.9406\n",
            "Step 1600 avg train loss = 3.9097\n",
            "Step 1700 avg train loss = 3.9314\n",
            "Step 1800 avg train loss = 3.9388\n",
            "Step 1900 avg train loss = 3.9174\n",
            "Step 2000 avg train loss = 3.9537\n",
            "Step 2100 avg train loss = 3.9427\n",
            "Step 2200 avg train loss = 3.9539\n",
            "Step 2300 avg train loss = 3.9729\n",
            "Step 2400 avg train loss = 3.9640\n",
            "Validation loss after 6 epoch = 5.3755\n",
            "Step 0 avg train loss = 3.7822\n",
            "Step 100 avg train loss = 3.6365\n",
            "Step 200 avg train loss = 3.6531\n",
            "Step 300 avg train loss = 3.6741\n",
            "Step 400 avg train loss = 3.6637\n",
            "Step 500 avg train loss = 3.7043\n",
            "Step 600 avg train loss = 3.6737\n",
            "Step 700 avg train loss = 3.7231\n",
            "Step 800 avg train loss = 3.7235\n",
            "Step 900 avg train loss = 3.7163\n",
            "Step 1000 avg train loss = 3.7501\n",
            "Step 1100 avg train loss = 3.7475\n",
            "Step 1200 avg train loss = 3.7483\n",
            "Step 1300 avg train loss = 3.7670\n",
            "Step 1400 avg train loss = 3.7549\n",
            "Step 1500 avg train loss = 3.7761\n",
            "Step 1600 avg train loss = 3.7613\n",
            "Step 1700 avg train loss = 3.7767\n",
            "Step 1800 avg train loss = 3.7785\n",
            "Step 1900 avg train loss = 3.7836\n",
            "Step 2000 avg train loss = 3.8006\n",
            "Step 2100 avg train loss = 3.7794\n",
            "Step 2200 avg train loss = 3.7850\n",
            "Step 2300 avg train loss = 3.8056\n",
            "Step 2400 avg train loss = 3.8014\n",
            "Validation loss after 7 epoch = 5.4719\n",
            "Step 0 avg train loss = 3.5151\n",
            "Step 100 avg train loss = 3.4982\n",
            "Step 200 avg train loss = 3.5009\n",
            "Step 300 avg train loss = 3.5243\n",
            "Step 400 avg train loss = 3.5202\n",
            "Step 500 avg train loss = 3.5344\n",
            "Step 600 avg train loss = 3.5566\n",
            "Step 700 avg train loss = 3.5698\n",
            "Step 800 avg train loss = 3.5740\n",
            "Step 900 avg train loss = 3.5883\n",
            "Step 1000 avg train loss = 3.5904\n",
            "Step 1100 avg train loss = 3.5711\n",
            "Step 1200 avg train loss = 3.5960\n",
            "Step 1300 avg train loss = 3.6224\n",
            "Step 1400 avg train loss = 3.6314\n",
            "Step 1500 avg train loss = 3.6115\n",
            "Step 1600 avg train loss = 3.6116\n",
            "Step 1700 avg train loss = 3.6117\n",
            "Step 1800 avg train loss = 3.6321\n",
            "Step 1900 avg train loss = 3.6276\n",
            "Step 2000 avg train loss = 3.6327\n",
            "Step 2100 avg train loss = 3.6326\n",
            "Step 2200 avg train loss = 3.6578\n",
            "Step 2300 avg train loss = 3.6335\n",
            "Step 2400 avg train loss = 3.6607\n",
            "Validation loss after 8 epoch = 5.5715\n",
            "Step 0 avg train loss = 3.3935\n",
            "Step 100 avg train loss = 3.3383\n",
            "Step 200 avg train loss = 3.3612\n",
            "Step 300 avg train loss = 3.3737\n",
            "Step 400 avg train loss = 3.3933\n",
            "Step 500 avg train loss = 3.3982\n",
            "Step 600 avg train loss = 3.4095\n",
            "Step 700 avg train loss = 3.4026\n",
            "Step 800 avg train loss = 3.4239\n",
            "Step 900 avg train loss = 3.4404\n",
            "Step 1000 avg train loss = 3.4640\n",
            "Step 1100 avg train loss = 3.4526\n",
            "Step 1200 avg train loss = 3.4546\n",
            "Step 1300 avg train loss = 3.4692\n",
            "Step 1400 avg train loss = 3.4936\n",
            "Step 1500 avg train loss = 3.4688\n",
            "Step 1600 avg train loss = 3.4947\n",
            "Step 1700 avg train loss = 3.5107\n",
            "Step 1800 avg train loss = 3.5144\n",
            "Step 1900 avg train loss = 3.4911\n",
            "Step 2000 avg train loss = 3.5131\n",
            "Step 2100 avg train loss = 3.5350\n",
            "Step 2200 avg train loss = 3.5237\n",
            "Step 2300 avg train loss = 3.5331\n",
            "Step 2400 avg train loss = 3.5294\n",
            "Validation loss after 9 epoch = 5.6814\n",
            "Step 0 avg train loss = 3.0327\n",
            "Step 100 avg train loss = 3.2090\n",
            "Step 200 avg train loss = 3.2490\n",
            "Step 300 avg train loss = 3.2668\n",
            "Step 400 avg train loss = 3.2697\n",
            "Step 500 avg train loss = 3.2815\n",
            "Step 600 avg train loss = 3.2876\n",
            "Step 700 avg train loss = 3.3146\n",
            "Step 800 avg train loss = 3.3071\n",
            "Step 900 avg train loss = 3.3215\n",
            "Step 1000 avg train loss = 3.3224\n",
            "Step 1100 avg train loss = 3.3219\n",
            "Step 1200 avg train loss = 3.3379\n",
            "Step 1300 avg train loss = 3.3503\n",
            "Step 1400 avg train loss = 3.3554\n",
            "Step 1500 avg train loss = 3.3446\n",
            "Step 1600 avg train loss = 3.3939\n",
            "Step 1700 avg train loss = 3.3791\n",
            "Step 1800 avg train loss = 3.3890\n",
            "Step 1900 avg train loss = 3.3869\n",
            "Step 2000 avg train loss = 3.4056\n",
            "Step 2100 avg train loss = 3.3935\n",
            "Step 2200 avg train loss = 3.3943\n",
            "Step 2300 avg train loss = 3.4037\n",
            "Step 2400 avg train loss = 3.4180\n",
            "Validation loss after 10 epoch = 5.7895\n",
            "Step 0 avg train loss = 2.8903\n",
            "Step 100 avg train loss = 3.1383\n",
            "Step 200 avg train loss = 3.1143\n",
            "Step 300 avg train loss = 3.1353\n",
            "Step 400 avg train loss = 3.1768\n",
            "Step 500 avg train loss = 3.1719\n",
            "Step 600 avg train loss = 3.1761\n",
            "Step 700 avg train loss = 3.1903\n",
            "Step 800 avg train loss = 3.1938\n",
            "Step 900 avg train loss = 3.2128\n",
            "Step 1000 avg train loss = 3.1902\n",
            "Step 1100 avg train loss = 3.2145\n",
            "Step 1200 avg train loss = 3.2526\n",
            "Step 1300 avg train loss = 3.2469\n",
            "Step 1400 avg train loss = 3.2426\n",
            "Step 1500 avg train loss = 3.2509\n",
            "Step 1600 avg train loss = 3.2589\n",
            "Step 1700 avg train loss = 3.2584\n",
            "Step 1800 avg train loss = 3.2709\n",
            "Step 1900 avg train loss = 3.2852\n",
            "Step 2000 avg train loss = 3.2846\n",
            "Step 2100 avg train loss = 3.2778\n",
            "Step 2200 avg train loss = 3.2944\n",
            "Step 2300 avg train loss = 3.3308\n",
            "Step 2400 avg train loss = 3.3094\n",
            "Validation loss after 11 epoch = 5.8828\n",
            "Step 0 avg train loss = 3.0910\n",
            "Step 100 avg train loss = 3.0178\n",
            "Step 200 avg train loss = 3.0330\n",
            "Step 300 avg train loss = 3.0473\n",
            "Step 400 avg train loss = 3.0549\n",
            "Step 500 avg train loss = 3.0530\n",
            "Step 600 avg train loss = 3.0847\n",
            "Step 700 avg train loss = 3.0864\n",
            "Step 800 avg train loss = 3.0970\n",
            "Step 900 avg train loss = 3.1050\n",
            "Step 1000 avg train loss = 3.1211\n",
            "Step 1100 avg train loss = 3.1404\n",
            "Step 1200 avg train loss = 3.1259\n",
            "Step 1300 avg train loss = 3.1181\n",
            "Step 1400 avg train loss = 3.1375\n",
            "Step 1500 avg train loss = 3.1584\n",
            "Step 1600 avg train loss = 3.1662\n",
            "Step 1700 avg train loss = 3.1725\n",
            "Step 1800 avg train loss = 3.1680\n",
            "Step 1900 avg train loss = 3.1846\n",
            "Step 2000 avg train loss = 3.1896\n",
            "Step 2100 avg train loss = 3.1954\n",
            "Step 2200 avg train loss = 3.1878\n",
            "Step 2300 avg train loss = 3.2092\n",
            "Step 2400 avg train loss = 3.2150\n",
            "Validation loss after 12 epoch = 5.9675\n",
            "Step 0 avg train loss = 2.9783\n",
            "Step 100 avg train loss = 2.9048\n",
            "Step 200 avg train loss = 2.9429\n",
            "Step 300 avg train loss = 2.9448\n",
            "Step 400 avg train loss = 2.9676\n",
            "Step 500 avg train loss = 2.9684\n",
            "Step 600 avg train loss = 2.9781\n",
            "Step 700 avg train loss = 2.9828\n",
            "Step 800 avg train loss = 2.9920\n",
            "Step 900 avg train loss = 3.0052\n",
            "Step 1000 avg train loss = 3.0129\n",
            "Step 1100 avg train loss = 3.0453\n",
            "Step 1200 avg train loss = 3.0511\n",
            "Step 1300 avg train loss = 3.0580\n",
            "Step 1400 avg train loss = 3.0458\n",
            "Step 1500 avg train loss = 3.0590\n",
            "Step 1600 avg train loss = 3.0758\n",
            "Step 1700 avg train loss = 3.0930\n",
            "Step 1800 avg train loss = 3.0658\n",
            "Step 1900 avg train loss = 3.1115\n",
            "Step 2000 avg train loss = 3.1061\n",
            "Step 2100 avg train loss = 3.1086\n",
            "Step 2200 avg train loss = 3.1080\n",
            "Step 2300 avg train loss = 3.1236\n",
            "Step 2400 avg train loss = 3.1132\n",
            "Validation loss after 13 epoch = 6.0703\n",
            "Step 0 avg train loss = 2.8731\n",
            "Step 100 avg train loss = 2.8398\n",
            "Step 200 avg train loss = 2.8621\n",
            "Step 300 avg train loss = 2.8618\n",
            "Step 400 avg train loss = 2.8679\n",
            "Step 500 avg train loss = 2.8782\n",
            "Step 600 avg train loss = 2.8914\n",
            "Step 700 avg train loss = 2.9137\n",
            "Step 800 avg train loss = 2.9158\n",
            "Step 900 avg train loss = 2.9425\n",
            "Step 1000 avg train loss = 2.9437\n",
            "Step 1100 avg train loss = 2.9451\n",
            "Step 1200 avg train loss = 2.9310\n",
            "Step 1300 avg train loss = 2.9597\n",
            "Step 1400 avg train loss = 2.9745\n",
            "Step 1500 avg train loss = 2.9727\n",
            "Step 1600 avg train loss = 2.9810\n",
            "Step 1700 avg train loss = 2.9899\n",
            "Step 1800 avg train loss = 3.0147\n",
            "Step 1900 avg train loss = 3.0164\n",
            "Step 2000 avg train loss = 3.0084\n",
            "Step 2100 avg train loss = 3.0050\n",
            "Step 2200 avg train loss = 3.0206\n",
            "Step 2300 avg train loss = 3.0476\n",
            "Step 2400 avg train loss = 3.0498\n",
            "Validation loss after 14 epoch = 6.1553\n",
            "Step 0 avg train loss = 2.7818\n",
            "Step 100 avg train loss = 2.7490\n",
            "Step 200 avg train loss = 2.7735\n",
            "Step 300 avg train loss = 2.7666\n",
            "Step 400 avg train loss = 2.7970\n",
            "Step 500 avg train loss = 2.8040\n",
            "Step 600 avg train loss = 2.8197\n",
            "Step 700 avg train loss = 2.8474\n",
            "Step 800 avg train loss = 2.8349\n",
            "Step 900 avg train loss = 2.8583\n",
            "Step 1000 avg train loss = 2.8626\n",
            "Step 1100 avg train loss = 2.8696\n",
            "Step 1200 avg train loss = 2.8801\n",
            "Step 1300 avg train loss = 2.8791\n",
            "Step 1400 avg train loss = 2.9030\n",
            "Step 1500 avg train loss = 2.9135\n",
            "Step 1600 avg train loss = 2.9102\n",
            "Step 1700 avg train loss = 2.9090\n",
            "Step 1800 avg train loss = 2.9311\n",
            "Step 1900 avg train loss = 2.9086\n",
            "Step 2000 avg train loss = 2.9217\n",
            "Step 2100 avg train loss = 2.9261\n",
            "Step 2200 avg train loss = 2.9566\n",
            "Step 2300 avg train loss = 2.9619\n",
            "Step 2400 avg train loss = 2.9721\n",
            "Validation loss after 15 epoch = 6.2485\n",
            "Step 0 avg train loss = 2.5668\n",
            "Step 100 avg train loss = 2.6929\n",
            "Step 200 avg train loss = 2.6929\n",
            "Step 300 avg train loss = 2.7125\n",
            "Step 400 avg train loss = 2.7135\n",
            "Step 500 avg train loss = 2.7303\n",
            "Step 600 avg train loss = 2.7504\n",
            "Step 700 avg train loss = 2.7528\n",
            "Step 800 avg train loss = 2.7709\n",
            "Step 900 avg train loss = 2.7720\n",
            "Step 1000 avg train loss = 2.7981\n",
            "Step 1100 avg train loss = 2.7916\n",
            "Step 1200 avg train loss = 2.7920\n",
            "Step 1300 avg train loss = 2.8145\n",
            "Step 1400 avg train loss = 2.8174\n",
            "Step 1500 avg train loss = 2.8180\n",
            "Step 1600 avg train loss = 2.8438\n",
            "Step 1700 avg train loss = 2.8491\n",
            "Step 1800 avg train loss = 2.8554\n",
            "Step 1900 avg train loss = 2.8658\n",
            "Step 2000 avg train loss = 2.8525\n",
            "Step 2100 avg train loss = 2.8779\n",
            "Step 2200 avg train loss = 2.8815\n",
            "Step 2300 avg train loss = 2.8795\n",
            "Step 2400 avg train loss = 2.8964\n",
            "Validation loss after 16 epoch = 6.3413\n",
            "Step 0 avg train loss = 2.6056\n",
            "Step 100 avg train loss = 2.5963\n",
            "Step 200 avg train loss = 2.6145\n",
            "Step 300 avg train loss = 2.6291\n",
            "Step 400 avg train loss = 2.6377\n",
            "Step 500 avg train loss = 2.6703\n",
            "Step 600 avg train loss = 2.6745\n",
            "Step 700 avg train loss = 2.6891\n",
            "Step 800 avg train loss = 2.6998\n",
            "Step 900 avg train loss = 2.7105\n",
            "Step 1000 avg train loss = 2.7068\n",
            "Step 1100 avg train loss = 2.7122\n",
            "Step 1200 avg train loss = 2.7520\n",
            "Step 1300 avg train loss = 2.7555\n",
            "Step 1400 avg train loss = 2.7477\n",
            "Step 1500 avg train loss = 2.7717\n",
            "Step 1600 avg train loss = 2.7634\n",
            "Step 1700 avg train loss = 2.7909\n",
            "Step 1800 avg train loss = 2.7942\n",
            "Step 1900 avg train loss = 2.7838\n",
            "Step 2000 avg train loss = 2.8128\n",
            "Step 2100 avg train loss = 2.8115\n",
            "Step 2200 avg train loss = 2.8254\n",
            "Step 2300 avg train loss = 2.8114\n",
            "Step 2400 avg train loss = 2.8193\n",
            "Validation loss after 17 epoch = 6.4270\n",
            "Step 0 avg train loss = 2.5140\n",
            "Step 100 avg train loss = 2.5560\n",
            "Step 200 avg train loss = 2.5445\n",
            "Step 300 avg train loss = 2.5535\n",
            "Step 400 avg train loss = 2.5887\n",
            "Step 500 avg train loss = 2.6163\n",
            "Step 600 avg train loss = 2.6158\n",
            "Step 700 avg train loss = 2.6324\n",
            "Step 800 avg train loss = 2.6554\n",
            "Step 900 avg train loss = 2.6444\n",
            "Step 1000 avg train loss = 2.6562\n",
            "Step 1100 avg train loss = 2.6605\n",
            "Step 1200 avg train loss = 2.6738\n",
            "Step 1300 avg train loss = 2.6762\n",
            "Step 1400 avg train loss = 2.6952\n",
            "Step 1500 avg train loss = 2.6882\n",
            "Step 1600 avg train loss = 2.7029\n",
            "Step 1700 avg train loss = 2.7159\n",
            "Step 1800 avg train loss = 2.7199\n",
            "Step 1900 avg train loss = 2.7224\n",
            "Step 2000 avg train loss = 2.7396\n",
            "Step 2100 avg train loss = 2.7458\n",
            "Step 2200 avg train loss = 2.7488\n",
            "Step 2300 avg train loss = 2.7504\n",
            "Step 2400 avg train loss = 2.7764\n",
            "Validation loss after 18 epoch = 6.5052\n",
            "Step 0 avg train loss = 2.2892\n",
            "Step 100 avg train loss = 2.4944\n",
            "Step 200 avg train loss = 2.5011\n",
            "Step 300 avg train loss = 2.5263\n",
            "Step 400 avg train loss = 2.5102\n",
            "Step 500 avg train loss = 2.5455\n",
            "Step 600 avg train loss = 2.5584\n",
            "Step 700 avg train loss = 2.5731\n",
            "Step 800 avg train loss = 2.5781\n",
            "Step 900 avg train loss = 2.5899\n",
            "Step 1000 avg train loss = 2.5859\n",
            "Step 1100 avg train loss = 2.6234\n",
            "Step 1200 avg train loss = 2.6151\n",
            "Step 1300 avg train loss = 2.6376\n",
            "Step 1400 avg train loss = 2.6130\n",
            "Step 1500 avg train loss = 2.6328\n",
            "Step 1600 avg train loss = 2.6496\n",
            "Step 1700 avg train loss = 2.6566\n",
            "Step 1800 avg train loss = 2.6552\n",
            "Step 1900 avg train loss = 2.6707\n",
            "Step 2000 avg train loss = 2.6854\n",
            "Step 2100 avg train loss = 2.6866\n",
            "Step 2200 avg train loss = 2.6947\n",
            "Step 2300 avg train loss = 2.7072\n",
            "Step 2400 avg train loss = 2.6940\n",
            "Validation loss after 19 epoch = 6.6090\n",
            "Saving best model with best hidden size...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c0b13b17a497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m'loss_cache'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplot_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m'model_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_model_overall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         }, './drive/My Drive/NLP/hid_tune_best_LSTM.pt')\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/My Drive/NLP/hid_tune_best_LSTM.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQux1ZwmIFbb",
        "colab_type": "code",
        "outputId": "e4634760-7dcd-4178-e657-138781521114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZK_pPW8Bvw",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IktjeUOat0Ol",
        "colab_type": "code",
        "outputId": "a83b952b-996e-482a-9957-0dbc540067e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# embedding_dim: 100-200-300-400-500 with default hidden_size: 128\n",
        "# [5.2549,5.2085,5.2010,5.2055,5.2255]\n",
        "# 6 - 5 - 5 - 5 - 4 epoch]\n",
        "\n",
        "with open('tune_val_loss.pkl', 'rb') as f:\n",
        "  tune_val_loss = pickle.load(f)\n",
        "\n",
        "plot_emb = tune_val_loss[0]\n",
        "perp_emb = [2**(i/np.log(2)) for i in plot_emb] \n",
        "\n",
        "emb = np.array([100,200,300,400,500])\n",
        "plt.plot(hid, perp_emb, color='b', linestyle='dashed', marker='o', \n",
        "         label='best validation ppl' )\n",
        "\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(100, 600, step=100))\n",
        "plt.title('Best PPL for Varying Embedding Dimension')\n",
        "plt.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRG+MMXHOEr0xxsQ5S/TGGBPn/h8HKPXw1AfntwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcBmcCR_8Bvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "b10812e1-d0c7-4051-d72a-b784a6518e0c"
      },
      "source": [
        "# hidden_size: 100-200-300-400-500 with best embedding_dim: 300\n",
        "# 5.2207 - 5.1723 - 5.1226 - 5.1319 - 5.1347\n",
        "# 7 - 4 - 3 - 3 - 3\n",
        "\n",
        "plot_hid = tune_val_loss[1]\n",
        "perp_hid = [2**(i/np.log(2)) for i in plot_hid] \n",
        "\n",
        "hid = np.array([100,200,300,400,500])\n",
        "plt.plot(hid, perp_hid, color='g', linestyle='dashed', marker='o', label='best validation ppl' )\n",
        "\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(100, 600, step=100))\n",
        "plt.title('Best PPL for Varying Hidden Size ')\n",
        "plt.show()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGMMYAnBGGOMwxKCMcYYwBKCMcYYx/8DLybv3nGYv5oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtUgQcLw8Bv1",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcC--G6w8Bv3",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X1AUQwoxmpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 300\n",
        "hidden_size = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmYE5Zx8Bv4",
        "colab_type": "code",
        "outputId": "23b1e427-27cb-4022-b00f-dfecc24dcfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# TODO: Get the best model parameter from 1.2.1 \n",
        "options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "model = LSTMModel(options)\n",
        "model.load_state_dict(torch.load(F'/content/drive/My Drive/NLP/best_emb_dim_300_hidden_size_300_LSTM.pt')['model_dict'])\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 494
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQK27bsS8Bv-",
        "colab_type": "code",
        "outputId": "36537d31-2101-4293-ef81-7a865327bc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy()) \n",
        "    # transfer tensor back to cpu\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['split']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDAT-P2L8BwC",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq1WyoUK8BwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(weight_matrix, words):\n",
        "#make sure at least appear in the text once \n",
        "    whole_word = list()\n",
        "    for word in words:\n",
        "        if word not in vocab:\n",
        "            raise NotImplementedError(\"selected token must appeared in the corpus once, please replace {}\".format(word))\n",
        "            \n",
        "    # define my cosine similarity. each input is 1 x 64. Therefore evaluate along x'axis returns a scalar \n",
        "    cos = nn.CosineSimilarity(dim = 0, eps = 1e-6)\n",
        "\n",
        "    dis_list = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    for w in words:\n",
        "        wid = train_dict.get_id(w)\n",
        "        temp_list = list()\n",
        "        for i in range(len(weight_matrix)):\n",
        "            if i == wid:\n",
        "                continue\n",
        "            else: \n",
        "                dis = cos(weight_matrix[wid], weight_matrix[i])\n",
        "                temp_list.append(tuple((train_dict.get_token(i), dis.item())))\n",
        "        temp_list = sorted(temp_list, key = lambda x: x[-1])\n",
        "        dis_list[w]['worst'] = temp_list[:10]\n",
        "        dis_list[w]['best'] = temp_list[-10:]\n",
        "        worst_words = [x[0] for x in temp_list[:10]]\n",
        "        best_words = [x[0] for x in temp_list[-10:]]\n",
        "        whole_word += worst_words + best_words\n",
        "    return dis_list, whole_word + words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9OXVOFX8BwI",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThaWEAgk8BwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_matrix_lkup = model.lookup.weight\n",
        "words = ['productive', 'teenage','south','antelope','smart']\n",
        "# some verbs, and nouns\n",
        "lkup_list, whole_words_lkup = cos_similarity(weight_matrix_lkup, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqtEzK2y8Bw5",
        "colab_type": "code",
        "outputId": "f3e5ce63-1740-4e6d-dcdf-cc5f29501400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "lkup_list['smart']['best']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('adequately', 0.19367575645446777),\n",
              " ('electroplating', 0.1939428597688675),\n",
              " ('1623', 0.19624313712120056),\n",
              " ('Bolton', 0.19649575650691986),\n",
              " ('Walking', 0.20014889538288116),\n",
              " ('Ackroyd', 0.20353351533412933),\n",
              " ('seasonal', 0.20835746824741364),\n",
              " ('uptempo', 0.22692306339740753),\n",
              " ('Kurt', 0.2292158007621765),\n",
              " ('13', 0.23212550580501556)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 452
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxt08B68Bw_",
        "colab_type": "code",
        "outputId": "8ab7f26e-ebfb-474c-94b4-ee7f8fe74599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "lkup_list['smart']['worst']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stephanie', -0.25110581517219543),\n",
              " ('Precinct', -0.2189103066921234),\n",
              " ('forget', -0.21829074621200562),\n",
              " ('sparkling', -0.218269482254982),\n",
              " ('Hibari.', -0.20343728363513947),\n",
              " ('pareiasaur', -0.2009078711271286),\n",
              " ('permanence', -0.1899358034133911),\n",
              " ('odes', -0.18928715586662292),\n",
              " ('Statistics', -0.18898172676563263),\n",
              " ('artist', -0.18862245976924896)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 453
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyW51O608BxE",
        "colab_type": "code",
        "outputId": "bdb69313-cb13-4d63-ed29-fdb67ea9821e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "whole_word_ids_lkup = train_dict.encode_token_seq(whole_words_lkup)   # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(weight_matrix_lkup, whole_word_ids_lkup, whole_words_lkup)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIMAAARiCAYAAAA3EzfQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8a_-xZz8BxG",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUnIC2tQ8BxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_matrix_proj = model.projection.weight\n",
        "words = ['productive', 'teenage','south','antelope','smart']\n",
        "# some verbs, and nouns\n",
        "proj_list, whole_words_proj = cos_similarity(weight_matrix_proj, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThISdsEb8BxI",
        "colab_type": "code",
        "outputId": "ea12572b-6804-41ca-8bc9-ac27911daba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "proj_list['productive']['best']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gowns', 0.1951064169406891),\n",
              " ('Dollar', 0.19579720497131348),\n",
              " ('part', 0.19603244960308075),\n",
              " ('Historia', 0.19711852073669434),\n",
              " ('Colima', 0.19887398183345795),\n",
              " ('Lucie', 0.20029447972774506),\n",
              " ('Hundreds', 0.20106525719165802),\n",
              " ('infused', 0.2062041461467743),\n",
              " ('fringe', 0.20863540470600128),\n",
              " ('CCU', 0.21876445412635803)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGTpi0718BxK",
        "colab_type": "code",
        "outputId": "c1105aae-a0cb-4460-dd0a-fbd815e2acd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "proj_list['productive']['worst']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jurists', -0.2903308570384979),\n",
              " ('Howard', -0.2559453547000885),\n",
              " ('Curtis', -0.2206348180770874),\n",
              " ('relatable', -0.21580320596694946),\n",
              " ('culminated', -0.2153409868478775),\n",
              " ('Charitable', -0.2135813683271408),\n",
              " ('Do', -0.20930202305316925),\n",
              " ('trillion', -0.20673182606697083),\n",
              " ('reopen', -0.20372317731380463),\n",
              " ('Joaquín', -0.20251835882663727)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y80MFDgD8BxM",
        "colab_type": "code",
        "outputId": "3c95ff32-4f08-4dc9-e1a7-b56d90875f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "whole_word_ids_proj = train_dict.encode_token_seq(whole_words_proj)   # e.g. use dictionary.get_id on a list of words\n",
        "umap_plot(weight_matrix_proj, whole_word_ids, whole_words_proj)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-484-51d748a55728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwhole_word_ids_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_token_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_words_proj\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# e.g. use dictionary.get_id on a list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mumap_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_matrix_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_words_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'whole_word_ids' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9TM9ui8BxQ",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpWeHqeu8BxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_logprob(seq, model):\n",
        "  \"\"\" Input: a sentence (string) \n",
        "      Output: the averaged log probability of the sentence.\n",
        "  \"\"\"\n",
        "  inp = [train_dict.get_id('<bos>')] + train_dict.encode_token_seq(seq.split(' '))\n",
        "  target = inp[1:] + [train_dict.get_id('<eos>')]\n",
        "    \n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      logits = model(torch.tensor([inp], dtype=torch.long))\n",
        "  \n",
        "  num = len(inp)-1 # minus the <bos>\n",
        "  logp = 0\n",
        "  for i in range(num):\n",
        "    # get the correct next word from the tarfet list.\n",
        "    next_word = target[i]   \n",
        "    # get teh distribution of the next possible word. \n",
        "    prob_dist = torch.softmax(logits[0,i], dim= -1) \n",
        "    # not sure whether to add this sentence.. \n",
        "    # prob_dist = prob_distr.current_device()\n",
        "    \n",
        "    # get the prob for the correct next word. \n",
        "    logp += np.log(prob_dist[next_word]).item()\n",
        "  \n",
        "  return logp/num\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYmrd7nOmLtl",
        "colab_type": "code",
        "outputId": "3033d893-8d34-40c1-e59b-7698dacfb009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# load the best model:\n",
        "embedding_size = 300\n",
        "hidden_size = 300\n",
        "\n",
        "options = {\n",
        "        'num_embeddings': len(train_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': train_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'lstm_dropout': lstm_dropout,\n",
        "        'bias': True,\n",
        "        'bid': False \n",
        "    }\n",
        "model = LSTMModel(options)\n",
        "model.load_state_dict(torch.load(F'/content/drive/My Drive/NLP/best_emb_dim_300_hidden_size_300_LSTM.pt')['model_dict'])\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lookup): Embedding(33178, 300, padding_idx=2)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=300, out_features=33178, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 495
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb12rDvnwco0",
        "colab_type": "code",
        "outputId": "a36aa29c-8040-4ff5-8800-c6c56b7f5701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO3n49iHeOrQ",
        "colab_type": "code",
        "outputId": "6f34af52-da3a-4e22-a136-d93c13ecf535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# test the function: \n",
        "get_logprob(\"I love cats and dogs.\", model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.40573251247406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 496
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tUpJJzp8BxT",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtyeXOcsQm-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a list of all sentences in the validation datasets.\n",
        "wiki_valid = [' '.join(a for a in aaa) for aaa in datasets['valid']]\n",
        "\n",
        "# get the score for each sentence\n",
        "logp_valid = [get_logprob(seq, model) for seq in wiki_valid]\n",
        "\n",
        "# sort the logp\n",
        "sorted_logp_valid = [i for i in sorted(enumerate(logp_valid), key=lambda x:x[1])]\n",
        "\n",
        "# get ten sentences with the lowest score.\n",
        "low_idx = [l[0] for l in sorted_logp_valid[:10]]\n",
        "low_score = [l[1] for l in sorted_logp_valid[:10]]\n",
        "low_seq = [wiki_valid[i] for i in low_idx]\n",
        "\n",
        "# get ten sentences with the highest score.\n",
        "high_idx = [l[0] for l in sorted_logp_valid[-10:]]\n",
        "high_score = [l[1] for l in sorted_logp_valid[-10:]]\n",
        "high_seq = [wiki_valid[i] for i in high_idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H4dxIV88BxU",
        "colab_type": "code",
        "outputId": "d9af3153-743c-489e-d673-39ec63bc89a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "# print the higest scored sentences\n",
        "print(\"These are the sentences withe the highest score:\")\n",
        "[print(f'sentence: {i}') for i in high_seq]\n",
        "print(\"\\n\")\n",
        "# pritn the lowest scored sentences\n",
        "print(\"These are the sentences withe the lowest score:\")\n",
        "[print(f'sentence: {i}') for i in low_seq]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are the sentences withe the highest score:\n",
            "sentence: = = Post @-@ war period = =\n",
            "sentence: \" Daydream Interlude ( Fantasy Sweet Dub Mix ) \"\n",
            "sentence: The series depicts the everyday lives of office employees in the Scranton , Pennsylvania branch of the fictional Dunder Mifflin Paper Company .\n",
            "sentence: The VHS was a commercial success , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units .\n",
            "sentence: A Critical Guide to The X @-@ Files , Millennium & The Lone Gunmen , rated \" ...\n",
            "sentence: It originally aired on the Fox network in the United States on March 4 , 2012 .\n",
            "sentence: = = Post @-@ war career = =\n",
            "sentence: The Boat Race is a side @-@ by @-@ side rowing competition between the University of Oxford ( sometimes referred to as the \" Dark Blues \" ) and the University of Cambridge ( sometimes referred to as the \" Light Blues \" ) .\n",
            "sentence: This means that it was seen by 4 @.@ 0 % of all 18- to 49 @-@ year @-@ olds , and 11 % of all 18- to 49 @-@ year @-@ olds watching television at the time of the broadcast .\n",
            "sentence: = = Early life and education = =\n",
            "\n",
            "\n",
            "These are the sentences withe the lowest score:\n",
            "sentence: Acts Of Rebellion : The Ward Churchill Reader .\n",
            "sentence: Giovanni Bianco – Graphic Design , Art Direction\n",
            "sentence: unk> corroborated Blythe 's testimony that Blythe asked \" Are you okay ?\n",
            "sentence: A Cloud Drifting in Silent Darkness , written by <unk>\n",
            "sentence: > also enjoyed the support of the United Mine Workers and Louisville mayor Neville Miller .\n",
            "sentence: Sega of America packaged it with American Genesis consoles , replacing <unk> Beast .\n",
            "sentence: = Raid on Manila ( 1798 ) =\n",
            "sentence: Churchill <unk> About Academic Freedom – Free Speech Radio News February 9 , 2005\n",
            "sentence: will cost Sonic a life regardless of rings or other protection .\n",
            "sentence: any of Fringe 's eleven @-@ herbs and spices .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 498
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iat6XrhD8BxW",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUmH3BiQ8BxY",
        "colab_type": "code",
        "outputId": "cd8ffa57-d8fe-4a77-975a-953d5784b89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# choose a long ont to midofy \n",
        "\n",
        "mod_low = \" The VHS was a commercial failure , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units . \"\n",
        "mod_high = \" The VHS was a commercial tea , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units . \"\n",
        "print(f'Original: \\n {high_seq[3]} \\nScore: \\n {high_score[3]}')\n",
        "print(f'Replace success with failure: \\n {mod_low} \\nScore: \\n {get_logprob(mod_low,model)}')\n",
        "print(f'Replace success with tea: \\n {mod_high} \\nScore: \\n {get_logprob(mod_high,model)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: \n",
            " The VHS was a commercial success , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units . \n",
            "Score: \n",
            " -1.6729712699229518\n",
            "Replace success with failure: \n",
            "  The VHS was a commercial failure , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units .  \n",
            "Score: \n",
            " -2.5508986361164716\n",
            "Replace success with tea: \n",
            "  The VHS was a commercial tea , being certified platinum by the Recording Industry Association of America ( RIAA ) , denoting shipments of over 100 @,@ 000 units .  \n",
            "Score: \n",
            " -3.7741181976816733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElfxRup68Bxc",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQnM7M18Bxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wGTu5G_8Bxh",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qa0ig2zh8Bxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd3bYBH78Bxl",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RPWkp7I88Bxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}